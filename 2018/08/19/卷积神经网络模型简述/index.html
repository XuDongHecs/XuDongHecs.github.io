<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ResNet,Inception,SENet,ShuffleNet,MobileNet," />










<meta name="description" content="上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNe">
<meta name="keywords" content="ResNet,Inception,SENet,ShuffleNet,MobileNet">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络模型简述">
<meta property="og:url" content="http://yoursite.com/2018/08/19/卷积神经网络模型简述/index.html">
<meta property="og:site_name" content="何先生的Blog">
<meta property="og:description" content="上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNe">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/CNN%20modules_acc_vs_net_vs_ops.png">
<meta property="og:image" content="http://yoursite.com/images/神经网络经典结构.png">
<meta property="og:image" content="http://yoursite.com/images/基础卷积架构.png">
<meta property="og:image" content="http://yoursite.com/images/lenet-5.png">
<meta property="og:image" content="http://yoursite.com/images/alexnet.jpg">
<meta property="og:image" content="http://yoursite.com/images/AlexNet-Ag.png">
<meta property="og:image" content="http://yoursite.com/images/VGG.jpg">
<meta property="og:image" content="http://yoursite.com/images/GoogLeNet-incaption.png">
<meta property="og:image" content="http://yoursite.com/images/1x3卷积核代替3x3卷积.jpg">
<meta property="og:image" content="http://yoursite.com/images/使用一维卷积核代替二维卷积核.jpg">
<meta property="og:image" content="http://yoursite.com/images/shortcut.jpg">
<meta property="og:image" content="http://yoursite.com/images/ResNet-34.jpg">
<meta property="og:image" content="http://yoursite.com/images/ResNet_mode.jpg">
<meta property="og:image" content="http://yoursite.com/images/DenseNetBlock.png">
<meta property="og:image" content="http://yoursite.com/images/densenet.png">
<meta property="og:image" content="http://yoursite.com/images/SENet.jpg">
<meta property="og:updated_time" content="2018-08-24T18:59:33.074Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷积神经网络模型简述">
<meta name="twitter:description" content="上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNe">
<meta name="twitter:image" content="http://yoursite.com/images/CNN%20modules_acc_vs_net_vs_ops.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/19/卷积神经网络模型简述/"/>





  <title>卷积神经网络模型简述 | 何先生的Blog</title>
  








  
  <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">何先生的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">该搬的砖一块都少不了，Come on!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-baidusitemap">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            baidusitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/19/卷积神经网络模型简述/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="会旋转的霸东">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="何先生的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">卷积神经网络模型简述</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-19T18:13:11+08:00">
                2018-08-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/卷积神经网络/" itemprop="url" rel="index">
                    <span itemprop="name">卷积神经网络</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/19/卷积神经网络模型简述/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/19/卷积神经网络模型简述/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>  上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNet，图像分割中的FCN。本文将以LeNet简单介绍卷积基础架构开始，以讲述Blocks演变的形式简单讲解不同网络的特点。</p>
<a id="more"></a>
<figure>
<img src="/images/CNN%20modules_acc_vs_net_vs_ops.png" alt="卷积模型发展"><figcaption>卷积模型发展</figcaption>
</figure>
<h2 id="卷机神经网络基础架构">卷机神经网络基础架构</h2>
<p>  典型的神经网络由一个输入层，多个隐藏层和一个输出层组成，在卷积神经网络中称为全连接。对于每一层的输出：<span class="math inline">\(y_i=f(W_{i}x +b)，其中f(x)为激活函数\)</span>；其经典结构如下图所示：</p>
<figure>
<img src="/images/神经网络经典结构.png" alt="神经网络结构"><figcaption>神经网络结构</figcaption>
</figure>
<p>  卷积神经网络的基础结构如下图所示：</p>
<figure>
<img src="/images/基础卷积架构.png" alt="完整的卷积结构"><figcaption>完整的卷积结构</figcaption>
</figure>
<p>  卷积神经网络采用的权值共享，相对与全连接操作，通常减少了数量级的参数；此外，卷积神经网络的局部操作，在处理具有局部相关性的任务时，与全连接相比具有明显的优势。</p>
<h4 id="todo">TODO:</h4>
<p>卷积神经网络的构成成分：</p>
<ul>
<li>[x] 卷积层</li>
<li>[x] 池化层</li>
<li>[x] 全连接层（FCN和输出层前是Global_pooling的架构没有）</li>
<li>激活函数</li>
<li>Batch Nomalization(BN层)</li>
<li>局部响应归一化（LRN）</li>
<li>Dropout层</li>
</ul>
<h2 id="lenet5">LeNet5</h2>
<p>  LeNet5架构是一个开创性的工作。图像特征是全局和局部的统一，LeNet5利用一组相同的卷积核在特征图的一个通道上对全局特征进行滤波处理，同时融合局部特征，利用下采样压缩局部的相似特征。当时没有GPU来帮助训练，甚至CPU速度都非常慢。因此，对比使用每个像素作为一个单独的输入的多层神经网络，LeNet5能够节省参数和计算是一个关键的优势。LeNet5论文中提到，全连接不应该被放在第一层，因为图像中有着高度的空间相关性，并利用图像各个像素作为单独的输入特征不会利用这些相关性。因此有了CNN的三个特性了：<strong>1.局部感知、2.平移不变性（下采样）、3.权值共享</strong>。</p>
<figure>
<img src="/images/lenet-5.png" alt="LeNet-5"><figcaption>LeNet-5</figcaption>
</figure>
<h2 id="alexnet">AlexNet</h2>
<p>AlexNet特点：</p>
<ul>
<li>由五层卷积和三层全连接组成，输入图像为三通道 224x224 大小，网络规模远大于 LeNet-5</li>
<li>使用了 ReLU 激活函数，提高了训练速度</li>
<li>使用了 Dropout，可以作为正则项防止过拟合，提升模型鲁棒性</li>
<li>加入了局部响应归一化（LRN）提高了精度</li>
<li>引入最大池化</li>
<li>在训练时使用了分组卷积操作（参见《卷积神经网络（上）》）</li>
<li>一些很好的训练技巧，包括数据增广、学习率策略、weight decay 等</li>
</ul>
<p>AlexNet架构：</p>
<figure>
<img src="/images/alexnet.jpg" alt="AlexNet"><figcaption>AlexNet</figcaption>
</figure>
<figure>
<img src="/images/AlexNet-Ag.png" alt="Ng-AlexNet"><figcaption>Ng-AlexNet</figcaption>
</figure>
<h2 id="vgg">VGG</h2>
<p>  VGG是一种更简单的架构模型，因为它没有使用太多的超参数。它总是使用3 x 3滤波器，在卷积层中步长为1，并使用SAME填充在2 x 2池中，步长为2。</p>
<ul>
<li><p>VGG使用3*3卷积核级联提高感受野，一改LeNet-5和AlexNet的<code>卷积-池化</code>结构。</p></li>
<li><p>VGG模型更深，参数更少，后续的模型基本都遵循了小卷积核级联的设计风格。</p>
<figure>
<img src="/images/VGG.jpg" alt="VGG"><figcaption>VGG</figcaption>
</figure></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keras API</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">block2_conv</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv1'</span>)(x)</span><br><span class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv2'</span>)(x)</span><br><span class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block2_pool'</span>)(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="googlenet">GoogLeNet</h2>
<p>  2014年ILSVRC的获胜者提出GoogLeNet架构也称为InceptionV1模型。它在具有不同感受野大小的并行路径中更深入，并且降低了Top5错误率到6.67％。该架构由22层深层组成。它将参数数量从6000万（AlexNet）减少到500万。<strong>GoogLeNet在使用了中间输出做了多任务学习，防止网络过深造成的梯度消失的情况；每层使用多个卷积核</strong>。</p>
<figure>
<img src="/images/GoogLeNet-incaption.png" alt="GoogLeNet-inception"><figcaption>GoogLeNet-inception</figcaption>
</figure>
<h2 id="inception-v2">Inception V2</h2>
<p>  Iception V2 主要提出了Batch Nomalization(BN)，BN层对mini-batch内部数据进行标准化，再给标准化数据乘以权重和加bias，保证了模型可以学习回原来的分布。用了BN层后减少或者取消 <strong>Dropout</strong> 和 <strong>LRN</strong>。</p>
<ul>
<li>[ ] 数学推导后续有时间再写一个博客专门写深度学习的正则化。</li>
</ul>
<h2 id="inception-v3">Inception V3</h2>
<p>  Inception V3将一个较大的二维卷积拆成两个较小的一维卷积，比如将7x7卷积拆成1x7卷积和7x1卷积，或者将3x3卷积拆成1x3卷积和3x1卷积，另外也使用了将5x5 用两个 3x3 卷积替换，7x7 用三个 3x3 卷积替换，如下图所示。一方面节约了大量参数，加速运算并减轻了过拟合，同时增加了一层非线性扩展模型表达能力。论文中指出，这种非对称的卷积结构拆分，其结果比对称地拆为几个相同的小卷积核效果更明显，可以处理更多、更丰富的空间特征，增加特征多样性。示意图如下：</p>
<figure>
<img src="/images/1x3卷积核代替3x3卷积.jpg" alt="1x3卷积核代替3x3卷积核"><figcaption>1x3卷积核代替3x3卷积核</figcaption>
</figure>
<figure>
<img src="/images/使用一维卷积核代替二维卷积核.jpg" alt="使用一维卷积核代替二维卷积核"><figcaption>使用一维卷积核代替二维卷积核</figcaption>
</figure>
<h2 id="resnet">ResNet</h2>
<p>  ILSRVC 2015的获胜者，被何凯明大神称为残差网络（ResNet）。该架构引入了一个名为“skip connections”的概念。ResNet采用Shortcut单元实现信息的跨层流通。与Inception 模块的Concat操作不同，Residual 模块使用 Add 操作完成信息融合。</p>
<figure>
<img src="/images/shortcut.jpg" alt="shotcut"><figcaption>shotcut</figcaption>
</figure>
<p>  通过引入直连，原来需要学习完全的重构映射，从头创建输出，并不容易，而引入直连之后，只需要学习输出和原来输入的差值即可，绝对量变相对量，容易很多，所以叫残差网络。并且，通过引入残差，identity 恒等映射，相当于一个梯度高速通道，可以容易地训练避免梯度消失的问题，所以可以得到很深的网络，网络层数由 GoogLeNet 的 22 层到了ResNet的 152 层。然而，恒等函数与<span class="math inline">\(H_{\ ell}\)</span>的输出是通过求和组合，这可能阻碍网络中的信息流。</p>
<p>  ResNet-34 的网络结构如下所示：</p>
<figure>
<img src="/images/ResNet-34.jpg" alt="ResNet-34"><figcaption>ResNet-34</figcaption>
</figure>
<p>不同的ResNet结构对比：</p>
<figure>
<img src="/images/ResNet_mode.jpg" alt="resnet结构"><figcaption>resnet结构</figcaption>
</figure>
<h2 id="densenet">DenseNet</h2>
<p>  说到DenseNet全文就俩公式，当时我还觉得我咋写不出来这样的论文~~。</p>
<p>先预览下<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">DenseNet论文</a>里的DenseBlocks:</p>
<figure>
<img src="/images/DenseNetBlock.png" alt="DenseBlock"><figcaption>DenseBlock</figcaption>
</figure>
<p>  <strong>ResNet：</strong>传统的前馈卷积神经网络将第+1层的输入，可表示为：<span class="math inline">\(x_{\ell} = H_{\ell}(x_{\ell-1})\)</span>。ResNet添加了一个跳连接，即使用恒等函数跳过非线性变换： <span class="math display">\[
x_{\ell} = H_{\ell}(x_{\ell-1}) + x_{\ell-1}
\]</span></p>
<p><strong>密集连接：</strong>为了进一步改善层之间的信息流，我们提出了不同的连接模式：我们提出从任何层到所有后续层的直接连接。因此，第<span class="math inline">\(x_{0},…,x_{\ell-1}\)</span>作为输入: <span class="math display">\[
x_{\ell} = H_{\ell}([x_{0},x_{1},...,x_{\ell-1}])
\]</span>   如果说Iception ResNet V2是Inception 思想融合了ResNet，那么我认为 DenseNet就是 Residual 思想（信息跨层流通）结合了Inception 理念。</p>
<figure>
<img src="/images/densenet.png" alt="densenet"><figcaption>densenet</figcaption>
</figure>
<p>DenseNet的几个重要参数：</p>
<p>  <strong>增长率：</strong>如果每个<span class="math inline">\(H_{\ell}\)</span>层输出K个特征图，那么第<span class="math inline">\(\ell\)</span>层输出<span class="math inline">\(K_{0}+K(\ell-1)\)</span>个特征图，其中<span class="math inline">\(K_{0}\)</span>是输入层的通道数。</p>
<p>  <strong>瓶颈层：</strong>即<span class="math inline">\(1\times1\)</span>卷积层。每一层输出<span class="math inline">\(K_{0}+K(\ell-1)\)</span>个，理论上将每个输出为个Feature Maps，理论上将每个Dense Block输出为K_{0}+_{1}^{}K(i-1)$个Fature Maps，。<span class="math inline">\(1\times1\)</span>卷积层的作用是将一个Dense Block的特征图压缩到<span class="math inline">\(K_{0}+K\ell\)</span>个。</p>
<p>  <strong>压缩：</strong>为了进一步提高模型的紧凑性，我们可以在过渡层减少特征图的数量。。作者选择压缩率（theta）为0.5。包含Bottleneck Layer的叫DenseNet-B，包含压缩层的叫DenseNet-C，两者都包含的叫DenseNet-BC</p>
<h2 id="senet">SENet</h2>
<p>  SENet(Squeeze-and-Excitation Networks)是基于特征通道之间的关系提出的，下图是SENet的Block单元，图中的Ftr是传统的卷积结构，X和U是Ftr的输入和输出，这些都是以往结构中已存在的。SENet增加的部分是U后的结构：对U先做一个Global Average Pooling（称为Squeeze过程），输出是一个1x1xC的数据，再经过两级全连接（称为Excitation过程），最后用sigmoid把输出限制到[0，1]的范围，把这个值作为scale再乘到U的C个通道上，作为下一级的输入数据。这种结构的原理是想通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。</p>
<figure>
<img src="/images/SENet.jpg" alt="SENet"><figcaption>SENet</figcaption>
</figure>
<p>其实<strong>SENet就是通道级attention机制</strong>。</p>
<h2 id="总结">总结：</h2>
<p>卷积神经网络设计：</p>
<ol type="1">
<li><p>从防止梯度消失出发</p></li>
<li><p>从信息流通出发</p></li>
<li><p>从多尺度特征图融合出发</p></li>
<li><p>从通道级或者像素级特征选择出发</p></li>
</ol>
<h2 id="reference">Reference</h2>
<ol type="1">
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">LeNet</a></li>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a></li>
<li><a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network-in-network</a></li>
<li><a href="http://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a></li>
<li><a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">GoogleNet</a></li>
<li><a href="http://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Inception V3</a></li>
<li><a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch-normalized</a></li>
<li><a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="noopener">ResNet</a></li>
<li>Hu J, Shen L, Sun G. Squeeze-and-Excitation Networks[J]. 2017.</li>
<li>https://chenzomi12.github.io/2016/12/13/CNN-Architectures/</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ResNet/" rel="tag"># ResNet</a>
          
            <a href="/tags/Inception/" rel="tag"># Inception</a>
          
            <a href="/tags/SENet/" rel="tag"># SENet</a>
          
            <a href="/tags/ShuffleNet/" rel="tag"># ShuffleNet</a>
          
            <a href="/tags/MobileNet/" rel="tag"># MobileNet</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/08/卷积神经网络/" rel="next" title="卷积神经网络">
                <i class="fa fa-chevron-left"></i> 卷积神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/24/python工具笔记/" rel="prev" title="python实用小工具">
                python实用小工具 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">会旋转的霸东</p>
              <p class="site-description motion-element" itemprop="description">写个博客，做做总结</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/XuDongHecs" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#卷机神经网络基础架构"><span class="nav-number">1.</span> <span class="nav-text">卷机神经网络基础架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#todo"><span class="nav-number">1.0.1.</span> <span class="nav-text">TODO:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lenet5"><span class="nav-number">2.</span> <span class="nav-text">LeNet5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#alexnet"><span class="nav-number">3.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vgg"><span class="nav-number">4.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#googlenet"><span class="nav-number">5.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inception-v2"><span class="nav-number">6.</span> <span class="nav-text">Inception V2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inception-v3"><span class="nav-number">7.</span> <span class="nav-text">Inception V3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#resnet"><span class="nav-number">8.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#densenet"><span class="nav-number">9.</span> <span class="nav-text">DenseNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#senet"><span class="nav-number">10.</span> <span class="nav-text">SENet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">11.</span> <span class="nav-text">总结：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-number">12.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">会旋转的霸东</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/08/19/卷积神经网络模型简述/';
          this.page.identifier = '2018/08/19/卷积神经网络模型简述/';
          this.page.title = '卷积神经网络模型简述';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
