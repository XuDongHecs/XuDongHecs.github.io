<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python实用小工具]]></title>
    <url>%2F2018%2F08%2F24%2Fpython%E5%B7%A5%E5%85%B7%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录一些经常用的实用小工具和函数包。会持续更新我实际需求用到的代码，相关当笔记用，方便查阅。 [x] OS 不可描述的强大 [x] shutil 复制/移动/修改文件夹操作 OS os.sep 可以取代操作系统特定的路径分隔符。windows下为 ‘\’ os.name 字符串指示你正在使用的平台。比如对于Windows，它是’nt’，而对于Linux/Unix用户，它是 ‘posix’ os.getcwd() 函数得到当前工作目录，即当前Python脚本工作的目录路径 os.getenv() 获取一个环境变量，如果没有返回none os.putenv(key, value) 设置一个环境变量值 os.listdir(path) 返回指定目录下的所有文件和目录名 os.remove(path) 函数用来删除一个文件 os.system(command) 函数用来运行shell命令 os.linesep 字符串给出当前平台使用的行终止符。例如，Windows使用 ‘’，Linux使用 ‘’ 而Mac使用 ‘’ os.path.split(path) 函数返回一个路径的目录名和文件名 os.path.isfile() 和os.path.isdir()函数分别检验给出的路径是一个文件还是目录 os.path.exists() 函数用来检验给出的路径是否真地存在 os.curdir 返回当前目录 (‘.’) os.mkdir(path) 创建一个目录 os.makedirs(path) 递归的创建目录 os.chdir(dirname) 改变工作目录到dirname os.path.getsize(name) 获得文件大小，如果name是目录返回0L os.path.abspath(name) 获得绝对路径 os.path.normpath(path) 规范path字符串形式 os.path.splitext() 分离文件名与扩展名 os.path.join(path,name) 连接目录与文件名或目录 os.path.basename(path) 返回文件名 os.path.dirname(path) 返回文件路径 os.walk(top,topdown=True,onerror=None) 遍历迭代目录 os.rename(src, dst) 重命名file或者directory src到dst 如果dst是一个存在的directory, 将抛出OSError. 在Unix, 如果dst在存且是一个file, 如果用户有权限的话，它将被安静的替换. 操作将会失败在某些Unix 中如果src和dst在不同的文件系统中. 如果成功, 这命名操作将会是一个原子操作 (这是POSIX 需要). 在 Windows上, 如果dst已经存在, 将抛出OSError，即使它是一个文件. 在unix，Windows中有效。 os.renames(old, new) 递归重命名文件夹或者文件。像rename() shutil 模块 shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉 shutil.move( src, dst) 移动文件或重命名 shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的 shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间 shutil.copy( src, dst) 复制一个文件到一个文件或一个目录 shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西 shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作 shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接 shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容 123456789101112131415161718192021222324"""keras需要读取文件夹生成类别，然而很多时候我们只有文件名和类别。需要我们写小爬虫，或者把所有文件分类到不同文件夹#记录示例代码：train.txt : IMAGENAME CLASSNAMEtrain: DIR/IMAGE"""import osimport shutilf = open('G:\\零样本识别\\data\Atrain\\train.txt')trainfile = f.readlines()trainID = &#123;&#125;for file in trainfile: file = file.rsplit('\t') trainID[file[0]] = file[1].rsplit('\n')[0]del trainfile,filef.close()trainpath = 'G:\\零样本识别\\data\Atrain\\train\\'i=0for img in trainID.keys(): i +=1 print (i) if not os.path.exists(trainpath+trainID[img]): os.mkdir(trainpath+trainID[img]) shutil.move(trainpath+img,trainpath+trainID[img])]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>批量文件操作工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络模型简述]]></title>
    <url>%2F2018%2F08%2F19%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNet，图像分割中的FCN。本文将以LeNet简单介绍卷积基础架构开始，以讲述Blocks演变的形式简单讲解不同网络的特点。 卷积模型发展 卷机神经网络基础架构 典型的神经网络由一个输入层，多个隐藏层和一个输出层组成，在卷积神经网络中称为全连接。对于每一层的输出：\(y_i=f(W_{i}x +b)，其中f(x)为激活函数\)；其经典结构如下图所示： 神经网络结构 卷积神经网络的基础结构如下图所示： 完整的卷积结构 卷积神经网络采用的权值共享，相对与全连接操作，通常减少了数量级的参数；此外，卷积神经网络的局部操作，在处理具有局部相关性的任务时，与全连接相比具有明显的优势。 TODO: 卷积神经网络的构成成分： [x] 卷积层 [x] 池化层 [x] 全连接层（FCN和输出层前是Global_pooling的架构没有） 激活函数 Batch Nomalization(BN层) 局部响应归一化（LRN） Dropout层 LeNet5 LeNet5架构是一个开创性的工作。图像特征是全局和局部的统一，LeNet5利用一组相同的卷积核在特征图的一个通道上对全局特征进行滤波处理，同时融合局部特征，利用下采样压缩局部的相似特征。当时没有GPU来帮助训练，甚至CPU速度都非常慢。因此，对比使用每个像素作为一个单独的输入的多层神经网络，LeNet5能够节省参数和计算是一个关键的优势。LeNet5论文中提到，全连接不应该被放在第一层，因为图像中有着高度的空间相关性，并利用图像各个像素作为单独的输入特征不会利用这些相关性。因此有了CNN的三个特性了：1.局部感知、2.平移不变性（下采样）、3.权值共享。 LeNet-5 AlexNet AlexNet特点： 由五层卷积和三层全连接组成，输入图像为三通道 224x224 大小，网络规模远大于 LeNet-5 使用了 ReLU 激活函数，提高了训练速度 使用了 Dropout，可以作为正则项防止过拟合，提升模型鲁棒性 加入了局部响应归一化（LRN）提高了精度 引入最大池化 在训练时使用了分组卷积操作（参见《卷积神经网络（上）》） 一些很好的训练技巧，包括数据增广、学习率策略、weight decay 等 AlexNet架构： AlexNet Ng-AlexNet VGG VGG是一种更简单的架构模型，因为它没有使用太多的超参数。它总是使用3 x 3滤波器，在卷积层中步长为1，并使用SAME填充在2 x 2池中，步长为2。 VGG使用3*3卷积核级联提高感受野，一改LeNet-5和AlexNet的卷积-池化结构。 VGG模型更深，参数更少，后续的模型基本都遵循了小卷积核级联的设计风格。 VGG 123456# Keras APIdef block2_conv(x): x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x) x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x) x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x) return x GoogLeNet 2014年ILSVRC的获胜者提出GoogLeNet架构也称为InceptionV1模型。它在具有不同感受野大小的并行路径中更深入，并且降低了Top5错误率到6.67％。该架构由22层深层组成。它将参数数量从6000万（AlexNet）减少到500万。GoogLeNet在使用了中间输出做了多任务学习，防止网络过深造成的梯度消失的情况；每层使用多个卷积核。 GoogLeNet-inception Inception V2 Iception V2 主要提出了Batch Nomalization(BN)，BN层对mini-batch内部数据进行标准化，再给标准化数据乘以权重和加bias，保证了模型可以学习回原来的分布。用了BN层后减少或者取消 Dropout 和 LRN。 [ ] 数学推导后续有时间再写一个博客专门写深度学习的正则化。 Inception V3 Inception V3将一个较大的二维卷积拆成两个较小的一维卷积，比如将7x7卷积拆成1x7卷积和7x1卷积，或者将3x3卷积拆成1x3卷积和3x1卷积，另外也使用了将5x5 用两个 3x3 卷积替换，7x7 用三个 3x3 卷积替换，如下图所示。一方面节约了大量参数，加速运算并减轻了过拟合，同时增加了一层非线性扩展模型表达能力。论文中指出，这种非对称的卷积结构拆分，其结果比对称地拆为几个相同的小卷积核效果更明显，可以处理更多、更丰富的空间特征，增加特征多样性。示意图如下： 1x3卷积核代替3x3卷积核 使用一维卷积核代替二维卷积核 ResNet ILSRVC 2015的获胜者，被何凯明大神称为残差网络（ResNet）。该架构引入了一个名为“skip connections”的概念。ResNet采用Shortcut单元实现信息的跨层流通。与Inception 模块的Concat操作不同，Residual 模块使用 Add 操作完成信息融合。 shotcut 通过引入直连，原来需要学习完全的重构映射，从头创建输出，并不容易，而引入直连之后，只需要学习输出和原来输入的差值即可，绝对量变相对量，容易很多，所以叫残差网络。并且，通过引入残差，identity 恒等映射，相当于一个梯度高速通道，可以容易地训练避免梯度消失的问题，所以可以得到很深的网络，网络层数由 GoogLeNet 的 22 层到了ResNet的 152 层。然而，恒等函数与\(H_{\ ell}\)的输出是通过求和组合，这可能阻碍网络中的信息流。 ResNet-34 的网络结构如下所示： ResNet-34 不同的ResNet结构对比： resnet结构 DenseNet 说到DenseNet全文就俩公式，当时我还觉得我咋写不出来这样的论文~~。 先预览下DenseNet论文里的DenseBlocks: DenseBlock ResNet：传统的前馈卷积神经网络将第+1层的输入，可表示为：\(x_{\ell} = H_{\ell}(x_{\ell-1})\)。ResNet添加了一个跳连接，即使用恒等函数跳过非线性变换： \[ x_{\ell} = H_{\ell}(x_{\ell-1}) + x_{\ell-1} \] 密集连接：为了进一步改善层之间的信息流，我们提出了不同的连接模式：我们提出从任何层到所有后续层的直接连接。因此，第\(x_{0},…,x_{\ell-1}\)作为输入: \[ x_{\ell} = H_{\ell}([x_{0},x_{1},...,x_{\ell-1}]) \] 如果说Iception ResNet V2是Inception 思想融合了ResNet，那么我认为 DenseNet就是 Residual 思想（信息跨层流通）结合了Inception 理念。 densenet DenseNet的几个重要参数： 增长率：如果每个\(H_{\ell}\)层输出K个特征图，那么第\(\ell\)层输出\(K_{0}+K(\ell-1)\)个特征图，其中\(K_{0}\)是输入层的通道数。 瓶颈层：即\(1\times1\)卷积层。每一层输出\(K_{0}+K(\ell-1)\)个，理论上将每个输出为个Feature Maps，理论上将每个Dense Block输出为K_{0}+_{1}^{}K(i-1)$个Fature Maps，。\(1\times1\)卷积层的作用是将一个Dense Block的特征图压缩到\(K_{0}+K\ell\)个。 压缩：为了进一步提高模型的紧凑性，我们可以在过渡层减少特征图的数量。。作者选择压缩率（theta）为0.5。包含Bottleneck Layer的叫DenseNet-B，包含压缩层的叫DenseNet-C，两者都包含的叫DenseNet-BC SENet SENet(Squeeze-and-Excitation Networks)是基于特征通道之间的关系提出的，下图是SENet的Block单元，图中的Ftr是传统的卷积结构，X和U是Ftr的输入和输出，这些都是以往结构中已存在的。SENet增加的部分是U后的结构：对U先做一个Global Average Pooling（称为Squeeze过程），输出是一个1x1xC的数据，再经过两级全连接（称为Excitation过程），最后用sigmoid把输出限制到[0，1]的范围，把这个值作为scale再乘到U的C个通道上，作为下一级的输入数据。这种结构的原理是想通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。 SENet 其实SENet就是通道级attention机制。 总结： 卷积神经网络设计： 从防止梯度消失出发 从信息流通出发 从多尺度特征图融合出发 从通道级或者像素级特征选择出发 Reference LeNet AlexNet Network-in-network VGG GoogleNet Inception V3 Batch-normalized ResNet Hu J, Shen L, Sun G. Squeeze-and-Excitation Networks[J]. 2017. https://chenzomi12.github.io/2016/12/13/CNN-Architectures/]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
        <tag>Inception</tag>
        <tag>SENet</tag>
        <tag>ShuffleNet</tag>
        <tag>MobileNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络]]></title>
    <url>%2F2018%2F08%2F08%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[很多人在接触卷积神经网络之前就接触过卷积运算，比如信号与系统的连续信号卷积和离散信号卷积。本文将我的理解出发对比卷积神经网络的卷积和我们信号卷积的区别与联系。介绍卷积神经网络中的卷积操作，以及不同的卷积策略。为后续深入高效小网络及模型压缩相关概念和原理做准备。 信号卷积 卷积表征函数f与经过翻转和平移的g的乘积函数所围成的曲边梯形的面积。信号与系统中，采样信号经过系统响应得到系统输出。一维卷积的数学表达： \[ 离散卷积：z[n]=(x * y)[n] = \sum_{m = -\infty}^{+\infty}x[m]\cdot y[n - m]\\连续卷积： y(x)=(f * h)(t) = \int_{-\infty}^{+\infty}f(\tau)\cdot h(t - \tau)\mathop{}\!\mathrm{d}\tau \] 我们采集到的信号通常是离散的，因此，后续讲解的就是离散卷积。看下维基百科的卷积可视化。 离散卷积3 一维离散卷积比较好理解，每个输出元素都是卷积核（系统响应）和信号的局部乘积的和。卷积操作是一种滤波操作，信号与系统的高通滤波，低通滤波等，因此，通过卷积操作可以获得某个方面属性的增强。一维离散卷积通常是在时域卷积，图像，激光雷达回波等属于空间属性。 CNN的卷积 卷积计算 以图像为例。图像卷积即二维离散卷积，图像的矩阵表示为\(h*w\)，元素数值大小为无符号整型通常在[0,255]之间。二维离散卷积的公式定义： \[ S[i, j] = (F ∗ K)(i, j) = \sum_m\sum_n F(m, n)K(i − m, j − n) \] 卷积是可交换的(commutative)，该公式可转变为： \[ S[i, j] = (F ∗ K)(i, j) = \sum_m\sum_n F(i-m, i-n)K( m, n),K即我们常说的卷积核 \] 图像卷积示例： 二维卷积示例1 参数：输入Shape：\((7\times7\times3)\) 卷积核Shape:\(3\times3\times3\times2\) \(zero-padding=1 stride=2\) 输出Shape计算： \[ \begin{align} W_2 &amp;= (W_1 - F + 2P)/S + 1\qquad(式2)\\ H_2 &amp;= (H_1 - F + 2P)/S + 1\qquad(式3) \end{align} \] 卷积层 卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。上一节中描述的卷积层的神经元和全连接网络一样都是一维结构。既然卷积网络主要应用在图像处理上，而图像为两维结构，因此为了更充分地利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度H×宽度W×深度C，有C个H ×W 大小的特征映射构成。 特征映射（feature map）为一幅图像（或其它特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征。为了卷积网络的表示能力，可以在每一层使用多个不同的特征映射，以更好地表示图像的特征。 卷积层的关键参数： 卷积核大小（Kernel Size）：定义了卷积操作的感受野。在二维卷积中，通常设置为3，即卷积核大小为3×3。 步幅（Stride）：定义了卷积核遍历图像时的步幅大小。其默认值通常设置为1，也可将步幅设置为2后对图像进行下采样，这种方式与最大池化类似。 边界扩充（Padding）：定义了网络层处理样本边界的方式。当卷积核大于1且不进行边界扩充，输出尺寸将相应缩小；当卷积核以标准方式进行边界扩充，则输出数据的空间尺寸将与输入相等。 输入与输出通道（Channels）：构建卷积层时需定义输入通道I，并由此确定输出通道O。这样，可算出每个网络层的参数量为I×O×K，其中K为卷积核的参数个数。例，某个网络层有64个大小为3×3的卷积核，则对应K值为 3×3 =9。 卷积层特性： 局部连接：卷积计算的通俗理解就是卷积核在图像上做滑动卷积，每次滑动输出一个值，局部乘积求和。局部计算如： 局部求和 权值共享：整张图像可以共用一个卷积核，对比全连接操作卷积不需要同时和每个像素点相乘求和(内积)。 池化（pooling） 池化也叫子采样（subsampling layer），其作用是进行特征选择，降低特征数量，并从而减少参数数量。 标准的卷积操作虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以在卷积操作之后加上一个池化操作，从而降低特征维数，避免过拟合。（也可通过增加滑动卷积的步长Stride来达到降维效果） 池化操作虽然说有降维功能，同时也是存在信息损失的。 池化 常见的池化方式 最大池化（max-pooling） 一般是在池化核对应区域内选取最大值来表征该区域。 平均池化（mean-pooling） 一般是取池化核对应区域内所有元素的均值来表征该区域。 全局池化（global pooling） 一般是对整个特征图进行最大池化或者平均池化。 重叠池化（overlapping-pooling） 相邻池化窗口之间会有重叠区域，此时kernel size&gt;stride 卷积神经网络中的特殊卷积操作 \(1\times1\)卷积核 自从GoogLenet接手发扬光大以后，\(1\times1\)卷积广泛出现在新的卷积架构中如xception和小卷积网络中。\(1\times1\) 卷积核在单通道上就是给每个像素点同等乘以一个系数，多通道上其实现了通道间的信息流通。从全连接的角度解释即C通道特征图，和C个\(1\times1\)卷积核卷积输出1个新的特征图。同理，和KC个1x1的卷积核卷积输出K个特征图。注：全连接运算是元素级的，\(1\times1\)卷积是通道级的，对于HW * C的特征图全连接需要K * H * W * C，而11卷积仅需要KC个参数。图片来自知乎。 \(1\times1\)卷积的作用 跨通道信息交互（channal 的变换） 升维（用最少的参数拓宽网络channal） 降维（减少参数） 深度可分离卷积（depthwise separable convolution） 深度可分离卷积在执行空间卷积，同时保持通道分离，然后进行深度卷积。 可分离卷积与标准的卷积相比，其在通道上先做了卷积然后在再用\(1\times1\)卷积进行通道融合。示意图： 可分离卷积 对比标准卷积过程： 标准卷积 假设我们在16个输入通道和32个输出通道上有一个3x3卷积层。详细情况是，16个3x3内核遍历16个通道中的每一个，产生512（16x32）个特征映射。接下来，我们通过添加它们来合并每个输入通道中的1个特征图。由于我们可以做32次，我们得到了我们想要的32个输出通道。 针对这个例子应用深度可分离卷积，用1个3×3大小的卷积核遍历16通道的数据，得到了16个特征图谱。在融合操作之前，接着用32个1×1大小的卷积核遍历这16个特征图谱，进行相加融合。这个过程使用了16×3×3+16×32×1×1=656个参数，远少于上面的16×32×3×3=4608个参数。 这个例子就是深度可分离卷积的具体操作，其中上面的深度乘数（depth multiplier）设为1，这也是目前这类网络层的通用参数。这么做是为了对空间信息和深度信息进行去耦。从Xception模型的效果可以看出，这种方法是比较有效的。由于能够有效利用参数，因此深度可分离卷积也可以用于移动设备中。 空洞卷积（dilated convolution） dilated convolution是针对图像语义分割问题中下采样会降低图像分辨率、丢失信息而提出的一种卷积思路。 空洞卷积的特点是在扩大感受野的同时不增加计算成本。示意图： 空洞卷积静态图 上图b可以理解为卷积核大小依然是3×3，但是每个卷积点之间有1个空洞，也就是在绿色7×7区域里面，只有9个红色点位置作了卷积处理，其余点权重为0。这样即使卷积核大小不变，但它看到的区域变得更大了 。 空洞卷积的动机：加pooling层，损失信息，降低精度；不加pooling层，感受野变小，模型学习不到全局信息 组卷积（Group convolution） 分组卷积（Group convolution） ，最早在AlexNet中出现，由于当时的硬件资源有限，训练AlexNet时卷积操作不能全部放在同一个GPU处理，因此作者把feature maps分给多个GPU分别进行处理，最后把多个GPU的结果进行融合（concatenate）。 下图分别是一个正常的、没有分组的卷积层结构和分组卷积结构的示意图： 组卷积 标准卷积_分组 组卷积通过将卷积运算的输入限制在每个组内，模型的计算量取得了显著的下降。然而这样做也带来了明显的问题：在多层逐点卷积堆叠时，模型的信息流被分割在各个组内，组与组之间没有信息交换。这将可能影响到模型的表示能力和识别精度。 目前旷世科技提出的ShuffleNet引入通道重排来处理组和组之间的信息流通。 分组卷积操作实例： 从一个具体的例子来看，Group conv本身就极大地减少了参数。比如当输入通道为256，输出通道也为256，kernel size为3×3，不做Group conv参数为256×3×3×256。实施分组卷积时，若group为8，每个group的input channel和output channel均为32，参数为8×32×3×3×32，是原来的八分之一。而Group conv最后每一组输出的feature maps以concatenate的方式组合。 Alex认为group conv的方式能够增加 filter之间的对角相关性，而且能够减少训练参数，不容易过拟合，这类似于正则的效果。 转置卷积 反卷积是一种上采样操作，在图像分割和卷积自编码中应用较多。示意图： 转置卷积 小结 本文简单的介绍了信号卷积的算法原理，引入二维离散卷积引导对卷积神经网络的卷积的理解。卷积是一种滤波操作，在卷积神经网络中有许多卷积变形结构，这些结构从维度控制，信息流通，模型参数大小控制，感受野等角度进行设计。在当前state of art卷积架构中多是融入了部分或者全部上述变形卷积。在设计新的模型时，可从设计目标出发采用上述结构优化，裁剪模型。 下一篇回顾卷积神经网络经典的Blocks。欢迎关注。 参考 邱锡鹏 ,《神经网络与深度学习》https://nndl.github.io/&gt; https://zhuanlan.zhihu.com/p/28749411 https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d https://www.zhihu.com/question/54149221 https://www.cnblogs.com/ranjiewen/articles/8699268.html https://blog.csdn.net/A_a_ron/article/details/79181108]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>信号卷积</tag>
        <tag>组卷积</tag>
        <tag>空洞卷积</tag>
        <tag>可分离卷积</tag>
        <tag>1x1卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入boosting经典算法]]></title>
    <url>%2F2018%2F08%2F02%2F%E6%B7%B1%E5%85%A5boosting%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文旨在梳理 Boosting方法相关的概念及理论推导。 在介绍Boosting方法之前，我们应该对机器学习模型的误差分析有所了解。从经典的Boosting算法—标准Adaboost的原理入手建立Boosting算法的基本理解，再来分析GBDT的原理(下文的GBDT特指(Greedy Function Approximation：A Gradient Boosting Machine )提出的算法)及其变体XGBoost和Lightgbm后续文章再讲。 机器学习中的误差，偏差与方差 误差反映的是整个模型的准确度，偏差反映的是一个模型在样本上的输出与真实值之间的误差，即模型本身的精准度，方差反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性/泛化能力。 偏差和方差的概念粗略的表示为训练误差和|训练误差-测试误差|。偏差过大表现为过拟合，方差过大表现为过拟合。 集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差(过拟合)（bagging）、偏差（boosting）或改进预测（stacking）的效果。 Boosting算法 boosting算法是通过多个弱学习器串联提升成为强分类器，通常为加法模型和前向分布算法，核心为下级弱学习器修正上级学习器的偏差。 问题：为什么多个弱分类器串联在一起可以提升模型的能力减小偏差而不出现过大方差？ boosting算法要求的基分类器不是没有条件的！首先，基分类器需要一定的的可靠性，其次基分类器需要具有多样性。然而很多情况下可靠性和多样性难以兼得。 Adaboost AdaBoost是Adaptive Boosting缩写，是由Yoav Freund和Robert Schapire提出的机器学习元算法。AdaBoost在某种意义上是适应性的，即随后的弱学习器会基于先前分类器分类错误的样本增加权重。AdaBoost对噪声数据和异常值敏感。在某些问题中，它可能比其他学习算法更不容易受到过拟合问题的影响。基学习器可能很弱，但只要每个学习者的表现好于随机猜测，最终的模型可以融合成表现优异的强分类器。 Adaboost算法有多种推导方法[1]，比较容易理解的就是基于加性模型(additive model),即基学习器的线性组合来最小化指数损失函数。 \[ l_{exp}(H|D)=E_{x-D}[e^{-f(x)H(x)}]···········(1) \] \[ H(x) = \sum_{t=1}^{T}\alpha_th_t(x)···········(2) \\ 其中,T表示弱学习器个数，\alpha_t表示第t个学习器的权重，h_t(x)表示第t个学习器的输出 \] Adaboost的核心思想：1. 给错误分类的样本增加权重，让随后的分类器可以有偏的输出；2. 给各个基学习器加上权重。那么怎么给，有什么依据？ Adaboost算法描述： 输入：训练集\(D={(x_1,y_1),(x_2,y_2),···,(x_m,y_m)}\),其中\(y_i\in [-1,1]\) 基学习器f； 迭代次数T； (1) 初始化样本权重\(W_1(x)=\frac{1}{m}\) (2)迭代T次(for)，算出每个模型的误差\(e_t = \sum_{i=1}^{M}I(y_i\ne h_t(x_i))\) (3)if \(\epsilon &gt;0.5\)then break (4)更新\(\alpha_t=\frac{1}{2}ln(\frac{1-e_t}{e_t})\) (5)更新样本新权重\(W_t(x)=\frac{W_{t-1}exp(-\alpha_{t-1}h_{t-1}y_i)}{Z_{t-1}},其中Z_{t-1}=\sum_{}^{}W_{t-1}\) 输出：\(H(x)=sign(\sum_{t=1}^{T}\alpha_th_t(x))\) 问题：显然按照这个算法流程，标准的adaboost实现的是二分类，那么多分类，回归任务需要改进。这仅仅是adaboost的一个应用。 adaboost理论推导的俩个核心：在界定了错误率\(\epsilon_t\)的情况下,求解或者估计\(\alpha_t\)和新的分布\(W_t\)使指数损失函数最小。 \(\alpha_t\)的求解 \[ l_{exp}(H|W)=E_{x-W}[e^{-f(x)H(x)}]=p_{x-W}(f(x)=h_t(x))e^{-\alpha_t}+p_{x-W}(f(x)\ne h_t(x))e^{\alpha_t}=e^{-\alpha_t}(1-\epsilon_t)+e^{\alpha_t}\epsilon_t \] \[ \frac{\nabla \ell_{exp}(H|W)}{\nabla\alpha_t}=-e^{-\alpha_t}(1-\epsilon_t)+e^\alpha_t\epsilon_t \] 令\(\frac{\nabla l_{exp}(H|D)}{\nabla\alpha_t}=0\)可解得： \[ \alpha_t=\frac{1}{2}ln\frac{1-\epsilon_t}{\epsilon_t} \] \(W_t\)的求解 Adaboost的算法核心是在上一个学习器的基础上，对错误分类的样本权重进行向上调整，使其在后面的学习中更被关注。这个地方我们就要说到模型的多样性的重要了。某个样本在一些弱分类器上的表现不好，就在其他分类器上还可能获得增强，然后加性融合时其可获得较低的偏差。然是如果他在所有弱分类器上的表现都不好，那么加性模型也难以修正其偏差，多样性是boost算法一个需要的重要的属性。 在基分类器\(h_t(x)\)时的模型损失函数表达为： \[ \ell_{exp}(H_{t-1}+h_t(x)|W_t)=E_{x-W}[e^{-f(x)(H_{t-1}(x)+h_t(x))}]=E_{x-W}[e^{-f(x)H_{t-1}(x)}e^{-f(x)h_t(x))}] \] 对二分类，将\(e^{-f(x)h_t(x)}\)的二阶泰勒展开带入损失函数近似可得： \[ \ell_{exp}(H_{t-1}+h_t{x}|W)=E_{x-W}[e^{-f(x)H_{t-1}(x)}(\frac{3}{2}-f(x)h_t(x))] \] 最理想情况在当前模型(\(H_{t}(x)\))的损失函数最小(即\(h_t(x)\)纠正了\(H_{t-1}(x)\))的条件下求解\(W_t\)。 即： \[ h_t(x)=\arg\min_{h}\ell_{exp}(H_{t-1}(x)+h_t(x)|W) = \arg\min_{h}E_{x-W}[e^{-f(x)H_{t-1}(x)}(\frac{3}{2}-f(x)h_t(x))]\\= \arg\max_{h}E_{x-W}[e^{-f(x)H_{t-1}(x)}f(x)h_t(x)] \] \(f(x)和H_{t-1}(x)\)已知。令： \[ W_t=\frac{W(x)e^{-f(x)H_{t-1}(x)}}{E_{x-W}[e^{-f(x)H_{t-1}(x)}]} \] \[ 理想的基分类器：h_t(x)=\arg\max_{h}E_{x-W}[W_tf(x)h_t(x)]=\arg\min_{h}E_{x-W_t}[I(f(x) \ne h_t(x))] \] \(h_t(x)\)在分布\(W_t\)下仍然可以达到最小分类误差。 GDBT 吐槽：GDBT真的是太难理解了！如果只从概念上知道GDBT用不断的加基模型减小残差，那可能并没有完全理解GDBT。GDBT模型怎么加的？怎么分类的？输入是多维的构建树的过程是什么样的？ GDBT的基分类器是CART回归树，GDBT是通过不断地最小化残差来实现加法模型，其中梯度提升的本质是每一次建立模型是在之前建立模型损失函数的梯度下降方向 。梯度提升树在结构化数据上的具有优越性，如推荐系统和计算广告里面。 GDBT在处理回归问题时的残差其实就在最小平方损失函数的负梯度方向，即： \[ r_{mi}=-\frac{\partial[\frac{1}{2}(y_i-f_m(x_i))^2]}{\partial f_m(x_i)}=y_i-f_m(x_i) \] 常见的损失函数即负梯度方向： GDBT常见损失函数 上面的叙述说明，残差是负梯度方向的一个例子，GDBT将梯度下降法的优化概理念从参数空间的优化扩展到函数空间的优化。熟悉神经网络的同学应该知道，标准的神经网络是在一个模型上迭代的更新参数\(\theta^{*}\)以达到损失函数最小的期望；在GDBT算法中拓展到函数空间是如何优化的呢？ 浓缩一下：一个变，一个不变！ 一个变：基CART回归树的拟合目标\(y^*\)是在变的，它是上个CART回归树的负梯度(残差)。 一个不变： 每个样本的ground truth是不变的。 GDBT算法： 输入：\((x_i,y_i)_{i=1}^{i=N}\)，迭代次数M，损失函数$$ 输出：\(F(x)=\sum_{j=1}^{j=M}f_j(x)\) 1. 学习第一棵树得到残差： \[ r_{mi}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)} \] 注：初始化第一棵树的负梯度方向（后面的要拟合的\(y_i\)） 2. for m =2:M do: 2.1 拟合上一棵树棵树的残差，计算输出响应\(y_{mi}\):\(y_{i}=[\frac {\partial \ell(y_i,F(x_{i}))}{\partial {F(x_{i})}}]_{F_(x)=F_{m-1}(x)}\) 2.2 学习第m棵树： \(\gamma^*=\arg\min_{w} \sum_{x_i \in R_m}^{}\ell (y_i,F_{m-1}(x_i)+\gamma)\),即利用线性搜索(line search)估计叶结点区域的值，使损失函数极小化 。 2.3 更新树： \[ F_m\left(x\right)=F_{m-1}\left(x\right)+\sum_{j=1}^{J}{\gamma{mj}I\left(x\in R_{mj}\right)},其中J是叶节点 \] 3. 训练结束，输出： \[ \hat{F}\left(x\right)=F_M\left(x\right)=\sum_{m=1}^M{\sum_{j=1}^J{\gamma_{mj}I\left(x\in R_{mj}\right)}} \] —— 参考： 周志华,《机器学习》 jerome H.Friedman,Greedy Function Approximation：A Gradient Boosting Machine 陈天奇，XGBoost 与 Boosted Tree]]></content>
      <categories>
        <category>机器学习算法</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>GDBT</tag>
        <tag>Adaboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析决策树]]></title>
    <url>%2F2018%2F07%2F28%2F%E6%B5%85%E6%9E%90%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[前言： 决策树归纳是从有类标号的训练元组中学习决策模型。常用的决策树算法有ID3，C4.5和CART。它们是采用贪心（即非回溯的）方法，自顶向下递归的分治方法构造。这几个算法选择属性划分的方法各不相同，ID3使用的是信息增益，C4.5使用的是信息增益率，而CART使用的是Gini基尼指数。下面来简单介绍下决策树的理论知识。内容包含决策树的算法构成，熵、信息增益、信息增益率以及Gini指数和树的剪枝的概念及公式。 快速系统的理解一个机器学习算法 算法要解决的问题什么? 算法的输入输出是什么? 算法的实现的机理是什么? 算法的优化方法是什么？ 算法的评价指标是什么？ 决策树的算法构成 决策树归纳是从有类标号的训练元组中学习决策模型。常用的决策树算法有ID3，C4.5和CART。它们都是采用贪心（即非回溯的）方法，自顶向下递归的分治方法构造。这几个算法选择属性划分的方法各不相同，ID3使用的是信息增益，C4.5使用的是信息增益率，而CART使用的是Gini基尼指数。下面来简单介绍下决策树的理论知识。内容包含决策树的算法构成，熵、信息增益、信息增益率以及Gini指数和树的剪枝的概念及公式。 ​ 决策树是是由特征空间作为结点和类空间作为叶子构建的一棵树。： 特征划分逻辑 以每个特征作为判别标志（根节点），用if…then…作为分裂规则(yes or no)，将特征空间划分成互斥且完备的子空间，通过不断地划分子空间，树不断地生长。具体的划分依据和逻辑见第三章。 分类规则： 从统计的角度出发，节点对应的最大条件概率作为分类准则。(要明白似然函数和概率的区别) 优化： 决策树的损失函数通常是正则化的极大似然函数，学习的策略是以损失函数为目标函数的最小化。决策树采用启发式算法来近似求解最优化问题，得到的是次最优的结果。 ​ 该启发式算法可分为三步： 特征选择 模型生成 决策树的剪枝 决策树生成与剪枝 信息量与熵 不管是分类任务还是回归任务，都是在做这样一件事情传递消息（类别or回归值）；特征就是我们对需要传递的消息的一种编码方式即信息。那么怎么衡量信息?信息论定义如下： \[ 信息量：l(x_i)=-log_2p(x_i) \] ​ 有了信息（特征）以后怎么确定这个信息是否可靠的表达消息？我们定义熵来度量信息是否可靠。 \[ 熵：H(x) = -\sum_{i=1}^np_ilog(p_i)\ \] \[ 条件熵： H(Y|X) = H(X,Y)-H(X) = \sum_XP(X)H(Y|X) = -\sum_{X,Y}logP(Y|X) \] 熵越大，说明系统越混乱，携带的信息就越少。熵越小，说明系统越有序，携带的信息就越多。信息的作用就是在于消除不确定性。 均匀分布的不确定性最大，即特征均匀分布（所有特征都一样）熵最大，特征完全没有区分性，无法准确传递消息（类别or回归）。条件熵H（Y|X）描述的是在X给定的条件下Y的不确定性，如果条件熵越小，表示不确定性就越小，那么B就越容易确定结果。 ID3算法与信息增益 ID3划分特征（创建节点的规则）使用的就是信息增益Info-Gain排序。一个特征的信息增益越大，表明该对样本的熵减少的能力就更强，该特征使得数据所属类别的不确定性降低。信息增益描述特征对分类(回归)的不确定性的降低程度，可以用来度量两个变量的相关性。比如，在给定一个变量的条件下，另一个变量它的不确定性能够降低多少，如果不确定性降低得越多，那么它的确定性就越大，就越容易区分，两者就越相关。 信息增益 信息增益在统计学中称为互信息，互信息是条件概率与后验概率的比值，化简之后就可以得到信息增益。所以说互信息其实就是信息增益。计算方法: \[ G(D, A) = H(D) - H(D|A) ，H(D)经验熵，H(D|A)经验条件熵，D表示训练集，A表示特征空间 \] \[ H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}，其中，|C_k|是属于类C_k的个数，|D|是所有样本的个数 \] \[ H(D|A)=\sum_{i=1}^np_{a_i}H(D|a_i)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}，\\特征A有个不同的划分取值\{a_1, a_2, ..., a_n\}，根据特征A的划分取值将D划分为n个子集D_1, D_2, ..., D_n， |D_i|是D_i的样\\本个数，D_ik是中属于C_k类的样本集合。 \] ID3算法流程图 对非空非单一特征空间A: C4.5算法与信息增益比 C4.5算法用信息增益比作为特征选择的依据。算法流程与ID3类似。 使用信息增益作为特征选择的标准时，容易偏向于那些取值比较多的特征，容易导致过拟合。而采用信息增益比则有效地抑制了这个缺点：取值多的特征，以它作为根节点的单节点树的熵很大，导致信息增益比减小，在特征选择上会更加合理。 \[ 信息增益比: g_R(D,A)=\frac{g(D,A)}{H_A(D)}, 其中 H_A(D) = -\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}，n为特征A取值的个数 \] 我的理解就是信息增益比在信息增益上做了个平滑处理，将特征对应的信息增益与特征空间划分的熵相结合起来，约束了不同取值特征的信息增益处理不均衡的问题，在一定程度生类似于标准化，正向统一特征划分的标准。 CART算法 CART(分类回归树)算法是由以特征为节点的多个二叉树串联形成可以完成回归任务和分类任务。CART算法同样由特征选择，树的生成及剪枝组成 ；其中回归树用最小平方误差准则，分类树用基尼指数(Gini index)最小化 准则，进行特征选择，生成二叉树。 1. 最小二乘回归树生成算法 带着问题看算法： 要搞明白最小二乘法是怎么进行特征选择的？ 采用启发式的方法，选择第j个特征和它的取值s作为切分变量和切分点，然后以该特征及切分情况(对应分割区域的输出与真实输出的平方误差和)遍历j的所有取值找到最优的切分点s。具体的优化公式如下： \[ \underset{j,s}{min}[\underset{c_1}{min}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\underset{c_2}{min}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]\\其中，R_1(j,s)=\{x|x^{(j)}≤s\},R_2(j,s)=\{x|x^{(j)}＞s\}，c_1={1\over N_1}\sum_{x_i \in R_1}y_i , \quad c_2={1\over N_2}\sum_{x_i \in R_2}y_i \] 统计学习方法上说 对上述俩个子空间递归调用该步骤,直到满足停止条件。 然后将输入空间划分为M个子区域，生成决策树： \[ f(x) = \sum_{m=1}^Mc_mI(x \in R_m) \] 2. CART分类算法与基尼系数 基尼指数 \[ 基尼指数 ：Gini(D) =1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2，其中C_k是D中属于第k类的子集。\\如果样本集合D根据特征A是否取某一可能值a被分割成D1和D2两部分，即\\ D_1=\{(x,y) \in D| A(x) =a\},D_2 = D-D_1\\则在特征A的条件下，集合D的基尼指数定义为 :Gini(D,A) = \frac{\vert D_1\vert}{\vert D\vert}Gini(D_1)+\frac{\vert D_2\vert}{\vert D\vert}Gini(D_2) \] 问题： 那为什么用基尼指数选择特征而不用信息增益或者信息增益率？​ 分类CART生成： 输入：训练数据集D，停止计算的条件； 输出：CART决策树； 根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树： （1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为“是”或“否”将D分割成D1和D2两部分，利用基尼指数计算公式计算。 （2）在所有可能的特征A以及它们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 （3）对两个子结点递归地调用（1），（2），直至满足停止条件。 （4）生成CART决策树 算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。 决策树的剪枝 因为决策树的生成算法容易构建过于复杂的决策树，产生过拟合。而剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，在这里我们可以联想其实剪枝就是一直正则化手段，简化模型，防止过拟合！下面我们进行详细介绍： 决策树剪枝的基本策略： - 预剪枝(prepruning) 在决策树生成过程中，对每个节点在分割前先进行估计。若当前的分割不能带来决策树泛化性能的提升，则停止分割并将当前节点标记为叶节点 在预剪枝过程中，决策树的很多分支都没有展开，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间和预测时间；但有些分支的当前分割虽不能提升泛化性能、甚至能导致泛化性能下降，但在其基础上的后续分割却有可能导致泛化性能的显著提高，容易导致欠拟合 - 后剪枝(postpurning) 先从训练样本集生成一棵最大规模的完整的决策树，然后自底向上地对非叶结点进行考察。若将该提升决策树的泛化性能，则将该子树替换为叶节点节点对应的子树替换为叶节点能 后剪枝决策树比预剪枝决策树保留了更多的分支。后剪枝过程的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中所有的非叶节点进行逐一考察，因此训练时间开销比未剪枝决策树和预剪枝决策树都要大的多 决策树的剪枝 决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。 设树T的叶结点个数为\(|T|\)，t是树T的叶结点，该叶结点有\(N_t\)个样本结点，其中k类的样本点有\(N_{tk}\)，k = 1, 2, …, K，\(H_t(T)\)为叶结点t上的经验熵，α≥0为参数，则决策树学习的损失函数可以定义为 :\(C_{\alpha}(T_t) = C(T_t) + \alpha|T_t|\) 其中经验熵:\(H_t(T)=-\sum_{i}^{K}\frac{|N_{ik}|}{N_i}log\frac{|N_{ik}|}{N_i}\) 在损失函数中，将第1项记作:\(C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}\) 则：\(C_\alpha(T)=C(T)+\alpha|T|\) C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，\(|T|\)表示模型的复杂度，参数\(\alpha\)控制两者之间的影响。较大的\(\alpha\)促使选择较简单的模型（树），较小的\(\alpha\)促使选择较复杂的模型（树）。\(\alpha=0\)意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 剪枝，就是当α确定时，选择损失函数最小的模型（树）。但α值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。 可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。 损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 决策树剪枝算法： 输入：生成算法产生的整个树T，参数\(\alpha\); 输出：修剪后的子树\(T_\alpha\) （1）计算每个结点的经验熵； （2）递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体树分别为\(T_B\)和\(T_A\)，其对应的损失函数值分别是\(C_\alpha(B)\)和\(C_\alpha(A)\)，如果\(C_\alpha(B)\le C_\alpha(A)\) 则进行剪枝，即父结点变为新的叶结点。 （3）返回（2），直至不能继续为止，得到损失函数最小的子树\(T_\alpha\)。 CART剪枝 相比一般剪枝算法，CART剪枝算法的优势在于，不用提前确定α值，而是在剪枝后从所有子树中找到最优子树对应的\(\alpha\)值。 对于固定的α值，一定存在让\(C_\alpha(T)\)最小的唯一的子树，记\(T_\alpha\)。 决策树小结 决策树算法对比 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类，回归 二叉树 基尼系数，均方差 支持 支持 支持 决策树算法的优缺点 优点： 决策树由明确的规则生成，可解释性强。 基本不需要预处理，不需要提前归一化，处理缺失值。 既可以处理离散值也可以处理连续值。 非参数模型，树模型在不同分布数据上表现更稳定。 可以交叉验证的剪枝来选择模型，从而提高泛化能力。(验证集(CART)也可以发挥调整模型的作用而不仅仅是评估模型) 缺点： 决策树算法非常容易过拟合，导致泛化能力不强。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。 参考： 李航. 统计学习方法 http://www.csuldw.com/2015/05/08/2015-05-08-decision%20tree/ https://applenob.github.io/decision_tree.html https://zealscott.com/posts/54670/]]></content>
      <categories>
        <category>机器学习算法</category>
        <category>树模型</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>CART</tag>
        <tag>ID3</tag>
        <tag>C4.5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用的内置函数]]></title>
    <url>%2F2018%2F07%2F27%2Fpython%E5%B8%B8%E7%94%A8%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[熟悉和掌握python的内置函数，可以在写算法的时候简化代码。 关键字 lambda表达式 :不定义函数名，使用一次的函数 1234&gt;&gt;&gt; a = [(1, 2), (4, 1), (9, 10), (13, -3)]#type :list a.sort(key=lambda x: x[1])#key的用法很高端，规则排序 print(a)&gt;&gt;&gt; [(13, -3), (4, 1), (1, 2), (9, 10)] yield函数:可构建生成器，对可迭代对象进行逐次返回，节约计算资源，如keras的 imagegenerator 1. 可迭代对象`iterable`是实现了`__iter__()`方法的对象，`iter()`方法返回`iterator`对象，通过`next`显示的获取元素,用完一个删一个，当迭代器为空时会抛出异常（StopIteration）（和c++迭代器 （`pop_front`+`iterator++`）理解类似），调用迭代器`for item in iterator:`。 1234567891011121314151617181920#如list是一个可迭代对象包含__iter__()方法，使用iter(list)变成迭代器iterator&gt;&gt;&gt;dir(list)['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']&gt;&gt;&gt; a = [1,2,3,4,5]&gt;&gt;&gt; type(a)list&gt;&gt;&gt; type(iter(a))list_iterator&gt;&gt;&gt; b = iter(a)&gt;&gt;&gt; next(b)1&gt;&gt;&gt; len(b)4#调用迭代器for item in iterator:...&gt;&gt;&gt; for item in b: print (item)2345 生成器generator就是带yield的函数，generator就是一种迭代器。yield相当于在迭代器迭代的时候加了个断点返回元素不终止执行，遇到下一个next()触发一次执行。 总结： 生成器迭代器关系图 可迭代对象(Iterable)是实现了__iter__()方法的对象,通过调用iter()方法可以获得一个迭代器(Iterator) 迭代器(Iterator)是实现了__iter__()和__next__()的对象 for ... in ...的迭代,实际是将可迭代对象转换成迭代器,再重复调用next()方法实现的 生成器(generator)是一个特殊的迭代器,它的实现更简单优雅. yield是生成器实现__next__()方法的关键.它作为生成器执行的暂停恢复点,可以对yield表达式进行赋值,也可以将yield表达式的值返回. 类型转换 tuple：根据传入的参数创建一个新的元组 1234&gt;&gt;&gt; tuple() #不传入参数，创建空元组()&gt;&gt;&gt; tuple('121') #传入可迭代对象。使用其元素创建新的元组('1', '2', '1') list：根据传入的参数创建一个新的列表 1234&gt;&gt;&gt;list() # 不传入参数，创建空列表[] &gt;&gt;&gt; list('abcd') # 传入可迭代对象，使用其元素创建新的列表['a', 'b', 'c', 'd'] dict：根据传入的参数创建一个新的字典 12345678&gt;&gt;&gt; dict() # 不传入任何参数时，返回空字典。&#123;&#125;&gt;&gt;&gt; dict(a = 1,b = 2) # 可以传入键值对创建字典。&#123;'b': 2, 'a': 1&#125;&gt;&gt;&gt; dict(zip(['a','b'],[1,2])) # 可以传入映射函数创建字典。&#123;'b': 2, 'a': 1&#125;&gt;&gt;&gt; dict((('a',1),('b',2))) # 可以传入可迭代对象创建字典。&#123;'b': 2, 'a': 1&#125; enumerate：根据可迭代对象创建枚举对象 1234567&gt;&gt;&gt; seasons = ['Spring', 'Summer', 'Fall', 'Winter']&gt;&gt;&gt; list(enumerate(seasons))[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]&gt;&gt;&gt; list(enumerate(seasons, start=1)) #指定起始值(range(0, 10), range(1, 10), range(1, 10, 3))(range(0, 10), range(1, 10), range(1, 10, 3))[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')] range：根据传入的参数创建一个新的range对象 1234567&gt;&gt;&gt; a = range(10)&gt;&gt;&gt; b = range(1,10)&gt;&gt;&gt; c = range(1,10,3)&gt;&gt;&gt; a,b,c # 分别输出a,b,c(range(0, 10), range(1, 10), range(1, 10, 3))&gt;&gt;&gt; list(a),list(b),list(c) # 分别输出a,b,c的元素([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 4, 7]) iter：根据传入的参数创建一个新的可迭代对象 12345678910111213141516&gt;&gt;&gt; a = iter('abcd') #字符串序列&gt;&gt;&gt; a&lt;str_iterator object at 0x03FB4FB0&gt;&gt;&gt;&gt; next(a)'a'&gt;&gt;&gt; next(a)'b'&gt;&gt;&gt; next(a)'c'&gt;&gt;&gt; next(a)'d'&gt;&gt;&gt; next(a)Traceback (most recent call last): File "&lt;pyshell#29&gt;", line 1, in &lt;module&gt; next(a)StopIteration 序列操作 all：判断可迭代对象的每个元素是否都为True值 12345678&gt;&gt;&gt; all([1,2]) #列表中每个元素逻辑值均为True，返回TrueTrue&gt;&gt;&gt; all([0,1,2]) #列表中0的逻辑值为False，返回FalseFalse&gt;&gt;&gt; all(()) #空元组True&gt;&gt;&gt; all(&#123;&#125;) #空字典True any：判断可迭代对象的元素是否有为True值的元素 12345678&gt;&gt;&gt; any([0,1,2]) #列表元素有一个为True，则返回TrueTrue&gt;&gt;&gt; any([0,0]) #列表元素全部为False，则返回FalseFalse&gt;&gt;&gt; any([]) #空列表False&gt;&gt;&gt; any(&#123;&#125;) #空字典False filter：使用指定方法过滤可迭代对象的元素 1234567&gt;&gt;&gt; a = list(range(1,10)) #定义序列&gt;&gt;&gt; a[1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; def if_odd(x): #定义奇数判断函数 return x%2==1&gt;&gt;&gt; list(filter(if_odd,a)) #筛选序列中的奇数[1, 3, 5, 7, 9] map：使用指定方法去作用传入的每个可迭代对象的元素，生成新的可迭代对象 12345&gt;&gt;&gt; a = str(map(list,['a','b','b','fd']))#每一个可迭代对象进行操作 b = list(['a','b','b','fd'])#对每个元素进行操作&gt;&gt;&gt; a,b[['a'], ['b'], ['b'], ['f', 'd']]['a', 'b', 'b', 'fd'] next：返回可迭代对象中的下一个元素值 1234567891011121314151617181920&gt;&gt;&gt; a = iter('abcd')&gt;&gt;&gt; next(a)'a'&gt;&gt;&gt; next(a)'b'&gt;&gt;&gt; next(a)'c'&gt;&gt;&gt; next(a)'d'&gt;&gt;&gt; next(a)Traceback (most recent call last): File "&lt;pyshell#18&gt;", line 1, in &lt;module&gt; next(a)StopIteration#传入default参数后，如果可迭代对象还有元素没有返回，则依次返回其元素值，如果所有元素已经返回，则返回default指定的默认值而不抛出StopIteration 异常&gt;&gt;&gt; next(a,'e')'e'&gt;&gt;&gt; next(a,'e')'e' reversed：反转序列生成新的可迭代对象 12345&gt;&gt;&gt; a = reversed(range(10)) # 传入range对象&gt;&gt;&gt; a # 类型变成迭代器&lt;range_iterator object at 0x035634E8&gt;&gt;&gt;&gt; list(a)[9, 8, 7, 6, 5, 4, 3, 2, 1, 0] sorted：对可迭代对象进行排序，返回一个新的列表 123456789&gt;&gt;&gt; a = ['a','b','d','c','B','A']&gt;&gt;&gt; a['a', 'b', 'd', 'c', 'B', 'A']&gt;&gt;&gt; sorted(a) # 默认按字符ascii码排序['A', 'B', 'a', 'b', 'c', 'd']&gt;&gt;&gt; sorted(a,key = str.lower) # 转换成小写后再排序，'a'和'A'值一样，'b'和'B'值一样['a', 'A', 'b', 'B', 'c', 'd'] zip：聚合传入的每个迭代器中相同位置的元素，返回一个新的元组类型迭代器 1234&gt;&gt;&gt; x = [1,2,3] #长度3&gt;&gt;&gt; y = [4,5,6,7,8] #长度5&gt;&gt;&gt; list(zip(x,y)) # 取最小长度3[(1, 4), (2, 5), (3, 6)] 对象操作 dir：返回对象或者当前作用域内的属性列表 12&gt;&gt;&gt; dir(list)#非常重要的list属性，leetcode写代码可以直接用！['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort'] type：返回对象的类型，或者根据传入的参数创建一个新的类型 12345678&gt;&gt;&gt; type(1) # 返回对象的类型&lt;class &apos;int&apos;&gt;#使用type函数创建类型D，含有属性InfoD&gt;&gt;&gt; D = type(&apos;D&apos;,(A,B),dict(InfoD=&apos;some thing defined in D&apos;))&gt;&gt;&gt; d = D()&gt;&gt;&gt; d.InfoD &apos;some thing defined in D&apos; len：返回对象的长度 12345678&gt;&gt;&gt; len('abcd') # 字符串&gt;&gt;&gt; len(bytes('abcd','utf-8')) # 字节数组&gt;&gt;&gt; len((1,2,3,4)) # 元组&gt;&gt;&gt; len([1,2,3,4]) # 列表&gt;&gt;&gt; len(range(1,5)) # range对象&gt;&gt;&gt; len(&#123;'a':1,'b':2,'c':3,'d':4&#125;) # 字典&gt;&gt;&gt; len(&#123;'a','b','c','d'&#125;) # 集合&gt;&gt;&gt; len(frozenset('abcd')) #不可变集合 变量操作 globals：返回当前作用域内的全局变量和其值组成的字典 12345&gt;&gt;&gt; globals()&#123;'__spec__': None, '__package__': None, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '__name__': '__main__', '__doc__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;&#125;&gt;&gt;&gt; a = 1&gt;&gt;&gt; globals() #多了一个a&#123;'__spec__': None, '__package__': None, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'a': 1, '__name__': '__main__', '__doc__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;&#125; global：定义全局变量 1234567891011121314&gt;&gt;&gt; def fg(): global a a=1&gt;&gt;&gt; fg()&gt;&gt;&gt; a&gt;&gt;&gt; 1&gt;&gt;&gt;def f(): a=1&gt;&gt;&gt; f()&gt;&gt;&gt; a&gt;&gt;&gt; Traceback (most recent call last): File "&lt;ipython-input-8-60b725f10c9c&gt;", line 1, in &lt;module&gt; a NameError: name 'a' is not defined locals：返回当前作用域内的局部变量和其值组成的字典(在调试函数的时候可以返回local) 123456789101112131415&gt;&gt;&gt; def f(): print('before define a ') print(locals()) #作用域内无变量 a = 1 print('after define a') print(locals()) #作用域内有一个a变量，值为1 b=a**2 return locals()&gt;&gt;&gt; f&lt;function f at 0x03D40588&gt;&gt;&gt;&gt; f()before define a &#123;&#125; after define a&#123;'a': 1, 'b': 1&#125; 交互操作 print：向标准输出对象打印输出 1234567&gt;&gt;&gt; print(1,2,3)1 2 3&gt;&gt;&gt; a = ['a', 'b', 'v', 'f', 's', 'a'] print(a,sep = '+')['a', 'b', 'v', 'f', 's', 'a']&gt;&gt;&gt; print(a,a,sep = '+',end = '=!')['a', 'b', 'v', 'f', 's', 'a']+['a', 'b', 'v', 'f', 's', 'a']=！ input：读取用户输入值 1234&gt;&gt;&gt; file = input('please enter your filename:')please input your name:cat.png&gt;&gt;&gt; file'cat.png' 文件操作 open：使用指定的模式和编码打开文件，返回文件读写对象 123456789# rb为二进制读,wb为二进制写操作&gt;&gt;&gt; f = open('test.txt','rt')&gt;&gt;&gt; f.read()'read test'&gt;&gt;&gt; f.close()#读操作: #read()将文本文件所有行读到一个字符串中。 #readline()是一行一行的读 #readlines()是将文本文件中所有行读到一个list中，文本文件每一行是list的一个元素。 参考： Python内置函数详解——总结篇 Python之生成器详解]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>list</tag>
        <tag>yield</tag>
        <tag>map</tag>
        <tag>zip</tag>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 排好序的数组合并求中位数(复杂度限制O(log (m+n)))]]></title>
    <url>%2F2018%2F07%2F25%2FLeetcode%20%E6%8E%92%E5%A5%BD%E5%BA%8F%E7%9A%84%E6%95%B0%E7%BB%84%E5%90%88%E5%B9%B6%E6%B1%82%E4%B8%AD%E4%BD%8D%E6%95%B0(%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%99%90%E5%88%B6O(log%20(m%2Bn)))%2F</url>
    <content type="text"><![CDATA[Leetcode 第四题 - There are two sorted arrays nums1 and nums2 of size m and n respectively. 1- Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). Solution:思路：将俩个数组合并成sorted数组，找中位数 123456789101112131415161718192021222324252627282930313233343536373839'''俩个数组排序合并，找中位数'''class Solution: def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ mergnums=[] i = 0 j = 0 print (len(nums1),len(nums2)) nums1 = list(nums1) nums2 = list(nums2) while True: if nums1!=[] and nums2 !=[]: if nums1[i]&lt;=nums2[j]: mergnums.append(nums1[i]) del nums1[i] else: mergnums.append(nums2[j]) del nums2[j] print (j) elif nums1 !=[] and nums2 ==[]: mergnums.append(nums1[i]) del nums1[i] elif nums1 ==[] and nums2 !=[]: mergnums.append(nums2[j]) del nums2[j] else: break if (len(mergnums))%2 ==0: median = mergnums[int((len(mergnums)-2)/2)] + mergnums[int(len(mergnums)/2)] median = median/2.0 else: median = mergnums[int(len(mergnums)/2)] return median ​ ​]]></content>
      <categories>
        <category>-Leetcode</category>
      </categories>
      <tags>
        <tag>-合并数组求中位数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前言]]></title>
    <url>%2F2018%2F07%2F23%2F%E5%89%8D%E8%A8%80%2F</url>
    <content type="text"><![CDATA[希望自己能有这个博客来鼓励自己养成总结归纳的好习惯！工具性代码还得review自己写的代码！ 向大佬们学习！ 博客从找工作的角度展开，从算法，算法编程练习俩个角度开始写博客。 算法： 机器学习基础： [ ] KNN [ ] Kmeans [ ] SVM [ ] 朴素贝叶斯 [ ] LR [ ] RF [x] GDBT [x] Adaboost [ ] LightGBM [ ] Xgboost 深度学习基础： [x] CNN 原理与架构发展 [ ] 反向传播推导 [ ] 正则化方法 [ ] 损失函数 [ ] CNN复杂度计算与模型压缩 [ ] SSD [ ] Retinanet [ ] YOLO系列 [ ] RCNN系列 深度学习优化方法： [ ] 梯度下降优化算法 项目方法总结： [ ] CTR预估算法 [ ] Multi-label imbalance Classification [ ] Zero-shot learing 编程相关 以后工作作为技术积累的记录 快乐Farm每一天！]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>题记</tag>
      </tags>
  </entry>
</search>
