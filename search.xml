<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[keras回调函数]]></title>
    <url>%2F2018%2F12%2F26%2Fkeras%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[回调函数在模型训练和模型学习结果分析中有非常重要的作用，如动态学习率调整，早停，tensorboard可视化分析模型训练过程。 Tensorboard可视化 1234567891011from keras.callbacks import TensorBoardtensorboard_k = TensorBoard(log_dir='./logs', # log 目录 histogram_freq=1, # 画直方图的频率 batch_size=32, # 用多大量的数据计算直方图 write_graph=True, # 是否存储网络结构图 write_grads=False, # 是否可视化梯度直方图 write_images=False,# 是否可视化权重 embeddings_freq= 0, #嵌入输出可视化 embeddings_layer_names=None, #可视化层的name embeddings_metadata=None) #metadata的通俗理解是嵌入的标签callbacks = [tensorboard_k] 嵌入层效果图： t_sne 编写自己的可视化回调函数示例—PR曲线 代码参考 123456789101112131415161718192021222324252627282930313233343536373839from keras.callbacks import TensorBoardfrom tensorboard.plugins.pr_curve import summary as pr_summaryclass PRTensorBoard(TensorBoard): def __init__(self, *args, **kwargs): # One extra argument to indicate whether or not to use the PR curve summary. self.pr_curve = kwargs.pop('pr_curve', True) super(PRTensorBoard, self).__init__(*args, **kwargs)#super用于多重继承可自行百度其用法 global tf import tensorflow as tf def set_model(self, model): super(PRTensorBoard, self).set_model(model) if self.pr_curve: # Get the prediction and label tensor placeholders. predictions = self.model._feed_outputs[0] labels = tf.cast(self.model._feed_targets[0], tf.bool) # Create the PR summary OP. self.pr_summary = pr_summary.op(name='pr_curve', predictions=predictions, labels=labels, display_name='Precision-Recall Curve') def on_epoch_end(self, epoch, logs=None): super(PRTensorBoard, self).on_epoch_end(epoch, logs) if self.pr_curve and self.validation_data: # Get the tensors again. tensors = self.model._feed_targets + self.model._feed_outputs # Predict the output. predictions = self.model.predict(self.validation_data[:-2]) # Build the dictionary mapping the tensor to the data. val_data = [self.validation_data[-2], predictions] feed_dict = dict(zip(tensors, val_data)) # Run and add summary. result = self.sess.run([self.pr_summary], feed_dict=feed_dict) self.writer.add_summary(result[0], epoch) self.writer.flush() 效果图展示： PR曲线 ModelCheckpoint 1234keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)#在每个训练期之后保存模型。#filepath 可以包括命名格式选项，可以由 epoch 的值和 logs 的键（由 on_epoch_end 参数传递）来填充。#例如：如果 filepath 是 weights.&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;.hdf5， 那么模型被保存的的文件名就会有训练轮数和验证损失。 EarlyStopping 123456keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')#monitor: 被监测的数据。#min_delta: 在被监测的数据中被认为是提升的最小变化， 例如，小于 min_delta 的绝对变化会被认为没有提升。#patience: 没有进步的训练轮数，在这之后训练就会被停止。#verbose: 详细信息模式。#mode: &#123;auto, min, max&#125; 其中之一。 在 min 模式中， 当被监测的数据停止下降，训练就会停止；在 max 模式中，当被监测的数据停止上升，训练就会停止；在 auto 模式中，方向会自动从被监测的数据的名字中判断出来。 LearningRateScheduler 123keras.callbacks.LearningRateScheduler(schedule, verbose=0)#schedule: 一个函数，接受轮索引数作为输入（整数，从 0 开始迭代） 然后返回一个学习速率作为输出（浮点数）。#verbose: 整数。 0：安静，1：更新信息。 Scheduler示例 123456789101112131415161718192021222324callbacks = []class Schedule: def __init__(self, nb_epochs, initial_lr): self.epochs = nb_epochs self.initial_lr = initial_lr def __call__(self, epoch_idx): if epoch_idx &lt; self.epochs * 0.1: return self.initial_lr elif epoch_idx &lt; self.epochs * 0.200: return self.initial_lr * 0.1 elif epoch_idx &lt; self.epochs * 0.3: return self.initial_lr * 0.005 return self.initial_lr * 0.0005 callbacks.append(LearningRateScheduler(schedule=Schedule(nb_epochs, lr))) 创建自定义回调函数 简单的创建使用LambdaCallback 12345678910111213141516171819202122232425keras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None)在训练进行中创建简单，自定义的回调函数的回调函数。这个回调函数和匿名函数在合适的时间被创建。 需要注意的是回调函数要求位置型参数，如下：on_epoch_begin 和 on_epoch_end 要求两个位置型的参数： epoch, logson_batch_begin 和 on_batch_end 要求两个位置型的参数： batch, logson_train_begin 和 on_train_end 要求一个位置型的参数： logs参数：on_epoch_begin: 在每轮开始时被调用。on_epoch_end: 在每轮结束时被调用。on_batch_begin: 在每批开始时被调用。on_batch_end: 在每批结束时被调用。on_train_begin: 在模型训练开始时被调用。on_train_end: 在模型训练结束时被调用。 12#示例 在每一个批开始时，打印出批数。batch_print_callback = LambdaCallback(on_batch_begin=lambda batch,logs: print(batch)) 通过继承回调函数基类定义回调函数 1234567891011121314151617181920#在训练时，保存一个列表的批量损失值：class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs=&#123;&#125;): self.losses = [] def on_batch_end(self, batch, logs=&#123;&#125;): self.losses.append(logs.get('loss')) history = LossHistory()model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history])print(history.losses)# 输出'''[0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789]''' Reference 所有参考均以贴上链接]]></content>
      <categories>
        <category>Keras</category>
        <category>深度学习框架</category>
      </categories>
      <tags>
        <tag>keras-20mins</tag>
        <tag>keras-回调函数</tag>
        <tag>keras-tensorboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras数据增强]]></title>
    <url>%2F2018%2F12%2F25%2Fkeras%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%2F</url>
    <content type="text"><![CDATA[本文重点讲述了ImageDataGenerator类的应用与测试；在理解该类的原理后编写俩种生成器作为示例方便未来适应多种场景下的数据增强。 ImageDataGenerator参数初始化 参考Keras中文手册其初始化如下： 1234567891011121314151617181920#ImageDataGenerator是 数据扩增类；下面代码即是类的初始化keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,#布尔值，使输入数据集去中心化（均值为0）, 按feature执行 samplewise_center=False,#布尔值，使输入数据的每个样本均值为0 featurewise_std_normalization=False,#布尔值，将输入除以数据集的标准差以完成标准化, 按feature执行 samplewise_std_normalization=False,#布尔值，将输入的每个样本除以其自身的标准差 zca_whitening=False,#布尔值，对输入数据施加ZCA白化 zca_epsilon=1e-6,# ZCA使用的eposilon，默认1e-6 rotation_range=0,#整数，数据提升时图片随机转动的角度 width_shift_range=0,#浮点数，图片宽度的某个比例，数据提升时图片水平偏移的幅度 height_shift_range=0,#浮点数，图片高度的某个比例，数据提升时图片竖直偏移的幅度 shear_range=0,#浮点数，剪切强度（逆时针方向的剪切变换角度） zoom_range=0,#浮点数或形如[lower,upper]的列表，随机缩放的幅度，若为浮点数，则相当于[lower,upper] = [1 - zoom_range, 1+zoom_range] channel_shift_range=0,#浮点数，随机通道偏移的幅度 fill_mode="nearest",#‘constant’，‘nearest’，‘reflect’或‘wrap’之一，当进行变换时超出边界的点将根据本参数给定的方法进行处理 cval=0.,#浮点数或整数，当fill_mode=constant时，指定要向超出边界的点填充的值 horizontal_flip=False,#布尔值，进行随机水平翻转 vertical_filp=False,#布尔值，进行随机竖直翻转 rescale=None,#重放缩因子,默认为None. 如果为None或0则不进行放缩,否则会将该数值乘到数据上(在应用其他变换之前) preprocessing_function=None,#将被应用于每个输入的函数。该函数将在任何其他修改之前运行。该函数接受一个参数，为一张图片（秩为3的numpy array），并且输出一个具有相同shape的numpy array data_format=K.image_data_format())#字符串，“channel_first”或“channel_last”之一，代表图像的通道维的位置。 图像仿射变换对应的矩阵操作 数据扩增示例 下载猫狗大战数据集 划分数据集参考我的博客python小工具 123456789101112131415161718192021222324252627#建立data文件夹，如下树状结构(不要包含其他文件夹)data/ train/ dog/ cat/ val/ dog/ cat/import osimport shutildog_iamge_list = os.listdir('./data/Dog/')cat_iamge_list = os.listdir('./data/Cat/')dog_nums = len(dog_iamge_list)#狗狗图片数目 cat_nums = len(cat_iamge_list)#...train_path = './data/train/'val_path = './data/val/'for idx,img in enumerate(dog_iamge_list): if idx &lt; 1000: shutil.move('./data/Dog/'+img,'./data/'+'train/dog/') #移动文件到文件夹 else:#可以稍微设置下验证集的数量，不然训练测试速度比较慢 shutil.move('./data/Dog/'+img,'./data/'+'val/dog/') for idx,img in enumerate(cat_iamge_list): if idx &lt; 1000: shutil.move('./data/Cat//'+img,'./data/'+'train/cat/') else: shutil.move('./data/Cat//'+img,'./data/'+'val/cat/') 测试数据增强效果 123456789101112131415161718192021#测试数据增强效果from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_imgdatagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')img = load_img('./data/train/cat/1.jpg') # 导入数据x = img_to_array(img) # 转成Numpy array格式 (3,150,150)x = x.reshape((1,) + x.shape) # Reshape为 (1, 3, 150, 150)# datagen.flow() 生成器 参考我前面的博客，用for分批读取i = 0for batch in datagen.flow(x, batch_size=1,save_to_dir='./test_gen_img', save_prefix='cat', save_format='jpeg'): i += 1 if i &gt; 17: break 原图 生成图像 使用ImageDataGenerator进行实战 12#build model见前面教程：本例是二分利之一输出层的激活函数选择sigmoid 12345678910111213141516171819202122#构建生成器batch_size = 16train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)#测试集只需要和训练机一样rescaletest_datagen = ImageDataGenerator(rescale=1./255)#train或者val的每个子目录表示一个类#训练集生成器train_generator = train_datagen.flow_from_directory( './data/train', # 训练集路径 target_size=(150, 150), # 图片尺寸，自动resize batch_size=batch_size, class_mode='binary') # 标签模式#验证集生成器validation_generator = test_datagen.flow_from_directory( './data/val', target_size=(150, 150), batch_size=batch_size, class_mode='binary') 12345678#训练模型#win10可能会遇到不能识别的图片，直接删除model.fit_generator( train_generator, steps_per_epoch=2000 // batch_size, epochs=50, validation_data=validation_generator, validation_steps=800 // batch_size) 编写自定义生成器 总结一下就是继承keras.utils.Sequence基类，重写__getitem__函数，[示例](https://github.com/yu4u/noise2noise)如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from keras.utils import Sequenceclass NoisyImageGenerator(Sequence):#继承Sequence基类 def __init__(self, image_dir, source_noise_model, target_noise_model, batch_size=32, image_size=64): self.image_paths = list(Path(image_dir).glob("*.jpg")) self.source_noise_model = source_noise_model self.target_noise_model = target_noise_model self.image_num = len(self.image_paths) self.batch_size = batch_size self.image_size = image_size def __len__(self):#重写len返回函数 return self.image_num // self.batch_size def __getitem__(self, idx):#重写getitem，返回数据批 batch_size = self.batch_size image_size = self.image_size x = np.zeros((batch_size, image_size, image_size, 3), dtype=np.uint8) y = np.zeros((batch_size, image_size, image_size, 3), dtype=np.uint8) sample_id = 0 while True: image_path = random.choice(self.image_paths) image = cv2.imread(str(image_path)) h, w, _ = image.shape if h &gt;= image_size and w &gt;= image_size: h, w, _ = image.shape i = np.random.randint(h - image_size + 1) j = np.random.randint(w - image_size + 1) clean_patch = image[i:i + image_size, j:j + image_size] x[sample_id] = self.source_noise_model(clean_patch) y[sample_id] = self.target_noise_model(x[sample_id]) # y[sample_id] = self.target_noise_model(clean_patch) sample_id += 1 if sample_id == batch_size: return x, yclass ValGenerator(Sequence):# def __init__(self, image_dir, val_noise_model):#在初始化函数里面处理完所有数据再在__getitem__返回，加载数据效率较低 image_paths = list(Path(image_dir).glob("*.*")) self.image_num = len(image_paths) self.data = [] for image_path in image_paths: y = cv2.imread(str(image_path)) h, w, _ = y.shape y = y[:(h // 16) * 16, :(w // 16) * 16] # for stride (maximum 16) x = val_noise_model(y) self.data.append([np.expand_dims(x, axis=0), np.expand_dims(y, axis=0)]) def __len__(self): return self.image_num def __getitem__(self, idx): return self.data[idx] Reference Keras Blog 使用超小数据集构建模型 建议完成该博客所有代码分析 其他参考均以贴上连接]]></content>
      <categories>
        <category>Keras</category>
        <category>深度学习框架</category>
      </categories>
      <tags>
        <tag>keras-20mins</tag>
        <tag>keras自定义生成器</tag>
        <tag>keras-数据增强</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras简明教程]]></title>
    <url>%2F2018%2F12%2F23%2Fkeras%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本教程适用于Keras的入门与提升。本教程所用Keras版本为2.1.0,backend是Tensorflow,Keras的详细内容应该结合Keras官方文档和源代码一同学习。目前Tensorflow 1.9.0及以上版本将Keras集成为tf.keras，本教程将不对此项进行扩展。 我认为Keras是个轻量级的高级API，学习的捷径就是尽可能的阅读其源码。当然这对于初学者难度也不是很大。Keras的很多功能函数的源码复杂度并不高，功能类耦合性弱，可学习性强，可扩展性灵活。可以将其部分功能模块剥离成独立的工具，对算法原理的理解都有一定的帮助。 使用Keras 构建模型 Keras设计了俩种构建模型的方式函数式模型API和顺序式模型API官网称30s入门。 顺序式模型API构建模型示例： 1234567from keras.models import Sequential#从models导入Sequentialmodel = Sequential()from keras.layers import Densemodel.add(Dense(units=64, activation='relu', input_dim=100))model.add(Dense(units=10, activation='softmax'))model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])model.fit(data,labels, batch_size=32, nb_epoch=10, verbose=1) 函数式模型API构建模型示例： 1234567891011121314from keras.layers import Input, Densefrom keras.models import Model# 这部分返回一个张量inputs = Input(shape=(784,))# 层的实例是可调用的，它以张量为参数，并且返回一个张量x = Dense(64, activation='relu')(inputs)x = Dense(64, activation='relu')(x)predictions = Dense(10, activation='softmax')(x)# 这部分创建了一个包含输入层和三个全连接层的模型model = Model(inputs=inputs, outputs=predictions)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(data, labels) # 开始训练 模型编译参数设置 主要配置包括： loss：损失函数–MSE、KL散度、交叉熵、对数损失函数等；源码：losses.py optimizer：优化函数–常用SGD、RMSprop、Adam等;源码：optimizers.py metrics：模型评估方法–准确率、AUC、F-score、包括损失函数代表的指标；源码：metrics.py Tricks: 自定义评估函数 123456789101112131415#可以参照metrics.py定义评估函数的写法编写自己的评估函数#F-Scoredef fbeta(y_true, y_pred, beta = 1, threshold_shift=0): # 枝剪预测标签的范围 y_pred = K.clip(y_pred, 0, 1) # 调整阈值 y_pred_bin = K.round(y_pred + threshold_shift) tp = K.sum(K.round(y_true * y_pred_bin), axis=1) + K.epsilon() fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)), axis=1) fn = K.sum(K.round(K.clip(y_true - y_pred_bin, 0, 1)), axis=1) precision = tp / (tp + fp) recall = tp / (tp + fn) beta_squared = beta ** 2 return K.mean((beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())) 自定义损失函数 1234567#参考losses.py编写自己的损失函数#Retinanet 针对正负样本不均衡问题设计的损失函数focal_loss#Keras 以tf为后端；可以使用Keras的math方法或者tf的math方法写都可以keras被识别def focal_loss(y_true, y_pred,gamma=2., alpha=.25): pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred)) return K.mean(-alpha * K.pow(1 - pt_1, gamma) * K.log(pt_1)-(1-alpha) * K.pow( pt_0, gamma) * K.log(1 - pt_0),axis=-1) 多任务(多头)训练编译配置 123#定义模型的输出是：sigmoid_1,sigmoid_2model.compile(loss=&#123;'sigmoid_1':"binary_crossentropy",'sigmoid_2':"mse" &#125;, loss_weights=&#123;'sigmoid_1':0.5,'sigmoid_2':0.5&#125;,optimizer=opt,metrics=&#123;'Normout':f2_score,'FMlayer':f2_score&#125;) 模型训练参数设置 模型训练接口 以model.fit为例： 1model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None) 12345678910111213141516171819#重点查看 注释部分高亮字参数:x: 训练数据的 Numpy 数组（如果模型只有一个输入）， 或者是 Numpy 数组的列表（如果模型有多个输入）。 如果模型中的输入层被命名，你也可以传递一个字典，将输入层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，x 可以是 None（默认）。y: 目标（标签）数据的 Numpy 数组（如果模型只有一个输出）， 或者是 Numpy 数组的列表（如果模型有多个输出）。 如果模型中的输出层被命名，你也可以传递一个字典，将输出层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，y 可以是 None（默认）。batch_size: 整数或 None。每次梯度更新的样本数。如果未指定，默认为 32。epochs: 整数。训练模型迭代轮次。一个轮次是在整个 x 和 y 上的一轮迭代。 请注意，与 initial_epoch 一起，epochs 被理解为 「最终轮次」。模型并不是训练了 epochs 轮，而是到第 epochs 轮停止训练。verbose: 0, 1 或 2。日志显示模式。 0 = 安静模式, 1 = 进度条, 2 = 每轮一行。#callbacks: 一系列的 keras.callbacks.Callback 实例。一系列可以在训练时使用的回调函数。 详见 callbacks。validation_split: 在 0 和 1 之间浮动。用作验证集的训练数据的比例。 模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。 验证数据是混洗之前 x 和y 数据的最后一部分样本中。validation_data: 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights)， 用来评估损失，以及在每轮结束时的任何模型度量指标。 模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split。shuffle: 布尔值（是否在每轮迭代之前混洗数据）或者 字符串 (batch)。 batch 是处理 HDF5 数据限制的特殊选项，它对一个 batch 内部的数据进行混洗。 #当 steps_per_epoch 非 None 时，这个参数无效。#class_weight: 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。 这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。#sample_weight: 训练样本的可选 Numpy 权重数组，用于对损失函数进行加权（仅在训练期间）。 您可以传递与输入样本长度相同的平坦（1D）Numpy 数组（权重和样本之间的 1:1 映射）， 或者在时序数据的情况下，可以传递尺寸为 (samples, sequence_length) 的 2D 数组，以对每个样本的每个时间步施加不同的权重。 在这种情况下，你应该确保在 compile() 中指定 sample_weight_mode="temporal"。#initial_epoch: 整数。开始训练的轮次（有助于恢复之前的训练）。steps_per_epoch: 整数或 None。 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。 使用 TensorFlow 数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为 1。validation_steps: 只有在指定了 steps_per_epoch时才有用。停止前要验证的总步数（批次样本）。返回一个 History 对象。其 History.history 属性是连续 epoch 训练损失和评估值，以及验证集损失和评估值的记录（如果适用）。#训练函数的返回是一个字典；训练的损失和评估值都在这个字典里面；训练参数可视化可用pyplot model.train_on_batch 12train_on_batch(x, y, sample_weight=None, class_weight=None)#以batch的形式训练数据，相当于tf的feed_dict，一次训练一个step model.fit_generator 12model.fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)#以生成器的形式喂数据，节省内存，以及兼容实时数据扩增 模型预测 model.predict 1predict(x, batch_size=None, verbose=0, steps=None) 保存和加载模型 保存和加载整个模型 12345from keras.models import load_modelmodel.save("x_model.hdf5") #创建一个文件my_model.hdf5del model #删除模型#返回一个已经编译过的模型，和之前的模型一模一样model = load_model("x_model.hdf5") 只保存/加载权重 1234#使用hdf5来只加载权重(需要先安装HDF5和h5py)model.save_weights("x_model_weights.h5")#定义好模型之后，加载权重model.load_weights("x_model_weights.h5") Keras模型中间阶段 获取中间层输出 12345from keras.models import Modelmodel = Model(inputs=inp,outputs=y_hat)layer_name = "Conv2D_1"Middle_layer_out_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)Middle_layer_out = Middle_layer_out_model.predict(x) 固定层参数 1234567891011#固定层的参数是否可训练在微调和迁移学习中是很关键的一步#可以定义层以后再固定a = Conv1D(1,2)a.trainable = False#也可以在定义层的时候固定参数layer = Conv1D(1,2,trainable = False)#fine tune参数设置for layer in base_model.layers[:25]: layer.trainable = Falsesgd = Adam(lr=1e-3, decay=lr/10)model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['acc']) 总结 本章主要讲述了Keras是如何构建模型，如何设置优化算法，评估函数，训练参数的基本使用方法；在此基础上扩展了多任务学习，获取中间层参数，fine tune方法等。本教程结合keras 案例 运行其部分代码对学习Keras帮助巨大。 Reference 本文所有参考均来自Keras官方手册]]></content>
      <categories>
        <category>Keras</category>
        <category>深度学习框架</category>
      </categories>
      <tags>
        <tag>keras-20mins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络模型简述]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNet，图像分割中的FCN。本文将以LeNet简单介绍卷积基础架构开始，以讲述Blocks演变的形式简单讲解不同网络的特点。 卷积模型发展 卷机神经网络基础架构 典型的神经网络由一个输入层，多个隐藏层和一个输出层组成，在卷积神经网络中称为全连接。对于每一层的输出：\(y_i=f(W_{i}x +b)，其中f(x)为激活函数\)；其经典结构如下图所示： 神经网络结构 卷积神经网络的基础结构如下图所示： 完整的卷积结构 卷积神经网络采用的权值共享，相对与全连接操作，通常减少了数量级的参数；此外，卷积神经网络的局部操作，在处理具有局部相关性的任务时，与全连接相比具有明显的优势。 TODO: 卷积神经网络的构成成分： [x] 卷积层 [x] 池化层 [x] 全连接层（FCN和输出层前是Global_pooling的架构没有） 激活函数 Batch Nomalization(BN层) 局部响应归一化（LRN） Dropout层 LeNet5 LeNet5架构是一个开创性的工作。图像特征是全局和局部的统一，LeNet5利用一组相同的卷积核在特征图的一个通道上对全局特征进行滤波处理，同时融合局部特征，利用下采样压缩局部的相似特征。当时没有GPU来帮助训练，甚至CPU速度都非常慢。因此，对比使用每个像素作为一个单独的输入的多层神经网络，LeNet5能够节省参数和计算是一个关键的优势。LeNet5论文中提到，全连接不应该被放在第一层，因为图像中有着高度的空间相关性，并利用图像各个像素作为单独的输入特征不会利用这些相关性。因此有了CNN的三个特性了：1.局部感知、2.平移不变性（下采样）、3.权值共享。 LeNet-5 AlexNet AlexNet特点： 由五层卷积和三层全连接组成，输入图像为三通道 224x224 大小，网络规模远大于 LeNet-5 使用了 ReLU 激活函数，提高了训练速度 使用了 Dropout，可以作为正则项防止过拟合，提升模型鲁棒性 加入了局部响应归一化（LRN）提高了精度 引入最大池化 在训练时使用了分组卷积操作（参见《卷积神经网络（上）》） 一些很好的训练技巧，包括数据增广、学习率策略、weight decay 等 AlexNet架构： AlexNet Ng-AlexNet VGG VGG是一种更简单的架构模型，因为它没有使用太多的超参数。它总是使用3 x 3滤波器，在卷积层中步长为1，并使用SAME填充在2 x 2池中，步长为2。 VGG使用3*3卷积核级联提高感受野，一改LeNet-5和AlexNet的卷积-池化结构。 VGG模型更深，参数更少，后续的模型基本都遵循了小卷积核级联的设计风格。 VGG 123456# Keras APIdef block2_conv(x): x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x) x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x) x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x) return x GoogLeNet 2014年ILSVRC的获胜者提出GoogLeNet架构也称为InceptionV1模型。它在具有不同感受野大小的并行路径中更深入，并且降低了Top5错误率到6.67％。该架构由22层深层组成。它将参数数量从6000万（AlexNet）减少到500万。GoogLeNet在使用了中间输出做了多任务学习，防止网络过深造成的梯度消失的情况；每层使用多个卷积核。 GoogLeNet-inception Inception V2 Iception V2 主要提出了Batch Nomalization(BN)，BN层对mini-batch内部数据进行标准化，再给标准化数据乘以权重和加bias，保证了模型可以学习回原来的分布。用了BN层后减少或者取消 LRN。 [ ] 后续有时间再写一个博客专门写深度学习的正则化， 可以关注深度学习中的Nomalization中的分析。 Inception V3 Inception V3将一个较大的二维卷积拆成两个较小的一维卷积，比如将7x7卷积拆成1x7卷积和7x1卷积，或者将3x3卷积拆成1x3卷积和3x1卷积，另外也使用了将5x5 用两个 3x3 卷积替换，7x7 用三个 3x3 卷积替换，如下图所示。一方面节约了大量参数，加速运算并减轻了过拟合，同时增加了一层非线性扩展模型表达能力。论文中指出，这种非对称的卷积结构拆分，其结果比对称地拆为几个相同的小卷积核效果更明显，可以处理更多、更丰富的空间特征，增加特征多样性。示意图如下： 1x3卷积核代替3x3卷积核 使用一维卷积核代替二维卷积核 ResNet ILSRVC 2015的获胜者，被何凯明大神称为残差网络（ResNet）。该架构引入了一个名为“skip connections”的概念。ResNet采用Shortcut单元实现信息的跨层流通。与Inception 模块的Concat操作不同，Residual 模块使用 Add 操作完成信息融合。 shotcut 通过引入直连，原来需要学习完全的重构映射，从头创建输出，并不容易，而引入直连之后，只需要学习输出和原来输入的差值即可，绝对量变相对量，容易很多，所以叫残差网络。并且，通过引入残差，identity 恒等映射，相当于一个梯度高速通道，可以容易地训练避免梯度消失的问题，所以可以得到很深的网络，网络层数由 GoogLeNet 的 22 层到了ResNet的 152 层。然而，恒等函数与\(H_{\ ell}\)的输出是通过求和组合，这可能阻碍网络中的信息流。 keras_applications实现的Identity Block: 123456789101112131415161718192021222324252627282930313233343536373839404142def identity_block(input_tensor, kernel_size, filters, stage, block): """ The identity block is the block that has no conv layer at shortcut. # Arguments input_tensor: input tensor kernel_size: default 3, the kernel size of middle conv layer at main path filters: list of integers, the filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names # Returns Output tensor for the block. """ filters1, filters2, filters3 = filters if backend.image_data_format() == 'channels_last': bn_axis = 3 else: bn_axis = 1 conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = layers.Conv2D(filters1, (1, 1), kernel_initializer='he_normal', name=conv_name_base + '2a')(input_tensor) x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x) x = layers.Activation('relu')(x) x = layers.Conv2D(filters2, kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base + '2b')(x) x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x) x = layers.Activation('relu')(x) x = layers.Conv2D(filters3, (1, 1), kernel_initializer='he_normal', name=conv_name_base + '2c')(x) x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x) x = layers.add([x, input_tensor]) x = layers.Activation('relu')(x) return x ResNet-34 的网络结构如下所示： ResNet-34 不同的ResNet结构对比： resnet结构 DenseNet 说到DenseNet全文就俩公式，当时我还觉得我咋写不出来这样的论文~~。 先预览下DenseNet论文里的DenseBlocks: DenseBlock ResNet：传统的前馈卷积神经网络将第+1层的输入，可表示为：\(x_{\ell} = H_{\ell}(x_{\ell-1})\)。ResNet添加了一个跳连接，即使用恒等函数跳过非线性变换： \[ x_{\ell} = H_{\ell}(x_{\ell-1}) + x_{\ell-1} \] 密集连接：为了进一步改善层之间的信息流，我们提出了不同的连接模式：我们提出从任何层到所有后续层的直接连接。因此，第\(x_{0},…,x_{\ell-1}\)作为输入: \[ x_{\ell} = H_{\ell}([x_{0},x_{1},...,x_{\ell-1}]) \] 如果说Iception ResNet V2是Inception 思想融合了ResNet，那么我认为 DenseNet就是 Residual 思想（信息跨层流通）结合了Inception 理念。 densenet DenseNet的几个重要参数： 增长率：如果每个\(H_{\ell}\)层输出K个特征图，那么第\(\ell\)层输出\(K_{0}+K(\ell-1)\)个特征图，其中\(K_{0}\)是输入层的通道数。 瓶颈层：即\(1\times1\)卷积层。每一层输出\(K_{0}+K(\ell-1)\)个，理论上将每个输出为个Feature Maps，理论上将每个Dense Block输出为K_{0}+_{1}^{}K(i-1)$个Fature Maps，。\(1\times1\)卷积层的作用是将一个Dense Block的特征图压缩到\(K_{0}+K\ell\)个。 压缩：为了进一步提高模型的紧凑性，我们可以在过渡层减少特征图的数量。。作者选择压缩率（theta）为0.5。包含Bottleneck Layer的叫DenseNet-B，包含压缩层的叫DenseNet-C，两者都包含的叫DenseNet-BC keras_applications实现的密集连接模块 1234567891011121314def dense_block(x, blocks, name): """ A dense block. # Arguments x: input tensor. blocks: integer, the number of building blocks. name: string, block label. # Returns output tensor for the block. """ for i in range(blocks): x = conv_block(x, 32, name=name + '_block' + str(i + 1)) #每个conv_blocks的输出是[x,conv_block(x, 32, name=name + '_block' + str(i + 1))] return x SENet SENet(Squeeze-and-Excitation Networks)是基于特征通道之间的关系提出的，下图是SENet的Block单元，图中的Ftr是传统的卷积结构，X和U是Ftr的输入和输出，这些都是以往结构中已存在的。SENet增加的部分是U后的结构：对U先做一个Global Average Pooling（称为Squeeze过程），输出是一个1x1xC的数据，再经过两级全连接（称为Excitation过程），最后用sigmoid把输出限制到[0，1]的范围，把这个值作为scale再乘到U的C个通道上，作为下一级的输入数据。这种结构的原理是想通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。 SENet 其实SENet就是通道级attention机制。 由Tensorflow 实现的SE模块： 1234567891011def Squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name): with tf.name_scope(layer_name) : squeeze = Global_Average_Pooling(input_x) excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1') excitation = Relu(excitation) excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2') excitation = Sigmoid(excitation) excitation = tf.reshape(excitation, [-1,1,1,out_dim]) scale = input_x * excitation return scale 总结： 卷积神经网络设计： 从防止梯度消失出发 从信息流通出发 从多尺度特征图融合出发 从通道级或者像素级特征选择出发 Reference LeNet AlexNet Network-in-network VGG GoogleNet Inception V3 Batch-normalized ResNet Hu J, Shen L, Sun G. Squeeze-and-Excitation Networks[J]. 2017. https://chenzomi12.github.io/2016/12/13/CNN-Architectures/]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
        <tag>Inception</tag>
        <tag>SENet</tag>
        <tag>ShuffleNet</tag>
        <tag>MobileNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[很多人在接触卷积神经网络之前就接触过卷积运算，比如信号与系统的连续信号卷积和离散信号卷积。本文将我的理解出发对比卷积神经网络的卷积和我们信号卷积的区别与联系。介绍卷积神经网络中的卷积操作，以及不同的卷积策略。为后续深入高效小网络及模型压缩相关概念和原理做准备。 信号卷积 卷积表征函数f与经过翻转和平移的g的乘积函数所围成的曲边梯形的面积。信号与系统中，采样信号经过系统响应得到系统输出。一维卷积的数学表达： \[ 离散卷积：z[n]=(x * y)[n] = \sum_{m = -\infty}^{+\infty}x[m]\cdot y[n - m]\\连续卷积： y(x)=(f * h)(t) = \int_{-\infty}^{+\infty}f(\tau)\cdot h(t - \tau)\mathop{}\!\mathrm{d}\tau \] 我们采集到的信号通常是离散的，因此，后续讲解的就是离散卷积。看下维基百科的卷积可视化。 离散卷积3 一维离散卷积比较好理解，每个输出元素都是卷积核（系统响应）和信号的局部乘积的和。卷积操作是一种滤波操作，信号与系统的高通滤波，低通滤波等，因此，通过卷积操作可以获得某个方面属性的增强。一维离散卷积通常是在时域卷积，图像，激光雷达回波等属于空间属性。 CNN的卷积 卷积计算 以图像为例。图像卷积即二维离散卷积，图像的矩阵表示为\(h*w\)，元素数值大小为无符号整型通常在[0,255]之间。二维离散卷积的公式定义： \[ S[i, j] = (F ∗ K)(i, j) = \sum_m\sum_n F(m, n)K(i − m, j − n) \] 卷积是可交换的(commutative)，该公式可转变为： \[ S[i, j] = (F ∗ K)(i, j) = \sum_m\sum_n F(i-m, i-n)K( m, n),K即我们常说的卷积核 \] 图像卷积示例： 二维卷积示例1 参数：输入Shape：\((7\times7\times3)\) 卷积核Shape:\(3\times3\times3\times2\) \(zero-padding=1 stride=2\) 输出Shape计算： \[ \begin{align} W_2 &amp;= (W_1 - F + 2P)/S + 1\qquad(式2)\\ H_2 &amp;= (H_1 - F + 2P)/S + 1\qquad(式3) \end{align} \] 卷积层 卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。上一节中描述的卷积层的神经元和全连接网络一样都是一维结构。既然卷积网络主要应用在图像处理上，而图像为两维结构，因此为了更充分地利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度H×宽度W×深度C，有C个H ×W 大小的特征映射构成。 特征映射（feature map）为一幅图像（或其它特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征。为了卷积网络的表示能力，可以在每一层使用多个不同的特征映射，以更好地表示图像的特征。 卷积层的关键参数： 卷积核大小（Kernel Size）：定义了卷积操作的感受野。在二维卷积中，通常设置为3，即卷积核大小为3×3。 步幅（Stride）：定义了卷积核遍历图像时的步幅大小。其默认值通常设置为1，也可将步幅设置为2后对图像进行下采样，这种方式与最大池化类似。 边界扩充（Padding）：定义了网络层处理样本边界的方式。当卷积核大于1且不进行边界扩充，输出尺寸将相应缩小；当卷积核以标准方式进行边界扩充，则输出数据的空间尺寸将与输入相等。 输入与输出通道（Channels）：构建卷积层时需定义输入通道I，并由此确定输出通道O。这样，可算出每个网络层的参数量为I×O×K，其中K为卷积核的参数个数。例，某个网络层有64个大小为3×3的卷积核，则对应K值为 3×3 =9。 卷积层特性： 局部连接：卷积计算的通俗理解就是卷积核在图像上做滑动卷积，每次滑动输出一个值，局部乘积求和。局部计算如： 局部求和 权值共享：整张图像可以共用一个卷积核，对比全连接操作卷积不需要同时和每个像素点相乘求和(内积)。 池化（pooling） 池化也叫子采样（subsampling layer），其作用是进行特征选择，降低特征数量，并从而减少参数数量。 标准的卷积操作虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以在卷积操作之后加上一个池化操作，从而降低特征维数，避免过拟合。（也可通过增加滑动卷积的步长Stride来达到降维效果） 池化操作虽然说有降维功能，同时也是存在信息损失的。 池化 常见的池化方式 最大池化（max-pooling） 一般是在池化核对应区域内选取最大值来表征该区域。 平均池化（mean-pooling） 一般是取池化核对应区域内所有元素的均值来表征该区域。 全局池化（global pooling） 一般是对整个特征图进行最大池化或者平均池化。 重叠池化（overlapping-pooling） 相邻池化窗口之间会有重叠区域，此时kernel size&gt;stride 卷积神经网络中的特殊卷积操作 \(1\times1\)卷积核 自从GoogLenet接手发扬光大以后，\(1\times1\)卷积广泛出现在新的卷积架构中如xception和小卷积网络中。\(1\times1\) 卷积核在单通道上就是给每个像素点同等乘以一个系数，多通道上其实现了通道间的信息流通。从全连接的角度解释即C通道特征图，和C个\(1\times1\)卷积核卷积输出1个新的特征图。同理，和KC个1x1的卷积核卷积输出K个特征图。注：全连接运算是元素级的，\(1\times1\)卷积是通道级的，对于HW * C的特征图全连接需要K * H * W * C，而11卷积仅需要KC个参数。图片来自知乎。 \(1\times1\)卷积的作用 跨通道信息交互（channal 的变换） 升维（用最少的参数拓宽网络channal） 降维（减少参数） 深度可分离卷积（depthwise separable convolution） 深度可分离卷积在执行空间卷积，同时保持通道分离，然后进行深度卷积。 可分离卷积与标准的卷积相比，其在通道上先做了卷积然后在再用\(1\times1\)卷积进行通道融合。示意图： 可分离卷积 对比标准卷积过程： 标准卷积 假设我们在16个输入通道和32个输出通道上有一个3x3卷积层。详细情况是，16个3x3内核遍历16个通道中的每一个，产生512（16x32）个特征映射。接下来，我们通过添加它们来合并每个输入通道中的1个特征图。由于我们可以做32次，我们得到了我们想要的32个输出通道。 针对这个例子应用深度可分离卷积，用1个3×3大小的卷积核遍历16通道的数据，得到了16个特征图谱。在融合操作之前，接着用32个1×1大小的卷积核遍历这16个特征图谱，进行相加融合。这个过程使用了16×3×3+16×32×1×1=656个参数，远少于上面的16×32×3×3=4608个参数。 这个例子就是深度可分离卷积的具体操作，其中上面的深度乘数（depth multiplier）设为1，这也是目前这类网络层的通用参数。这么做是为了对空间信息和深度信息进行去耦。从Xception模型的效果可以看出，这种方法是比较有效的。由于能够有效利用参数，因此深度可分离卷积也可以用于移动设备中。 空洞卷积（dilated convolution） dilated convolution是针对图像语义分割问题中下采样会降低图像分辨率、丢失信息而提出的一种卷积思路。 空洞卷积的特点是在扩大感受野的同时不增加计算成本。示意图： 空洞卷积静态图 上图b可以理解为卷积核大小依然是3×3，但是每个卷积点之间有1个空洞，也就是在绿色7×7区域里面，只有9个红色点位置作了卷积处理，其余点权重为0。这样即使卷积核大小不变，但它看到的区域变得更大了 。 空洞卷积的动机：加pooling层，损失信息，降低精度；不加pooling层，感受野变小，模型学习不到全局信息 组卷积（Group convolution） 分组卷积（Group convolution） ，最早在AlexNet中出现，由于当时的硬件资源有限，训练AlexNet时卷积操作不能全部放在同一个GPU处理，因此作者把feature maps分给多个GPU分别进行处理，最后把多个GPU的结果进行融合（concatenate）。 下图分别是一个正常的、没有分组的卷积层结构和分组卷积结构的示意图： 组卷积 标准卷积_分组 组卷积通过将卷积运算的输入限制在每个组内，模型的计算量取得了显著的下降。然而这样做也带来了明显的问题：在多层逐点卷积堆叠时，模型的信息流被分割在各个组内，组与组之间没有信息交换。这将可能影响到模型的表示能力和识别精度。 目前旷世科技提出的ShuffleNet引入通道重排来处理组和组之间的信息流通。 分组卷积操作实例： 从一个具体的例子来看，Group conv本身就极大地减少了参数。比如当输入通道为256，输出通道也为256，kernel size为3×3，不做Group conv参数为256×3×3×256。实施分组卷积时，若group为8，每个group的input channel和output channel均为32，参数为8×32×3×3×32，是原来的八分之一。而Group conv最后每一组输出的feature maps以concatenate的方式组合。 Alex认为group conv的方式能够增加 filter之间的对角相关性，而且能够减少训练参数，不容易过拟合，这类似于正则的效果。 转置卷积 反卷积是一种上采样操作，在图像分割和卷积自编码中应用较多。示意图： 转置卷积 小结 本文简单的介绍了信号卷积的算法原理，引入二维离散卷积引导对卷积神经网络的卷积的理解。卷积是一种滤波操作，在卷积神经网络中有许多卷积变形结构，这些结构从维度控制，信息流通，模型参数大小控制，感受野等角度进行设计。在当前state of art卷积架构中多是融入了部分或者全部上述变形卷积。在设计新的模型时，可从设计目标出发采用上述结构优化，裁剪模型。 参考 邱锡鹏 ,《神经网络与深度学习》https://nndl.github.io/&gt; https://zhuanlan.zhihu.com/p/28749411 https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d https://www.zhihu.com/question/54149221 https://www.cnblogs.com/ranjiewen/articles/8699268.html https://blog.csdn.net/A_a_ron/article/details/79181108]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>信号卷积</tag>
        <tag>组卷积</tag>
        <tag>空洞卷积</tag>
        <tag>可分离卷积</tag>
        <tag>1x1卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow Object Detection API Win10和Ubuntu双系统教程]]></title>
    <url>%2F2018%2F12%2F23%2FTensorflow%20Object%20Detection%20API%20Win10%E5%92%8CUbuntu%E5%8F%8C%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本实验的Faster RCNN 源代码实现来自tensorflow/models 的 object_detection API ；在COCO2014数据集上完成训练和测试，本教程由我、胡保林大神以及韩博士在win10和Ubuntu16.04上完成并测试通过。 1. 环境和依赖准备 Tensorflow Object Detection API 根据官方安装指示安装下载安装： 123456最后的测试代码： python object_detection/builders/model_builder_test.py如果返回下图结果，表示上述API安装成功、如果安装失败，看代码报错debug.注意：将 object_detection 和 slim 配置到环境变量中，否则会出现 没有这俩个模块的报错win10: 可以在上述俩个文件夹下运行： python setup.py install 完整环境配置完成测试结果 下载 COCO2014数据集. 保持下列数据存储: 12345678COCO/DIR/ annotations/ instances_train2014.json instances_val2014.json train2014/ COCO_train2014_*.jpg val2014/ COCO_val2014_*.jpg 2. Object Detection API部分代码用法 1. 创建COCO或者VOC格式的TFRecord文件 object detection/dataset_tools中包含创建TFRecord的python程序。以创建COCO2014为例： 1234567python object detection/dataset_tools/create_coco_tf_record.py --logtostderr \ --train_image_dir="E:\TEMP_PROJECTS\\COCO\\train2014\\" \ --val_image_dir="E:\TEMP_PROJECTS\\COCO\\val2014\\" \ -- train_annotations_file= "E:\TEMP_PROJECTS\COCO\\annotations/instances_train2014.json"\ --val_annotations_file="E:\TEMP_PROJECTS\COCO\\annotations/instances_val2014.json" \ --output_dir="../data/coco" PS:如果不转换测试数据，请在create_coco_tf_record.py中检测assert函数将test_file相关代码注释掉 create_tfrcord 2.训练Faster RCNN Nasnet 下载预训练模型和修改config文件 123451. 类别，迭代数，预训练模型路径，tfrecord的路径。（注意：如果只有11G显存，请把config中图像尺寸改成600*600以下）2. 针对自己数据集修改 类别个数--num_classes3. samples/configs/faster_rcnn_nas_coco.config文件里面PATH_TO_BE_CONFIGURED 共有5处应替换成对应文件：比如训练集路径，label_map路径，模型权重路径等，详见config文件4. fine_tune_checkpoint：设置预训练模型权重路径。5. num_steps 根据实际情况修改 输入训练命令（根目录：research文件夹） 12345python object_detection/legacy/train.py --logtostderr \ --train_dir=object_detection/data/coco/ \ --pipeline_config_path=object_detection/samples/configs/faster_rcnn_nas_coco.config #train_dir---训练数据路径 #pipeline_config_path---config 路径 训练过程截图 模型评估（根目录同上） 123注意：1. legacy/evaluator.py 第58行改为 ：EVAL_DEFAULT_METRIC = 'coco_detection_metrics'2. utils/object_detection_evaluation.py 如果用python3的话将 unicode 改成str 123456python object_detection/legacy/eval.py --logtostderr --checkpoint_dir=object_detection/faster_rcnn_nas_coco_2018_01_28 --eval_dir=object_detection/data/coco/ --pipeline_config_path=object_detection/samples/configs/faster_rcnn_nas_coco.config --run_once True 测试结果–简略版 3. 在VOC上微调 构建VOC TFRcord格式数据 1234python object_detection/dataset_tools/create_pascal_tf_record.py \ --data_dir=/mnt/fpan/HBL_data/data_set/VOCdevkit \ --year=VOC2007 \ --output_path=object_detection/data/voc/train2007.record #是文件名 重新写一个object_detection/samples/configs/faster_rcnn_nas_voc.config 文件 在 fine_tune_checkpoint：设置为COCO训练好的模型权重路径。 测试方法同上 4. Faster RCNN Nasnet demo 1见object_detection_tutorial.py 该文件需要在object detection目录下或者你把里面的路径设置正确 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173# coding: utf-8# # Object Detection Demo# Welcome to the object detection inference walkthrough! This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start.import numpy as npimport osimport six.moves.urllib as urllibimport sysimport tarfileimport tensorflow as tfimport zipfilefrom distutils.version import StrictVersionfrom collections import defaultdictfrom io import StringIOfrom matplotlib import pyplot as pltfrom PIL import Image# This is needed since the notebook is stored in the object_detection folder.sys.path.append("..")from object_detection.utils import ops as utils_opsif StrictVersion(tf.__version__) &lt; StrictVersion('1.9.0'): raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')# ## Env setup# This is needed to display the images.#get_ipython().magic('matplotlib inline')# ## Object detection imports# Here are the imports from the object detection module.from utils import label_map_utilfrom utils import visualization_utils as vis_util# # Model preparation # ## Variables# # Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file. # # By default we use an "SSD with Mobilenet" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.# What model to download.MODEL_NAME = 'faster_rcnn_nas_coco_2018_01_28'MODEL_FILE = MODEL_NAME + '.tar.gz'DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'# Path to frozen detection graph. This is the actual model that is used for the object detection.PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'# List of the strings that is used to add correct label for each box.PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')# ## Download Model# opener = urllib.request.URLopener()# opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)# tar_file = tarfile.open(MODEL_FILE)# for file in tar_file.getmembers():# file_name = os.path.basename(file.name)# if 'frozen_inference_graph.pb' in file_name:# tar_file.extract(file, os.getcwd())# ## Load a (frozen) Tensorflow model into memory.detection_graph = tf.Graph()with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='')# ## Loading label map# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be finecategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)def load_image_into_numpy_array(image): (im_width, im_height) = image.size return np.array(image.getdata()).reshape( (im_height, im_width, 3)).astype(np.uint8)# # Detection# For the sake of simplicity we will use only 2 images:# image1.jpg# image2.jpg# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.PATH_TO_TEST_IMAGES_DIR = 'test_images'TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image&#123;&#125;.jpg'.format(i)) for i in range(1, 3) ]# Size, in inches, of the output images.IMAGE_SIZE = (12, 8)def run_inference_for_single_image(image, graph): with graph.as_default(): with tf.Session() as sess: # Get handles to input and output tensors ops = tf.get_default_graph().get_operations() all_tensor_names = &#123;output.name for op in ops for output in op.outputs&#125; tensor_dict = &#123;&#125; for key in [ 'num_detections', 'detection_boxes', 'detection_scores', 'detection_classes', 'detection_masks' ]: tensor_name = key + ':0' if tensor_name in all_tensor_names: tensor_dict[key] = tf.get_default_graph().get_tensor_by_name( tensor_name) if 'detection_masks' in tensor_dict: # The following processing is only for single image detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0]) detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0]) # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size. real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32) detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1]) detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1]) detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks( detection_masks, detection_boxes, image.shape[0], image.shape[1]) detection_masks_reframed = tf.cast( tf.greater(detection_masks_reframed, 0.5), tf.uint8) # Follow the convention by adding back the batch dimension tensor_dict['detection_masks'] = tf.expand_dims( detection_masks_reframed, 0) image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0') # Run inference output_dict = sess.run(tensor_dict, feed_dict=&#123;image_tensor: np.expand_dims(image, 0)&#125;) # all outputs are float32 numpy arrays, so convert types as appropriate output_dict['num_detections'] = int(output_dict['num_detections'][0]) output_dict['detection_classes'] = output_dict[ 'detection_classes'][0].astype(np.uint8) output_dict['detection_boxes'] = output_dict['detection_boxes'][0] output_dict['detection_scores'] = output_dict['detection_scores'][0] if 'detection_masks' in output_dict: output_dict['detection_masks'] = output_dict['detection_masks'][0] return output_dictfor image_path in TEST_IMAGE_PATHS: image = Image.open(image_path) # the array based representation of the image will be used later in order to prepare the # result image with boxes and labels on it. image_np = load_image_into_numpy_array(image) # Expand dimensions since the model expects images to have shape: [1, None, None, 3] image_np_expanded = np.expand_dims(image_np, axis=0) # Actual detection. output_dict = run_inference_for_single_image(image_np, detection_graph) # Visualization of the results of a detection. vis_util.visualize_boxes_and_labels_on_image_array( image_np, output_dict['detection_boxes'], output_dict['detection_classes'], output_dict['detection_scores'], category_index, instance_masks=output_dict.get('detection_masks'), use_normalized_coordinates=True, line_thickness=8) plt.figure(figsize=IMAGE_SIZE) plt.imshow(image_np)]]></content>
      <categories>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Faster RCNN</tag>
        <tag>Tensorflow</tag>
        <tag>COCO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程]]></title>
    <url>%2F2018%2F08%2F28%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[首先感谢况老师的指导和潇哥的指点。以下内容来自初学的我，谨防入坑！ 促使我对并发/并行操作的关注是我对python同步多线程和异步多线程的理解空白，导致我2个月前的一次失败的同步多线程操作直到昨天改keras的imge处理底层代码时发现还有异步多线程实现的并发操作。在此记录下并发操作相关概念和理解。 理解python的并发操作需要掌握python的全局解释器锁(GIL)以及多进程、多线程操作和协程操作。相关概念的区别如阻塞和非阻塞、并发和并行、同步和异步、计算操作密集和I/O密集的概念和区分。本文重点区分上述概念。 python为什么比C++/Java慢 首先C++和Java是编译型语言，而Python则是一种解释型语言。编译型语言在程序执行前需要一个专门编译额过程，将代码编译成机器语言；而python是解释型语言不需要编译，在程序执行时将代码一步步翻译成机器语言。python的变量类型是动态的，解释器会根据程序将变量和所有的变量类型存放在内存中；而静态类型直接将变量与其类型绑定。形象的理解是python解释器制定规则，python执行代码时去匹配这个规则，然后再执行。 全局解释器锁(GIL) 为了利用多核系统，Python必须支持多线程运行。但作为解释型语言，Python的解释器需要做到既安全又高效。解释器要注意避免在不同的线程操作内部共享的数据，同时还要保证在管理用户线程时保证总是有最大化的计算资源。为了保证不同线程同时访问数据时的安全性，Python使用了全局解释器锁(GIL)的机制。从名字上我们很容易明白，它是一个加在解释器上的全局（从解释器的角度看）锁（从互斥或者类似角度看）。这种方式当然很安全，但它也意味着：对于任何Python程序，不管有多少的处理器，任何时候都总是只有一个线程在执行。即：只有获得了全局解释器锁的线程才能操作Python对象或者调用Python/C API函数。 Python的GIL CPython的线程是操作系统的原生线程。在Linux上为pthread，在Windows上为Win thread，完全由操作系统调度线程的执行。一个Python解释器进程内有一个主线程，以及多个用户程序的执行线程。即便使用多核心CPU平台，由于GIL的存在，也将禁止多线程的并行执行。 Python解释器进程内的多线程是以协作多任务方式执行。当一个线程遇到I/O任务时，将释放GIL。计算密集型（CPU-bound）的线程在执行大约100次解释器的计步（ticks）时，将释放GIL。计步（ticks）可粗略看作Python虚拟机的指令。计步实际上与时间片长度无关。可以通过sys.setcheckinterval()设置计步长度。 在单核CPU上，数百次的间隔检查才会导致一次线程切换。在多核CPU上，存在严重的线程抖动（thrashing）。 Python 3.2开始使用新的GIL。新的GIL实现中用一个固定的超时时间来指示当前的线程放弃全局锁。在当前线程保持这个锁，且其他线程请求这个锁时，当前线程就会在5毫秒后被强制释放该锁。 可以创建独立的进程来实现并行化。Python 2.6引进了多进程包multiprocessing。或者将关键组件用C/C++编写为Python扩展，通过ctypes使Python程序直接调用C语言编译的动态链接库的导出函数。【来自维基百科】 小结： 由于GIL的存在，一个python解释器进程执行多线程时实际上也只是在保护一个主线程的运行。 I/O操作主要是读写任务，CPU操作主要是计算任务，当python执行I/O密集型任务，将释放GIL开多线程仍然可以通过让其他线程加快程序运行效率。如果是cpu密集型任务，线程切换导致cache missing造成不必要的开销切换，反而影响程序效率。在keras图像预处理接口keras.preprocessing.image中实现多线程读取文件夹名和文件名操作然后分批，在读取分批数据以达到减小内存的作用的 进程 进程之间不共享任何状态，进程的调度由操作系统完成，每个进程都有自己独立的内存空间，进程间通讯主要是通过信号传递的方式来实现的，实现方式有多种，信号量、管道、事件等，任何一种方式的通讯效率都需要过内核，导致通讯效率比较低。由于是独立的内存空间，上下文切换的时候需要保存先调用栈的信息、cpu各寄存器的信息、虚拟内存、以及打开的相关句柄等信息，所以导致上下文进程间切换开销很大，通讯麻烦。 简单来讲，每一个应用程序都有一个自己的进程。 操作系统会为这些进程分配一些执行资源，例如内存空间等。 在进程中，又可以创建一些线程，他们共享这些内存空间，并由操作系统调用， 以便并行计算。 创建进程 Python2.6以后提供了非常好用的多进程包multiprocessing，只需要定义一个函数，Python会完成其他所有事情。multiprocessing给每个进程赋予单独的Python解释器，这样就规避了全局解释锁所带来的问题。借助这个包，可以轻松完成从单进程到并发执行的转换。 multiprocessing支持子进程、通信和共享数据、执行不同形式的同步，提供了Process、Queue、Pipe、Lock等组件。具体的细节以后再说吧，实践很少就是全记下来了也记不久留个坑；以后有需求了再补充：进程通信，锁，列队，进程通信管道等。 用multiprocessing开启进程，其代码块必须放在if __name__ == '__main__':下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import timeimport randomfrom multiprocessing import Processdef start_process(name): print('%s cur ele' %name) time.sleep(3) print('%s cur ele end' %name)# 进程开启必须放在main()下，multiprocessing为每个进程开一个解释器if __name__ == '__main__': for i in ('ele1', 'ele2', 'ele3'): p = Process(target=start_process, args=(i,)) p.start() print('主进程') """OUTPUT 主进程ele1 cur eleele2 cur eleele3 cur eleele1 cur ele endele2 cur ele endele3 cur ele end """#没有time.sleepimport timeimport randomfrom multiprocessing import Processdef start_process(name): print('%s cur ele' %name)# time.sleep(3) print('%s cur ele end' %name)if __name__ == '__main__': # 进程开启必须放在main()下 for i in ('ele1', 'ele2', 'ele3'): p = Process(target=start_process, args=(i,)) p.start() print('主进程') """ 主进程ele1 cur eleele1 cur ele endele2 cur eleele2 cur ele endele3 cur eleele3 cur ele end """class start_process(Process): def __init__(self,name): super().__init__() self.name = name def run(self): print('%s cur ele' %self.name) time.sleep(3) print('%s cur ele' %self.name)if __name__ == '__main__': # 进程开启必须放在main()下 print("CPU个数：",cpu_count()) for i in ('ele1', 'ele2', 'ele3','ele4','ele5'): p = start_process(i,) p.start() p.join()#加入进程同步 print('主进程')"""CPU个数： 4ele1 cur eleele1 cur eleele2 cur eleele2 cur eleele3 cur eleele3 cur eleele4 cur eleele4 cur eleele5 cur eleele5 cur ele主进程""" 进程池 进程池就是预先创建进程，需要的时候就从进程池拿，进程的创建和销毁统一由进程池管理。Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来它。python内置的multiprocessing.pool实现进程池管理。在python官方文档里显示该功能在交互式解释器中并不能完好运行，而且实践发现spyder不打印子进程输出 。 线程 操作系统为进程分配执行的资源比如内存空间，而线程就在进程下面共享进程的资源。多线程可以处理多进程访问同一资源麻烦的问题。 创建线程 Python提供两个模块进行多线程的操作，分别是thread和threading，前者是比较低级的模块，用于更底层的操作，一般应用级别的开发不常用。 第一种方法是创建threading.Thread的子类，重写run方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import threadingimport timeclass MyThread(threading.Thread): def __init__(self,name): super().__init__() self.name = name def run(self): for i in range(5): print ('thread &#123;&#125;, @number: &#123;&#125;'.format(self.name, i)) time.sleep(2)def main(): print ("Start main threading") # 创建三个线程 for i in range(3): threads = MyThread("thread-%i"%i) # 启动三个线程 threads.start() threads.join() print ("End Main threading")if __name__ == '__main__': print("=======创建同步三个线程=======") main() print("=======创建异步多线程=======") threads = [MyThread("thread-%i"%i) for i in range(3)] for t in threads: t.start() # 一次让新创建的线程执行 join for t in threads: t.join() print ("End Async Main threading")"""受GIL控制同步时由一个线程控制，多进程在等待上个进程时受系统调度已经开始执行了异步多线程在等待时执行了下一个线程=======创建同步三个线程=======Start main threadingthread thread-2, @number: 0thread thread-2, @number: 1thread thread-2, @number: 2thread thread-2, @number: 3thread thread-2, @number: 4End Main threading=======创建异步多线程=======thread thread-0, @number: 0thread thread-1, @number: 0thread thread-2, @number: 0thread thread-0, @number: 1thread thread-1, @number: 1thread thread-2, @number: 1thread thread-0, @number: 2thread thread-1, @number: 2thread thread-2, @number: 2thread thread-0, @number: 3thread thread-1, @number: 3thread thread-2, @number: 3thread thread-0, @number: 4thread thread-2, @number: 4thread thread-1, @number: 4End Async Main threading""" 线程池 线程池的出发点和进程池类似，线程为了控制和管理线程即创建和销毁。具体的将线程放进一个池子，一方面我们可以控制同时工作的线程数量，一方面也避免了创建和销毁产生的开销。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import timeimport threadingfrom random import randomimport queuedef double(n): return n * 2class Worker(threading.Thread): def __init__(self, queue): super(Worker, self).__init__() self._q = queue self.daemon = True self.start() def run(self): while 1: f, args, kwargs = self._q.get() try: print('USE:&#123;&#125;'.format(self.name)) print(f(*args, **kwargs)) except Exception as e: print(e) self._q.task_done()class ThreadPool(object): def __init__(self, max_num=5): self._q = queue.Queue(max_num) for _ in range(max_num): Worker(self._q) # create worker thread def add_task(self, f, *args, **kwargs): self._q.put((f, args, kwargs)) def wait_compelete(self): self._q.join()pool = ThreadPool()for _ in range(8): wt = random() pool.add_task(double, wt) time.sleep(wt)pool.wait_compelete() 多进程和多线程的计算开销 code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224from multiprocessing import Process , Queue , cpu_countimport timefrom threading import Thread # 定义全局变量Queueg_queue = Queue()# 定义一个队列，并定义初始化队列的函数def init_queue(): print("init g_queue start") while not g_queue.empty(): g_queue.get() for _index in range(10): g_queue.put(_index) print("init g_queue end") return # 定义IO密集型任务和计算密集型任务，分别从队列中获取任务数据# 定义一个IO密集型任务：利用time.sleep()def task_io(task_id): print("IOTask[%s] start" % task_id) while not g_queue.empty(): time.sleep(1) try: data = g_queue.get(block=True, timeout=1) print("IOTask[%s] get data: %s" % (task_id, data)) except Exception as excep: print("IOTask[%s] error: %s" % (task_id, str(excep))) print("IOTask[%s] end" % task_id) return g_search_list = list(range(10000))# 定义一个计算密集型任务：利用一些复杂加减乘除、列表查找等def task_cpu(task_id): print("CPUTask[%s] start" % task_id) while not g_queue.empty(): count = 0 for i in range(10000): count += pow(3*2, 3*2) if i in g_search_list else 0 try: data = g_queue.get(block=True, timeout=1) print("CPUTask[%s] get data: %s" % (task_id, data)) except Exception as excep: print("CPUTask[%s] error: %s" % (task_id, str(excep))) print("CPUTask[%s] end" % task_id) return task_id if __name__ == '__main__': print("cpu count:", cpu_count(), "\n") print("========== 直接执行IO密集型任务 ==========") init_queue() time_0 = time.time() task_io(0) print("结束：", time.time() - time_0, "\n") print("========== 多线程执行IO密集型任务 ==========") init_queue() time_0 = time.time() thread_list = [Thread(target=task_io, args=(i,)) for i in range(5)] for t in thread_list: t.start() for t in thread_list: if t.is_alive(): t.join() print("结束：", time.time() - time_0, "\n") print("========== 多进程执行IO密集型任务 ==========") init_queue() time_0 = time.time() process_list = [Process(target=task_io, args=(i,)) for i in range(cpu_count())] for p in process_list: p.start() for p in process_list: if p.is_alive(): p.join() print("结束：", time.time() - time_0, "\n") print("========== 直接执行CPU密集型任务 ==========") init_queue() time_0 = time.time() task_cpu(0) print("结束：", time.time() - time_0, "\n") print("========== 多线程执行CPU密集型任务 ==========") init_queue() time_0 = time.time() thread_list = [Thread(target=task_cpu, args=(i,)) for i in range(5)] for t in thread_list: t.start() for t in thread_list: if t.is_alive(): t.join() print("结束：", time.time() - time_0, "\n") print("========== 多进程执行cpu密集型任务 ==========") init_queue() time_0 = time.time() process_list = [Process(target=task_cpu, args=(i,)) for i in range(cpu_count())] for p in process_list: p.start() for p in process_list: if p.is_alive(): p.join() print("结束：", time.time() - time_0, "\n")"""cpu count: 4========== 直接执行IO密集型任务 ==========init g_queue startinit g_queue endIOTask[0] startIOTask[0] get data: 0IOTask[0] get data: 1IOTask[0] get data: 2IOTask[0] get data: 3IOTask[0] get data: 4IOTask[0] get data: 5IOTask[0] get data: 6IOTask[0] get data: 7IOTask[0] get data: 8IOTask[0] get data: 9IOTask[0] end结束： 10.0093834400177========== 多线程执行IO密集型任务 ==========init g_queue startinit g_queue endIOTask[0] startIOTask[1] startIOTask[2] startIOTask[3] startIOTask[4] startIOTask[0] get data: 0IOTask[1] get data: 1IOTask[2] get data: 2IOTask[3] get data: 3IOTask[4] get data: 4IOTask[0] get data: 5IOTask[1] get data: 6IOTask[3] get data: 7IOTask[2] get data: 8IOTask[4] get data: 9IOTask[2] endIOTask[4] endIOTask[0] error:IOTask[0] endIOTask[1] error:IOTask[1] endIOTask[3] error:IOTask[3] end结束： 4.019008159637451========== 多进程执行IO密集型任务 ==========init g_queue startinit g_queue endIOTask[3] startIOTask[3] endIOTask[0] startIOTask[0] endIOTask[1] startIOTask[1] endIOTask[2] startIOTask[2] end结束： 0.2742750644683838========== 直接执行CPU密集型任务 ==========init g_queue startinit g_queue endCPUTask[0] startCPUTask[0] get data: 0CPUTask[0] get data: 1CPUTask[0] get data: 2CPUTask[0] get data: 3CPUTask[0] get data: 4CPUTask[0] get data: 5CPUTask[0] get data: 6CPUTask[0] get data: 7CPUTask[0] get data: 8CPUTask[0] get data: 9CPUTask[0] end结束： 8.974508285522461========== 多线程执行CPU密集型任务 ==========init g_queue startinit g_queue endCPUTask[0] startCPUTask[1] startCPUTask[2] startCPUTask[3] startCPUTask[4] startCPUTask[1] get data: 0CPUTask[3] get data: 1CPUTask[0] get data: 2CPUTask[2] get data: 3CPUTask[4] get data: 4CPUTask[1] get data: 5CPUTask[3] get data: 6CPUTask[2] get data: 7CPUTask[0] get data: 8CPUTask[4] get data: 9CPUTask[4] endCPUTask[1] error:CPUTask[1] endCPUTask[2] error:CPUTask[2] endCPUTask[3] error:CPUTask[3] endCPUTask[0] error:CPUTask[0] end结束： 13.33148455619812========== 多进程执行cpu密集型任务 ==========init g_queue startinit g_queue endCPUTask[1] startCPUTask[1] endCPUTask[2] startCPUTask[2] endCPUTask[3] startCPUTask[3] endCPUTask[0] startCPUTask[0] end结束： 0.2652151584625244""" 协程 协程，又称微线程，纤程。英文名Coroutine。协程的概念很早就提出来了，但直到最近几年才在某些语言（如Lua）中得到广泛应用。协程描述部分来自博客园 子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。子程序调用总是一个入口，一次返回，调用顺序是明确的。而协程的调用和子程序不同。 协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。 线程和进程的操作是由程序触发系统接口，最后的执行者是系统；协程的操作则是程序员 协程的优点： 无需线程上下文切换的开销 无需原子操作锁定及同步的开销 方便切换控制流，简化编程模型 高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。 缺点： 无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。 进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序 Python中的协程和生成器很相似但又稍有不同。主要区别在于： 生成器是数据的生产者 协程则是数据的消费者 在前面的博客中讲过生成器用yeild来实现动态的返回程序的结果，即实现了一个协程。协程会消费掉(拿去用)发送给它的值。 生产者消费者模式 生产者负责生产数据，存放在缓存区，而消费者负责从缓存区获得数据并处理数据。生产者消费者模式即将一件事分成流水线模型，生产者产生的输出交付给消费者处理，可以实现同一时刻，前后同时运行。 生产者消费者模式 生产者消费者模式的优点： 解耦 假设生产者和消费者分别是两个线程。如果让生产者直接调用消费者的某个方法，那么生产者对于消费者就会产生依赖（也就是耦合）。如果未来消费者的代码发生变化，可能会影响到生产者的代码。而如果两者都依赖于某个缓冲区，两者之间不直接依赖，耦合也就相应降低了。 并发 由于生产者与消费者是两个独立的并发体，他们之间是用缓冲区通信的，生产者只需要往缓冲区里丢数据，就可以继续生产下一个数据，而消费者只需要从缓冲区拿数据即可，这样就不会因为彼此的处理速度而发生阻塞。 支持忙闲不均 当生产者制造数据快的时候，消费者来不及处理，未处理的数据可以暂时存在缓冲区中，慢慢处理掉。而不至于因为消费者的性能造成数据丢失或影响生产者生产。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from queue import Queueimport random,threading,time#生产者类class Producer(threading.Thread): def __init__(self, name,queue): threading.Thread.__init__(self, name=name) self.data=queue def run(self): for i in range(5): print("%s is producing %d to the queue!" % (self.getName(), i)) self.data.put(i)#将数据放入队列 time.sleep(random.randrange(10)/5) print("%s finished!" % self.getName())#消费者类class Consumer(threading.Thread): def __init__(self,name,queue): threading.Thread.__init__(self,name=name) self.data=queue def run(self): for i in range(5): val = self.data.get()#从队列中获取数据执行下面的操作 print("%s is consuming. %d in the queue is consumed!" % (self.getName(),val)) time.sleep(random.randrange(10)) print("%s finished!" % self.getName())def main(): queue = Queue() producer = Producer('Producer',queue) consumer = Consumer('Consumer',queue) producer.start() consumer.start() producer.join() consumer.join() print ('All threads finished!')if __name__ == '__main__': main() """Producer is producing 0 to the queue!Consumer is consuming. 0 in the queue is consumed!Producer is producing 1 to the queue!Producer is producing 2 to the queue!Consumer is consuming. 1 in the queue is consumed!Producer is producing 3 to the queue!Producer is producing 4 to the queue!Producer finished!Consumer is consuming. 2 in the queue is consumed!Consumer is consuming. 3 in the queue is consumed!Consumer is consuming. 4 in the queue is consumed!Consumer finished!All threads finished! """ 阻塞与非阻塞 阻塞是指调用线程或者进程被操作系统挂起。 非阻塞是指调用线程或者进程不会被操作系统挂起。 同步与异步 同步是指代码调用IO操作时，必须等待IO操作完成才返回的调用方式。 异步是指代码调用IO操作时，不必等IO操作完成就返回的调用方式。 同步是最原始的调用方式。 异步则需要多线程，多CPU或者非阻塞IO的支持。 在multiprocessing中通常用apply/map和apply_async/map_async函数完成同步异步(阻塞和非阻塞) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118"""从keras image剥离的文件夹处理的源码，用阻塞和非阻塞方式实现并发操作"""from __future__ import absolute_importfrom __future__ import print_functionimport numpy as npimport refrom scipy import linalgimport scipy.ndimage as ndifrom six.moves import rangeimport osimport threadingimport warningsimport multiprocessing.poolfrom functools import partialimport timetry: from PIL import Image as pil_imageexcept ImportError: pil_image = Nonedef _count_valid_files_in_directory(directory, white_list_formats, follow_links): def _recursive_list(subpath): return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0]) samples = 0 for root, _, files in _recursive_list(directory): for fname in files: is_valid = False for extension in white_list_formats: if fname.lower().endswith('.' + extension): is_valid = True break if is_valid: samples += 1 return samplesdef _list_valid_filenames_in_directory(directory, white_list_formats, class_indices, follow_links): def _recursive_list(subpath): return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0]) classes = [] filenames = [] subdir = os.path.basename(directory) basedir = os.path.dirname(directory) for root, _, files in _recursive_list(directory): for fname in files: is_valid = False for extension in white_list_formats: if fname.lower().endswith('.' + extension): is_valid = True break if is_valid: classes.append(class_indices[subdir]) # add filename relative to directory absolute_path = os.path.join(root, fname) filenames.append(os.path.relpath(absolute_path, basedir)) return classes, filenamesdef main(pool,apply): for dirpath in (os.path.join(directory, subdir) for subdir in classes): results.append(apply(_list_valid_filenames_in_directory, (dirpath, white_list_formats,class_indices, False))) pool.close() pool.join() if __name__ =='__main__': results = [] directory = 'E:\\hxd\\零样本学习\\data\\Atrain\\train\\' white_list_formats = &#123;'png', 'jpg', 'jpeg', 'bmp', 'ppm'&#125; classes = [] for subdir in sorted(os.listdir(directory)): if os.path.isdir(os.path.join(directory, subdir)): classes.append(subdir) class_indices = dict(zip(classes, range(len(classes)))) print('==========IO密集型非阻塞多线程==========') start=time.time() pool = multiprocessing.pool.ThreadPool() main(pool,pool.apply_async) pool.close() pool.join() print('ThreadPool Time:%f'%(time.time()-start)) print('==========IO密集型非阻塞多进程==========') results = [] start=time.time() pool = multiprocessing.Pool() main(pool,pool.apply_async) pool.close() pool.join() print('ProcessPool Time:%f'%(time.time()-start)) print('==========IO密集型阻塞多线程==========') start=time.time() pool = multiprocessing.pool.ThreadPool() main(pool,pool.apply) pool.close() pool.join() print('ThreadPool Time:%f'%(time.time()-start)) print('==========IO密集型阻塞多进程==========') results = [] start=time.time() pool = multiprocessing.Pool() main(pool,pool.apply) pool.close() pool.join() print('ProcessPool Time:%f'%(time.time()-start)) """==========IO密集型非阻塞多线程==========ThreadPool Time:1.554570==========IO密集型非阻塞多进程==========ProcessPool Time:1.008292==========IO密集型阻塞多线程==========ThreadPool Time:1.458078==========IO密集型阻塞多进程==========ProcessPool Time:2.155637 """ 总结 在实际应用中合理的应用并发多进程和多线程可以提高程序运行效率，针对不同任务类型如IO密集型任务的网络爬虫，CPU密集型的XGBoost的c++实现上均使用了多线程。本文从概念和简单实践出发分别阐述了多进程，多线程，协程等具体操作，以及生产者与消费者，同步与异步和阻塞与非阻塞等相关概念。期望能在将来遇到实际问题时能够有法可循。 Reference GIL-维基百科 https://www.cnblogs.com/vipchenwei/p/7809967.html https://blog.csdn.net/SecondLieutenant/article/details/79396984 https://eastlakeside.gitbooks.io/interpy-zh/content/Coroutines/ https://segmentfault.com/a/1190000008909344]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>多进程</tag>
        <tag>多线程</tag>
        <tag>协程</tag>
        <tag>并行操纵</tag>
        <tag>同步和异步</tag>
        <tag>全局解释器锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python小工具]]></title>
    <url>%2F2018%2F08%2F24%2Fpython%E6%89%B9%E9%87%8F%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[记录一些经常用的实用小工具和函数包。会持续更新我实际需求用到的代码，相关当笔记用，方便查阅。 [x] OS 不可描述的强大 [x] shutil 复制/移动/修改文件夹操作 OS os.sep 可以取代操作系统特定的路径分隔符。windows下为 ‘\’ os.name 字符串指示你正在使用的平台。比如对于Windows，它是’nt’，而对于Linux/Unix用户，它是 ‘posix’ os.getcwd() 函数得到当前工作目录，即当前Python脚本工作的目录路径 os.getenv() 获取一个环境变量，如果没有返回none os.putenv(key, value) 设置一个环境变量值 os.listdir(path) 返回指定目录下的所有文件和目录名 os.remove(path) 函数用来删除一个文件 os.system(command) 函数用来运行shell命令 os.linesep 字符串给出当前平台使用的行终止符。例如，Windows使用 ‘’，Linux使用 ‘’ 而Mac使用 ‘’ os.path.split(path) 函数返回一个路径的目录名和文件名 os.path.isfile() 和os.path.isdir()函数分别检验给出的路径是一个文件还是目录 os.path.exists() 函数用来检验给出的路径是否真地存在 os.curdir 返回当前目录 (‘.’) os.mkdir(path) 创建一个目录 os.makedirs(path) 递归的创建目录 os.chdir(dirname) 改变工作目录到dirname os.path.getsize(name) 获得文件大小，如果name是目录返回0L os.path.abspath(name) 获得绝对路径 os.path.normpath(path) 规范path字符串形式 os.path.splitext() 分离文件名与扩展名 os.path.join(path,name) 连接目录与文件名或目录 os.path.basename(path) 返回文件名 os.path.dirname(path) 返回文件路径 os.walk(top,topdown=True,onerror=None) 遍历迭代目录 return 三元组 （路径，文件夹，filename） os.rename(src, dst) 重命名file或者directory src到dst 如果dst是一个存在的directory, 将抛出OSError. 在Unix, 如果dst在存且是一个file, 如果用户有权限的话，它将被安静的替换. 操作将会失败在某些Unix 中如果src和dst在不同的文件系统中. 如果成功, 这命名操作将会是一个原子操作 (这是POSIX 需要). 在 Windows上, 如果dst已经存在, 将抛出OSError，即使它是一个文件. 在unix，Windows中有效。 os.renames(old, new) 递归重命名文件夹或者文件。像rename() shutil 模块 shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉 shutil.move( src, dst) 移动文件或重命名 shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的 shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间 shutil.copy( src, dst) 复制一个文件到一个文件或一个目录 shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西 shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作 shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接 shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容 123456789101112131415161718192021222324"""keras需要读取文件夹生成类别，然而很多时候我们只有文件名和类别。需要我们写小爬虫，或者把所有文件分类到不同文件夹#记录示例代码：train.txt : IMAGENAME CLASSNAMEtrain: DIR/IMAGE"""import osimport shutilf = open('G:\\零样本识别\\data\Atrain\\train.txt')trainfile = f.readlines()trainID = &#123;&#125;for file in trainfile: file = file.rsplit('\t') trainID[file[0]] = file[1].rsplit('\n')[0]del trainfile,filef.close()trainpath = 'G:\\零样本识别\\data\Atrain\\train\\'i=0for img in trainID.keys(): i +=1 print (i) if not os.path.exists(trainpath+trainID[img]): os.mkdir(trainpath+trainID[img]) shutil.move(trainpath+img,trainpath+trainID[img])]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>批量文件操作工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入boosting经典算法]]></title>
    <url>%2F2018%2F08%2F02%2F%E6%B7%B1%E5%85%A5boosting%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文旨在梳理 Boosting方法相关的概念及理论推导。 在介绍Boosting方法之前，我们应该对机器学习模型的误差分析有所了解。从经典的Boosting算法—标准Adaboost的原理入手建立Boosting算法的基本理解，再来分析GBDT的原理(下文的GBDT特指(Greedy Function Approximation：A Gradient Boosting Machine )提出的算法)及其变体XGBoost和Lightgbm后续文章再讲。 机器学习中的误差，偏差与方差 误差反映的是整个模型的准确度，偏差反映的是一个模型在样本上的输出与真实值之间的误差，即模型本身的精准度，方差反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性/泛化能力。 偏差和方差的概念粗略的表示为训练误差和|训练误差-测试误差|。偏差过大表现为过拟合，方差过大表现为过拟合。 集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差(过拟合)（bagging）、偏差（boosting）或改进预测（stacking）的效果。 Boosting算法 boosting算法是通过多个弱学习器串联提升成为强分类器，通常为加法模型和前向分布算法，核心为下级弱学习器修正上级学习器的偏差。 问题：为什么多个弱分类器串联在一起可以提升模型的能力减小偏差而不出现过大方差？ boosting算法要求的基分类器不是没有条件的！首先，基分类器需要一定的的可靠性，其次基分类器需要具有多样性。然而很多情况下可靠性和多样性难以兼得。 Adaboost AdaBoost是Adaptive Boosting缩写，是由Yoav Freund和Robert Schapire提出的机器学习元算法。AdaBoost在某种意义上是适应性的，即随后的弱学习器会基于先前分类器分类错误的样本增加权重。AdaBoost对噪声数据和异常值敏感。在某些问题中，它可能比其他学习算法更不容易受到过拟合问题的影响。基学习器可能很弱，但只要每个学习者的表现好于随机猜测，最终的模型可以融合成表现优异的强分类器。 Adaboost算法有多种推导方法[1]，比较容易理解的就是基于加性模型(additive model),即基学习器的线性组合来最小化指数损失函数。 \[ l_{exp}(H|D)=E_{x-D}[e^{-f(x)H(x)}]···········(1) \] \[ H(x) = \sum_{t=1}^{T}\alpha_th_t(x)···········(2) \\ 其中,T表示弱学习器个数，\alpha_t表示第t个学习器的权重，h_t(x)表示第t个学习器的输出 \] Adaboost的核心思想：1. 给错误分类的样本增加权重，让随后的分类器可以有偏的输出；2. 给各个基学习器加上权重。那么怎么给，有什么依据？ Adaboost算法描述： 输入：训练集\(D={(x_1,y_1),(x_2,y_2),···,(x_m,y_m)}\),其中\(y_i\in [-1,1]\) 基学习器f； 迭代次数T； (1) 初始化样本权重\(W_1(x)=\frac{1}{m}\) (2)迭代T次(for)，算出每个模型的误差\(e_t = \sum_{i=1}^{M}I(y_i\ne h_t(x_i))\) (3)if \(\epsilon &gt;0.5\)then break (4)更新\(\alpha_t=\frac{1}{2}ln(\frac{1-e_t}{e_t})\) (5)更新样本新权重\(W_t(x)=\frac{W_{t-1}exp(-\alpha_{t-1}h_{t-1}y_i)}{Z_{t-1}},其中Z_{t-1}=\sum_{}^{}W_{t-1}\) 输出：\(H(x)=sign(\sum_{t=1}^{T}\alpha_th_t(x))\) 问题：显然按照这个算法流程，标准的adaboost实现的是二分类，那么多分类，回归任务需要改进。这仅仅是adaboost的一个应用。 adaboost理论推导的俩个核心：在界定了错误率\(\epsilon_t\)的情况下,求解或者估计\(\alpha_t\)和新的分布\(W_t\)使指数损失函数最小。 \(\alpha_t\)的求解 \[ l_{exp}(H|W)=E_{x-W}[e^{-f(x)H(x)}]=p_{x-W}(f(x)=h_t(x))e^{-\alpha_t}+p_{x-W}(f(x)\ne h_t(x))e^{\alpha_t}=e^{-\alpha_t}(1-\epsilon_t)+e^{\alpha_t}\epsilon_t \] \[ \frac{\nabla \ell_{exp}(H|W)}{\nabla\alpha_t}=-e^{-\alpha_t}(1-\epsilon_t)+e^\alpha_t\epsilon_t \] 令\(\frac{\nabla l_{exp}(H|D)}{\nabla\alpha_t}=0\)可解得： \[ \alpha_t=\frac{1}{2}ln\frac{1-\epsilon_t}{\epsilon_t} \] \(W_t\)的求解 Adaboost的算法核心是在上一个学习器的基础上，对错误分类的样本权重进行向上调整，使其在后面的学习中更被关注。这个地方我们就要说到模型的多样性的重要了。某个样本在一些弱分类器上的表现不好，就在其他分类器上还可能获得增强，然后加性融合时其可获得较低的偏差。然是如果他在所有弱分类器上的表现都不好，那么加性模型也难以修正其偏差，多样性是boost算法一个需要的重要的属性。 在基分类器\(h_t(x)\)时的模型损失函数表达为： \[ \ell_{exp}(H_{t-1}+h_t(x)|W_t)=E_{x-W}[e^{-f(x)(H_{t-1}(x)+h_t(x))}]=E_{x-W}[e^{-f(x)H_{t-1}(x)}e^{-f(x)h_t(x))}] \] 对二分类，将\(e^{-f(x)h_t(x)}\)的二阶泰勒展开带入损失函数近似可得： \[ \ell_{exp}(H_{t-1}+h_t{x}|W)=E_{x-W}[e^{-f(x)H_{t-1}(x)}(\frac{3}{2}-f(x)h_t(x))] \] 最理想情况在当前模型(\(H_{t}(x)\))的损失函数最小(即\(h_t(x)\)纠正了\(H_{t-1}(x)\))的条件下求解\(W_t\)。 即： \[ h_t(x)=\arg\min_{h}\ell_{exp}(H_{t-1}(x)+h_t(x)|W) = \arg\min_{h}E_{x-W}[e^{-f(x)H_{t-1}(x)}(\frac{3}{2}-f(x)h_t(x))]\\= \arg\max_{h}E_{x-W}[e^{-f(x)H_{t-1}(x)}f(x)h_t(x)] \] \(f(x)和H_{t-1}(x)\)已知。令： \[ W_t=\frac{W(x)e^{-f(x)H_{t-1}(x)}}{E_{x-W}[e^{-f(x)H_{t-1}(x)}]} \] \[ 理想的基分类器：h_t(x)=\arg\max_{h}E_{x-W}[W_tf(x)h_t(x)]=\arg\min_{h}E_{x-W_t}[I(f(x) \ne h_t(x))] \] \(h_t(x)\)在分布\(W_t\)下仍然可以达到最小分类误差。 GDBT 吐槽：GDBT真的是太难理解了！如果只从概念上知道GDBT用不断的加基模型减小残差，那可能并没有完全理解GDBT。GDBT模型怎么加的？怎么分类的？输入是多维的构建树的过程是什么样的？ GDBT的基分类器是CART回归树，GDBT是通过不断地最小化残差来实现加法模型，其中梯度提升的本质是每一次建立模型是在之前建立模型损失函数的梯度下降方向 。梯度提升树在结构化数据上的具有优越性，如推荐系统和计算广告里面。 GDBT在处理回归问题时的残差其实就在最小平方损失函数的负梯度方向，即： \[ r_{mi}=-\frac{\partial[\frac{1}{2}(y_i-f_m(x_i))^2]}{\partial f_m(x_i)}=y_i-f_m(x_i) \] 常见的损失函数即负梯度方向： GDBT常见损失函数 上面的叙述说明，残差是负梯度方向的一个例子，GDBT将梯度下降法的优化概理念从参数空间的优化扩展到函数空间的优化。熟悉神经网络的同学应该知道，标准的神经网络是在一个模型上迭代的更新参数\(\theta^{*}\)以达到损失函数最小的期望；在GDBT算法中拓展到函数空间是如何优化的呢？ 浓缩一下：一个变，一个不变！ 一个变：基CART回归树的拟合目标\(y^*\)是在变的，它是上个CART回归树的负梯度(残差)。 一个不变： 每个样本的ground truth是不变的。 GDBT算法： 输入：\((x_i,y_i)_{i=1}^{i=N}\)，迭代次数M，损失函数$$ 输出：\(F(x)=\sum_{j=1}^{j=M}f_j(x)\) 1. 学习第一棵树得到残差： \[ r_{mi}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)} \] 注：初始化第一棵树的负梯度方向（后面的要拟合的\(y_i\)） 2. for m =2:M do: 2.1 拟合上一棵树棵树的残差，计算输出响应\(y_{mi}\):\(y_{i}=[\frac {\partial \ell(y_i,F(x_{i}))}{\partial {F(x_{i})}}]_{F_(x)=F_{m-1}(x)}\) 2.2 学习第m棵树： \(\gamma^*=\arg\min_{w} \sum_{x_i \in R_m}^{}\ell (y_i,F_{m-1}(x_i)+\gamma)\),即利用线性搜索(line search)估计叶结点区域的值，使损失函数极小化 。 2.3 更新树： \[ F_m\left(x\right)=F_{m-1}\left(x\right)+\sum_{j=1}^{J}{\gamma{mj}I\left(x\in R_{mj}\right)},其中J是叶节点 \] 3. 训练结束，输出： \[ \hat{F}\left(x\right)=F_M\left(x\right)=\sum_{m=1}^M{\sum_{j=1}^J{\gamma_{mj}I\left(x\in R_{mj}\right)}} \] —— 参考： 周志华,《机器学习》 jerome H.Friedman,Greedy Function Approximation：A Gradient Boosting Machine 陈天奇，XGBoost 与 Boosted Tree]]></content>
      <categories>
        <category>机器学习算法</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>GDBT</tag>
        <tag>Adaboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析决策树]]></title>
    <url>%2F2018%2F07%2F28%2F%E6%B5%85%E6%9E%90%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[前言： 决策树归纳是从有类标号的训练元组中学习决策模型。常用的决策树算法有ID3，C4.5和CART。它们是采用贪心（即非回溯的）方法，自顶向下递归的分治方法构造。这几个算法选择属性划分的方法各不相同，ID3使用的是信息增益，C4.5使用的是信息增益率，而CART使用的是Gini基尼指数。下面来简单介绍下决策树的理论知识。内容包含决策树的算法构成，熵、信息增益、信息增益率以及Gini指数和树的剪枝的概念及公式。 快速系统的理解一个机器学习算法 算法要解决的问题什么? 算法的输入输出是什么? 算法的实现的机理是什么? 算法的优化方法是什么？ 算法的评价指标是什么？ 决策树的算法构成 决策树归纳是从有类标号的训练元组中学习决策模型。常用的决策树算法有ID3，C4.5和CART。它们都是采用贪心（即非回溯的）方法，自顶向下递归的分治方法构造。这几个算法选择属性划分的方法各不相同，ID3使用的是信息增益，C4.5使用的是信息增益率，而CART使用的是Gini基尼指数。下面来简单介绍下决策树的理论知识。内容包含决策树的算法构成，熵、信息增益、信息增益率以及Gini指数和树的剪枝的概念及公式。 ​ 决策树是是由特征空间作为结点和类空间作为叶子构建的一棵树。： 特征划分逻辑 以每个特征作为判别标志（根节点），用if…then…作为分裂规则(yes or no)，将特征空间划分成互斥且完备的子空间，通过不断地划分子空间，树不断地生长。具体的划分依据和逻辑见第三章。 分类规则： 从统计的角度出发，节点对应的最大条件概率作为分类准则。(要明白似然函数和概率的区别) 优化： 决策树的损失函数通常是正则化的极大似然函数，学习的策略是以损失函数为目标函数的最小化。决策树采用启发式算法来近似求解最优化问题，得到的是次最优的结果。 ​ 该启发式算法可分为三步： 特征选择 模型生成 决策树的剪枝 决策树生成与剪枝 信息量与熵 不管是分类任务还是回归任务，都是在做这样一件事情传递消息（类别or回归值）；特征就是我们对需要传递的消息的一种编码方式即信息。那么怎么衡量信息?信息论定义如下： \[ 信息量：l(x_i)=-log_2p(x_i) \] ​ 有了信息（特征）以后怎么确定这个信息是否可靠的表达消息？我们定义熵来度量信息是否可靠。 \[ 熵：H(x) = -\sum_{i=1}^np_ilog(p_i)\ \] \[ 条件熵： H(Y|X) = H(X,Y)-H(X) = \sum_XP(X)H(Y|X) = -\sum_{X,Y}logP(Y|X) \] 熵越大，说明系统越混乱，携带的信息就越少。熵越小，说明系统越有序，携带的信息就越多。信息的作用就是在于消除不确定性。 均匀分布的不确定性最大，即特征均匀分布（所有特征都一样）熵最大，特征完全没有区分性，无法准确传递消息（类别or回归）。条件熵H（Y|X）描述的是在X给定的条件下Y的不确定性，如果条件熵越小，表示不确定性就越小，那么B就越容易确定结果。 ID3算法与信息增益 ID3划分特征（创建节点的规则）使用的就是信息增益Info-Gain排序。一个特征的信息增益越大，表明该对样本的熵减少的能力就更强，该特征使得数据所属类别的不确定性降低。信息增益描述特征对分类(回归)的不确定性的降低程度，可以用来度量两个变量的相关性。比如，在给定一个变量的条件下，另一个变量它的不确定性能够降低多少，如果不确定性降低得越多，那么它的确定性就越大，就越容易区分，两者就越相关。 信息增益 信息增益在统计学中称为互信息，互信息是条件概率与后验概率的比值，化简之后就可以得到信息增益。所以说互信息其实就是信息增益。计算方法: \[ G(D, A) = H(D) - H(D|A) ，H(D)经验熵，H(D|A)经验条件熵，D表示训练集，A表示特征空间 \] \[ H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}，其中，|C_k|是属于类C_k的个数，|D|是所有样本的个数 \] \[ H(D|A)=\sum_{i=1}^np_{a_i}H(D|a_i)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}，\\特征A有个不同的划分取值\{a_1, a_2, ..., a_n\}，根据特征A的划分取值将D划分为n个子集D_1, D_2, ..., D_n， |D_i|是D_i的样\\本个数，D_ik是中属于C_k类的样本集合。 \] ID3算法流程图 对非空非单一特征空间A: C4.5算法与信息增益比 C4.5算法用信息增益比作为特征选择的依据。算法流程与ID3类似。 使用信息增益作为特征选择的标准时，容易偏向于那些取值比较多的特征，容易导致过拟合。而采用信息增益比则有效地抑制了这个缺点：取值多的特征，以它作为根节点的单节点树的熵很大，导致信息增益比减小，在特征选择上会更加合理。 \[ 信息增益比: g_R(D,A)=\frac{g(D,A)}{H_A(D)}, 其中 H_A(D) = -\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}，n为特征A取值的个数 \] 我的理解就是信息增益比在信息增益上做了个平滑处理，将特征对应的信息增益与特征空间划分的熵相结合起来，约束了不同取值特征的信息增益处理不均衡的问题，在一定程度生类似于标准化，正向统一特征划分的标准。 CART算法 CART(分类回归树)算法是由以特征为节点的多个二叉树串联形成可以完成回归任务和分类任务。CART算法同样由特征选择，树的生成及剪枝组成 ；其中回归树用最小平方误差准则，分类树用基尼指数(Gini index)最小化 准则，进行特征选择，生成二叉树。 1. 最小二乘回归树生成算法 带着问题看算法： 要搞明白最小二乘法是怎么进行特征选择的？ 采用启发式的方法，选择第j个特征和它的取值s作为切分变量和切分点，然后以该特征及切分情况(对应分割区域的输出与真实输出的平方误差和)遍历j的所有取值找到最优的切分点s。具体的优化公式如下： \[ \underset{j,s}{min}[\underset{c_1}{min}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\underset{c_2}{min}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]\\其中，R_1(j,s)=\{x|x^{(j)}≤s\},R_2(j,s)=\{x|x^{(j)}＞s\}，c_1={1\over N_1}\sum_{x_i \in R_1}y_i , \quad c_2={1\over N_2}\sum_{x_i \in R_2}y_i \] 统计学习方法上说 对上述俩个子空间递归调用该步骤,直到满足停止条件。 然后将输入空间划分为M个子区域，生成决策树： \[ f(x) = \sum_{m=1}^Mc_mI(x \in R_m) \] 2. CART分类算法与基尼系数 基尼指数 \[ 基尼指数 ：Gini(D) =1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2，其中C_k是D中属于第k类的子集。\\如果样本集合D根据特征A是否取某一可能值a被分割成D1和D2两部分，即\\ D_1=\{(x,y) \in D| A(x) =a\},D_2 = D-D_1\\则在特征A的条件下，集合D的基尼指数定义为 :Gini(D,A) = \frac{\vert D_1\vert}{\vert D\vert}Gini(D_1)+\frac{\vert D_2\vert}{\vert D\vert}Gini(D_2) \] 问题： 那为什么用基尼指数选择特征而不用信息增益或者信息增益率？​ 分类CART生成： 输入：训练数据集D，停止计算的条件； 输出：CART决策树； 根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树： （1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为“是”或“否”将D分割成D1和D2两部分，利用基尼指数计算公式计算。 （2）在所有可能的特征A以及它们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 （3）对两个子结点递归地调用（1），（2），直至满足停止条件。 （4）生成CART决策树 算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。 决策树的剪枝 因为决策树的生成算法容易构建过于复杂的决策树，产生过拟合。而剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，在这里我们可以联想其实剪枝就是一直正则化手段，简化模型，防止过拟合！下面我们进行详细介绍： 决策树剪枝的基本策略： - 预剪枝(prepruning) 在决策树生成过程中，对每个节点在分割前先进行估计。若当前的分割不能带来决策树泛化性能的提升，则停止分割并将当前节点标记为叶节点 在预剪枝过程中，决策树的很多分支都没有展开，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间和预测时间；但有些分支的当前分割虽不能提升泛化性能、甚至能导致泛化性能下降，但在其基础上的后续分割却有可能导致泛化性能的显著提高，容易导致欠拟合 - 后剪枝(postpurning) 先从训练样本集生成一棵最大规模的完整的决策树，然后自底向上地对非叶结点进行考察。若将该提升决策树的泛化性能，则将该子树替换为叶节点节点对应的子树替换为叶节点能 后剪枝决策树比预剪枝决策树保留了更多的分支。后剪枝过程的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中所有的非叶节点进行逐一考察，因此训练时间开销比未剪枝决策树和预剪枝决策树都要大的多 决策树的剪枝 决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。 设树T的叶结点个数为\(|T|\)，t是树T的叶结点，该叶结点有\(N_t\)个样本结点，其中k类的样本点有\(N_{tk}\)，k = 1, 2, …, K，\(H_t(T)\)为叶结点t上的经验熵，α≥0为参数，则决策树学习的损失函数可以定义为 :\(C_{\alpha}(T_t) = C(T_t) + \alpha|T_t|\) 其中经验熵:\(H_t(T)=-\sum_{i}^{K}\frac{|N_{ik}|}{N_i}log\frac{|N_{ik}|}{N_i}\) 在损失函数中，将第1项记作:\(C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}\) 则：\(C_\alpha(T)=C(T)+\alpha|T|\) C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，\(|T|\)表示模型的复杂度，参数\(\alpha\)控制两者之间的影响。较大的\(\alpha\)促使选择较简单的模型（树），较小的\(\alpha\)促使选择较复杂的模型（树）。\(\alpha=0\)意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 剪枝，就是当α确定时，选择损失函数最小的模型（树）。但α值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。 可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。 损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 决策树剪枝算法： 输入：生成算法产生的整个树T，参数\(\alpha\); 输出：修剪后的子树\(T_\alpha\) （1）计算每个结点的经验熵； （2）递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体树分别为\(T_B\)和\(T_A\)，其对应的损失函数值分别是\(C_\alpha(B)\)和\(C_\alpha(A)\)，如果\(C_\alpha(B)\le C_\alpha(A)\) 则进行剪枝，即父结点变为新的叶结点。 （3）返回（2），直至不能继续为止，得到损失函数最小的子树\(T_\alpha\)。 CART剪枝 相比一般剪枝算法，CART剪枝算法的优势在于，不用提前确定α值，而是在剪枝后从所有子树中找到最优子树对应的\(\alpha\)值。 对于固定的α值，一定存在让\(C_\alpha(T)\)最小的唯一的子树，记\(T_\alpha\)。 决策树小结 决策树算法对比 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类，回归 二叉树 基尼系数，均方差 支持 支持 支持 决策树算法的优缺点 优点： 决策树由明确的规则生成，可解释性强。 基本不需要预处理，不需要提前归一化，处理缺失值。 既可以处理离散值也可以处理连续值。 非参数模型，树模型在不同分布数据上表现更稳定。 可以交叉验证的剪枝来选择模型，从而提高泛化能力。(验证集(CART)也可以发挥调整模型的作用而不仅仅是评估模型) 缺点： 决策树算法非常容易过拟合，导致泛化能力不强。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。 参考： 李航. 统计学习方法 http://www.csuldw.com/2015/05/08/2015-05-08-decision%20tree/ https://applenob.github.io/decision_tree.html https://zealscott.com/posts/54670/]]></content>
      <categories>
        <category>机器学习算法</category>
        <category>树模型</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>CART</tag>
        <tag>ID3</tag>
        <tag>C4.5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用的内置函数]]></title>
    <url>%2F2018%2F07%2F27%2Fpython%E5%B8%B8%E7%94%A8%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[熟悉和掌握python的内置函数，可以在写算法的时候简化代码。 关键字 lambda表达式 :不定义函数名，使用一次的函数 1234&gt;&gt;&gt; a = [(1, 2), (4, 1), (9, 10), (13, -3)]#type :list a.sort(key=lambda x: x[1])#key的用法很高端，规则排序 print(a)&gt;&gt;&gt; [(13, -3), (4, 1), (1, 2), (9, 10)] yield函数:可构建生成器，对可迭代对象进行逐次返回，节约计算资源，如keras的 imagegenerator 1. 可迭代对象`iterable`是实现了`__iter__()`方法的对象，`iter()`方法返回`iterator`对象，通过`next`显示的获取元素,用完一个删一个，当迭代器为空时会抛出异常（StopIteration）（和c++迭代器 （`pop_front`+`iterator++`）理解类似），调用迭代器`for item in iterator:`。 1234567891011121314151617181920#如list是一个可迭代对象包含__iter__()方法，使用iter(list)变成迭代器iterator&gt;&gt;&gt;dir(list)['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']&gt;&gt;&gt; a = [1,2,3,4,5]&gt;&gt;&gt; type(a)list&gt;&gt;&gt; type(iter(a))list_iterator&gt;&gt;&gt; b = iter(a)&gt;&gt;&gt; next(b)1&gt;&gt;&gt; len(b)4#调用迭代器for item in iterator:...&gt;&gt;&gt; for item in b: print (item)2345 生成器generator就是带yield的函数，generator就是一种迭代器。yield相当于在迭代器迭代的时候加了个断点返回元素不终止执行，遇到下一个next()触发一次执行。 总结： 生成器迭代器关系图 可迭代对象(Iterable)是实现了__iter__()方法的对象,通过调用iter()方法可以获得一个迭代器(Iterator) 迭代器(Iterator)是实现了__iter__()和__next__()的对象 for ... in ...的迭代,实际是将可迭代对象转换成迭代器,再重复调用next()方法实现的 生成器(generator)是一个特殊的迭代器,它的实现更简单优雅. yield是生成器实现__next__()方法的关键.它作为生成器执行的暂停恢复点,可以对yield表达式进行赋值,也可以将yield表达式的值返回. 类型转换 tuple：根据传入的参数创建一个新的元组 1234&gt;&gt;&gt; tuple() #不传入参数，创建空元组()&gt;&gt;&gt; tuple('121') #传入可迭代对象。使用其元素创建新的元组('1', '2', '1') list：根据传入的参数创建一个新的列表 1234&gt;&gt;&gt;list() # 不传入参数，创建空列表[] &gt;&gt;&gt; list('abcd') # 传入可迭代对象，使用其元素创建新的列表['a', 'b', 'c', 'd'] dict：根据传入的参数创建一个新的字典 12345678&gt;&gt;&gt; dict() # 不传入任何参数时，返回空字典。&#123;&#125;&gt;&gt;&gt; dict(a = 1,b = 2) # 可以传入键值对创建字典。&#123;'b': 2, 'a': 1&#125;&gt;&gt;&gt; dict(zip(['a','b'],[1,2])) # 可以传入映射函数创建字典。&#123;'b': 2, 'a': 1&#125;&gt;&gt;&gt; dict((('a',1),('b',2))) # 可以传入可迭代对象创建字典。&#123;'b': 2, 'a': 1&#125; enumerate：根据可迭代对象创建枚举对象 1234567&gt;&gt;&gt; seasons = ['Spring', 'Summer', 'Fall', 'Winter']&gt;&gt;&gt; list(enumerate(seasons))[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]&gt;&gt;&gt; list(enumerate(seasons, start=1)) #指定起始值(range(0, 10), range(1, 10), range(1, 10, 3))(range(0, 10), range(1, 10), range(1, 10, 3))[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')] range：根据传入的参数创建一个新的range对象 1234567&gt;&gt;&gt; a = range(10)&gt;&gt;&gt; b = range(1,10)&gt;&gt;&gt; c = range(1,10,3)&gt;&gt;&gt; a,b,c # 分别输出a,b,c(range(0, 10), range(1, 10), range(1, 10, 3))&gt;&gt;&gt; list(a),list(b),list(c) # 分别输出a,b,c的元素([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 4, 7]) iter：根据传入的参数创建一个新的可迭代对象 12345678910111213141516&gt;&gt;&gt; a = iter('abcd') #字符串序列&gt;&gt;&gt; a&lt;str_iterator object at 0x03FB4FB0&gt;&gt;&gt;&gt; next(a)'a'&gt;&gt;&gt; next(a)'b'&gt;&gt;&gt; next(a)'c'&gt;&gt;&gt; next(a)'d'&gt;&gt;&gt; next(a)Traceback (most recent call last): File "&lt;pyshell#29&gt;", line 1, in &lt;module&gt; next(a)StopIteration 序列操作 all：判断可迭代对象的每个元素是否都为True值 12345678&gt;&gt;&gt; all([1,2]) #列表中每个元素逻辑值均为True，返回TrueTrue&gt;&gt;&gt; all([0,1,2]) #列表中0的逻辑值为False，返回FalseFalse&gt;&gt;&gt; all(()) #空元组True&gt;&gt;&gt; all(&#123;&#125;) #空字典True any：判断可迭代对象的元素是否有为True值的元素 12345678&gt;&gt;&gt; any([0,1,2]) #列表元素有一个为True，则返回TrueTrue&gt;&gt;&gt; any([0,0]) #列表元素全部为False，则返回FalseFalse&gt;&gt;&gt; any([]) #空列表False&gt;&gt;&gt; any(&#123;&#125;) #空字典False filter：使用指定方法过滤可迭代对象的元素 1234567&gt;&gt;&gt; a = list(range(1,10)) #定义序列&gt;&gt;&gt; a[1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; def if_odd(x): #定义奇数判断函数 return x%2==1&gt;&gt;&gt; list(filter(if_odd,a)) #筛选序列中的奇数[1, 3, 5, 7, 9] map：使用指定方法去作用传入的每个可迭代对象的元素，生成新的可迭代对象 12345&gt;&gt;&gt; a = str(map(list,['a','b','b','fd']))#每一个可迭代对象进行操作 b = list(['a','b','b','fd'])#对每个元素进行操作&gt;&gt;&gt; a,b[['a'], ['b'], ['b'], ['f', 'd']]['a', 'b', 'b', 'fd'] next：返回可迭代对象中的下一个元素值 1234567891011121314151617181920&gt;&gt;&gt; a = iter('abcd')&gt;&gt;&gt; next(a)'a'&gt;&gt;&gt; next(a)'b'&gt;&gt;&gt; next(a)'c'&gt;&gt;&gt; next(a)'d'&gt;&gt;&gt; next(a)Traceback (most recent call last): File "&lt;pyshell#18&gt;", line 1, in &lt;module&gt; next(a)StopIteration#传入default参数后，如果可迭代对象还有元素没有返回，则依次返回其元素值，如果所有元素已经返回，则返回default指定的默认值而不抛出StopIteration 异常&gt;&gt;&gt; next(a,'e')'e'&gt;&gt;&gt; next(a,'e')'e' reversed：反转序列生成新的可迭代对象 12345&gt;&gt;&gt; a = reversed(range(10)) # 传入range对象&gt;&gt;&gt; a # 类型变成迭代器&lt;range_iterator object at 0x035634E8&gt;&gt;&gt;&gt; list(a)[9, 8, 7, 6, 5, 4, 3, 2, 1, 0] sorted：对可迭代对象进行排序，返回一个新的列表 123456789&gt;&gt;&gt; a = ['a','b','d','c','B','A']&gt;&gt;&gt; a['a', 'b', 'd', 'c', 'B', 'A']&gt;&gt;&gt; sorted(a) # 默认按字符ascii码排序['A', 'B', 'a', 'b', 'c', 'd']&gt;&gt;&gt; sorted(a,key = str.lower) # 转换成小写后再排序，'a'和'A'值一样，'b'和'B'值一样['a', 'A', 'b', 'B', 'c', 'd'] zip：聚合传入的每个迭代器中相同位置的元素，返回一个新的元组类型迭代器 1234&gt;&gt;&gt; x = [1,2,3] #长度3&gt;&gt;&gt; y = [4,5,6,7,8] #长度5&gt;&gt;&gt; list(zip(x,y)) # 取最小长度3[(1, 4), (2, 5), (3, 6)] 对象操作 dir：返回对象或者当前作用域内的属性列表 12&gt;&gt;&gt; dir(list)#非常重要的list属性，leetcode写代码可以直接用！['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort'] type：返回对象的类型，或者根据传入的参数创建一个新的类型 12345678&gt;&gt;&gt; type(1) # 返回对象的类型&lt;class &apos;int&apos;&gt;#使用type函数创建类型D，含有属性InfoD&gt;&gt;&gt; D = type(&apos;D&apos;,(A,B),dict(InfoD=&apos;some thing defined in D&apos;))&gt;&gt;&gt; d = D()&gt;&gt;&gt; d.InfoD &apos;some thing defined in D&apos; len：返回对象的长度 12345678&gt;&gt;&gt; len('abcd') # 字符串&gt;&gt;&gt; len(bytes('abcd','utf-8')) # 字节数组&gt;&gt;&gt; len((1,2,3,4)) # 元组&gt;&gt;&gt; len([1,2,3,4]) # 列表&gt;&gt;&gt; len(range(1,5)) # range对象&gt;&gt;&gt; len(&#123;'a':1,'b':2,'c':3,'d':4&#125;) # 字典&gt;&gt;&gt; len(&#123;'a','b','c','d'&#125;) # 集合&gt;&gt;&gt; len(frozenset('abcd')) #不可变集合 变量操作 globals：返回当前作用域内的全局变量和其值组成的字典 12345&gt;&gt;&gt; globals()&#123;'__spec__': None, '__package__': None, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '__name__': '__main__', '__doc__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;&#125;&gt;&gt;&gt; a = 1&gt;&gt;&gt; globals() #多了一个a&#123;'__spec__': None, '__package__': None, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'a': 1, '__name__': '__main__', '__doc__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;&#125; global：定义全局变量 1234567891011121314&gt;&gt;&gt; def fg(): global a a=1&gt;&gt;&gt; fg()&gt;&gt;&gt; a&gt;&gt;&gt; 1&gt;&gt;&gt;def f(): a=1&gt;&gt;&gt; f()&gt;&gt;&gt; a&gt;&gt;&gt; Traceback (most recent call last): File "&lt;ipython-input-8-60b725f10c9c&gt;", line 1, in &lt;module&gt; a NameError: name 'a' is not defined locals：返回当前作用域内的局部变量和其值组成的字典(在调试函数的时候可以返回local) 123456789101112131415&gt;&gt;&gt; def f(): print('before define a ') print(locals()) #作用域内无变量 a = 1 print('after define a') print(locals()) #作用域内有一个a变量，值为1 b=a**2 return locals()&gt;&gt;&gt; f&lt;function f at 0x03D40588&gt;&gt;&gt;&gt; f()before define a &#123;&#125; after define a&#123;'a': 1, 'b': 1&#125; 交互操作 print：向标准输出对象打印输出 1234567&gt;&gt;&gt; print(1,2,3)1 2 3&gt;&gt;&gt; a = ['a', 'b', 'v', 'f', 's', 'a'] print(a,sep = '+')['a', 'b', 'v', 'f', 's', 'a']&gt;&gt;&gt; print(a,a,sep = '+',end = '=!')['a', 'b', 'v', 'f', 's', 'a']+['a', 'b', 'v', 'f', 's', 'a']=！ input：读取用户输入值 1234&gt;&gt;&gt; file = input('please enter your filename:')please input your name:cat.png&gt;&gt;&gt; file'cat.png' 文件操作 open：使用指定的模式和编码打开文件，返回文件读写对象 123456789# rb为二进制读,wb为二进制写操作&gt;&gt;&gt; f = open('test.txt','rt')&gt;&gt;&gt; f.read()'read test'&gt;&gt;&gt; f.close()#读操作: #read()将文本文件所有行读到一个字符串中。 #readline()是一行一行的读 #readlines()是将文本文件中所有行读到一个list中，文本文件每一行是list的一个元素。 参考： Python内置函数详解——总结篇 Python之生成器详解]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>list</tag>
        <tag>yield</tag>
        <tag>map</tag>
        <tag>zip</tag>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 排好序的数组合并求中位数(复杂度限制O(log (m+n)))]]></title>
    <url>%2F2018%2F07%2F25%2FLeetcode%20%E6%8E%92%E5%A5%BD%E5%BA%8F%E7%9A%84%E6%95%B0%E7%BB%84%E5%90%88%E5%B9%B6%E6%B1%82%E4%B8%AD%E4%BD%8D%E6%95%B0(%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%99%90%E5%88%B6O(log%20(m%2Bn)))%2F</url>
    <content type="text"><![CDATA[Leetcode 第四题 - There are two sorted arrays nums1 and nums2 of size m and n respectively. 1- Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). Solution:思路：将俩个数组合并成sorted数组，找中位数 123456789101112131415161718192021222324252627282930313233343536373839'''俩个数组排序合并，找中位数'''class Solution: def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ mergnums=[] i = 0 j = 0 print (len(nums1),len(nums2)) nums1 = list(nums1) nums2 = list(nums2) while True: if nums1!=[] and nums2 !=[]: if nums1[i]&lt;=nums2[j]: mergnums.append(nums1[i]) del nums1[i] else: mergnums.append(nums2[j]) del nums2[j] print (j) elif nums1 !=[] and nums2 ==[]: mergnums.append(nums1[i]) del nums1[i] elif nums1 ==[] and nums2 !=[]: mergnums.append(nums2[j]) del nums2[j] else: break if (len(mergnums))%2 ==0: median = mergnums[int((len(mergnums)-2)/2)] + mergnums[int(len(mergnums)/2)] median = median/2.0 else: median = mergnums[int(len(mergnums)/2)] return median ​ ​]]></content>
      <categories>
        <category>-Leetcode</category>
      </categories>
      <tags>
        <tag>-合并数组求中位数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前言]]></title>
    <url>%2F2018%2F07%2F23%2F%E5%89%8D%E8%A8%80%2F</url>
    <content type="text"><![CDATA[希望自己能有这个博客来鼓励自己养成总结归纳的好习惯！工具性代码还得review自己写的代码！ 向大佬们学习！ 博客从找工作的角度展开，从算法，算法编程练习俩个角度开始写博客。 算法： 机器学习基础： [ ] KNN [ ] Kmeans [ ] SVM [ ] 朴素贝叶斯 [ ] LR [ ] RF [x] GDBT [x] Adaboost [ ] LightGBM [ ] Xgboost 深度学习基础： [x] CNN 原理与架构发展 [ ] 反向传播推导 [ ] 正则化方法 [ ] 损失函数 [ ] CNN复杂度计算与模型压缩 [ ] SSD [ ] Retinanet [ ] YOLO系列 [ ] RCNN系列 深度学习优化方法： [ ] 梯度下降优化算法 项目方法总结： [ ] CTR预估算法 [ ] Multi-label imbalance Classification [ ] Zero-shot learing 编程相关 以后工作作为技术积累的记录 快乐Farm每一天！]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>题记</tag>
      </tags>
  </entry>
</search>
