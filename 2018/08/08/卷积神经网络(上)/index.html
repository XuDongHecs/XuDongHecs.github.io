<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="信号卷积,组卷积,空洞卷积,可分离卷积,1x1卷积," />










<meta name="description" content="很多人在接触卷积神经网络之前就接触过卷积运算，比如信号与系统的连续信号卷积和离散信号卷积。本文将我的理解出发对比卷积神经网络的卷积和我们信号卷积的区别与联系。介绍卷积神经网络中的卷积操作，以及不同的卷积策略。为后续深入高效小网络及模型压缩相关概念和原理做准备。">
<meta name="keywords" content="信号卷积,组卷积,空洞卷积,可分离卷积,1x1卷积">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络（上）">
<meta property="og:url" content="http://yoursite.com/2018/08/08/卷积神经网络(上)/index.html">
<meta property="og:site_name" content="何先生的Blog">
<meta property="og:description" content="很多人在接触卷积神经网络之前就接触过卷积运算，比如信号与系统的连续信号卷积和离散信号卷积。本文将我的理解出发对比卷积神经网络的卷积和我们信号卷积的区别与联系。介绍卷积神经网络中的卷积操作，以及不同的卷积策略。为后续深入高效小网络及模型压缩相关概念和原理做准备。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/Convolution_of_box_signal_with_itself2.gif">
<meta property="og:image" content="http://yoursite.com/images/Convolution_of_spiky_function_with_box2.gif">
<meta property="og:image" content="http://yoursite.com/images/automatic_investment_plan.gif">
<meta property="og:image" content="http://yoursite.com/images/滑动卷积.gif">
<meta property="og:image" content="http://yoursite.com/images/hudongjuanji.gif">
<meta property="og:image" content="http://yoursite.com/images/池化.png">
<meta property="og:image" content="http://yoursite.com/images/全连接.jpg">
<meta property="og:image" content="http://yoursite.com/images/可分离卷积.jpg">
<meta property="og:image" content="http://yoursite.com/images/普通卷积.jpg">
<meta property="og:image" content="http://yoursite.com/images/空洞卷积静态图.jpg">
<meta property="og:image" content="http://yoursite.com/images/gc0.jpg">
<meta property="og:image" content="http://yoursite.com/images/gc1.jpg">
<meta property="og:image" content="http://yoursite.com/images/反卷积.gif">
<meta property="og:updated_time" content="2018-08-07T19:16:00.774Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷积神经网络（上）">
<meta name="twitter:description" content="很多人在接触卷积神经网络之前就接触过卷积运算，比如信号与系统的连续信号卷积和离散信号卷积。本文将我的理解出发对比卷积神经网络的卷积和我们信号卷积的区别与联系。介绍卷积神经网络中的卷积操作，以及不同的卷积策略。为后续深入高效小网络及模型压缩相关概念和原理做准备。">
<meta name="twitter:image" content="http://yoursite.com/images/Convolution_of_box_signal_with_itself2.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/08/卷积神经网络(上)/"/>





  <title>卷积神经网络（上） | 何先生的Blog</title>
  








  
  <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">何先生的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">该搬的砖一块都少不了，Come on!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-baidusitemap">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            baidusitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/08/卷积神经网络(上)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="会旋转的霸东">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="何先生的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">卷积神经网络（上）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-08T00:17:12+08:00">
                2018-08-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/卷积神经网络/" itemprop="url" rel="index">
                    <span itemprop="name">卷积神经网络</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/08/卷积神经网络(上)/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/08/卷积神经网络(上)/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>  很多人在接触卷积神经网络之前就接触过卷积运算，比如信号与系统的连续信号卷积和离散信号卷积。本文将我的理解出发对比卷积神经网络的卷积和我们信号卷积的区别与联系。介绍卷积神经网络中的卷积操作，以及不同的卷积策略。为后续深入高效小网络及模型压缩相关概念和原理做准备。</p>
<a id="more"></a>
<h2 id="信号卷积">信号卷积</h2>
<p>  卷积表征函数<em>f</em>与经过翻转和平移的<em>g</em>的乘积函数所围成的曲边梯形的面积。信号与系统中，采样信号经过系统响应得到系统输出。一维卷积的数学表达： <span class="math display">\[
离散卷积：z[n]=(x * y)[n] = \sum_{m = -\infty}^{+\infty}x[m]\cdot y[n - m]\\连续卷积： y(x)=(f * h)(t) = \int_{-\infty}^{+\infty}f(\tau)\cdot h(t - \tau)\mathop{}\!\mathrm{d}\tau
\]</span>   我们采集到的信号通常是离散的，因此，后续讲解的就是离散卷积。看下维基百科的卷积可视化。 <img src="/images/Convolution_of_box_signal_with_itself2.gif" alt="离散卷积1"> <img src="/images/Convolution_of_spiky_function_with_box2.gif" alt="离散卷积2"></p>
<figure>
<img src="/images/automatic_investment_plan.gif" alt="离散卷积3"><figcaption>离散卷积3</figcaption>
</figure>
<p>  一维离散卷积比较好理解，每个输出元素都是卷积核（系统响应）和信号的局部乘积的和。卷积操作是一种滤波操作，信号与系统的高通滤波，低通滤波等，因此，通过卷积操作可以获得某个方面属性的增强。一维离散卷积通常是在时域卷积，图像，激光雷达回波等属于空间属性。</p>
<h2 id="cnn的卷积">CNN的卷积</h2>
<h3 id="卷积计算">卷积计算</h3>
<p>  以图像为例。图像卷积即二维离散卷积，图像的矩阵表示为<span class="math inline">\(h*w\)</span>，元素数值大小为无符号整型通常在[0,255]之间。二维离散卷积的公式定义： <span class="math display">\[
S[i, j] = (F ∗ K)(i, j) = \sum_m\sum_n F(m, n)K(i − m, j − n)
\]</span> 卷积是可交换的(commutative)，该公式可转变为： <span class="math display">\[
S[i, j] = (F ∗ K)(i, j) = \sum_m\sum_n F(i-m, i-n)K( m, n),K即我们常说的卷积核
\]</span></p>
<p>图像卷积示例：</p>
<figure>
<img src="/images/滑动卷积.gif" alt="二维卷积示例1"><figcaption>二维卷积示例1</figcaption>
</figure>
<p>  参数：输入Shape：<span class="math inline">\((7\times7\times3)\)</span> 卷积核Shape:<span class="math inline">\(3\times3\times3\times2\)</span> <span class="math inline">\(zero-padding=1 stride=2\)</span></p>
<p>输出Shape计算： <span class="math display">\[
\begin{align}
W_2 &amp;= (W_1 - F + 2P)/S + 1\qquad(式2)\\
H_2 &amp;= (H_1 - F + 2P)/S + 1\qquad(式3)
\end{align}
\]</span></p>
<h3 id="卷积层">卷积层</h3>
<p>  卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。上一节中描述的卷积层的神经元和全连接网络一样都是一维结构。既然卷积网络主要应用在图像处理上，而图像为两维结构，因此为了更充分地利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度H×宽度W×深度C，有C个H ×W 大小的特征映射构成。   特征映射（feature map）为一幅图像（或其它特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征。为了卷积网络的表示能力，可以在每一层使用多个不同的特征映射，以更好地表示图像的特征。</p>
<p><strong>卷积层的关键参数</strong>：</p>
<ul>
<li><strong>卷积核大小（Kernel Size）</strong>：定义了卷积操作的感受野。在二维卷积中，通常设置为3，即卷积核大小为3×3。</li>
<li><strong>步幅（Stride）</strong>：定义了卷积核遍历图像时的步幅大小。其默认值通常设置为1，也可将步幅设置为2后对图像进行下采样，这种方式与最大池化类似。</li>
<li><strong>边界扩充（Padding）</strong>：定义了网络层处理样本边界的方式。当卷积核大于1且不进行边界扩充，输出尺寸将相应缩小；当卷积核以标准方式进行边界扩充，则输出数据的空间尺寸将与输入相等。</li>
<li><strong>输入与输出通道（Channels）</strong>：构建卷积层时需定义输入通道I，并由此确定输出通道O。这样，可算出每个网络层的参数量为I×O×K，其中K为卷积核的参数个数。例，某个网络层有64个大小为3×3的卷积核，则对应K值为 3×3 =9。</li>
</ul>
<p><strong>卷积层特性</strong>：</p>
<ul>
<li><p><strong>局部连接</strong>：卷积计算的通俗理解就是卷积核在图像上做滑动卷积，每次滑动输出一个值，局部乘积求和。局部计算如：</p>
<figure>
<img src="/images/hudongjuanji.gif" alt="局部求和"><figcaption>局部求和</figcaption>
</figure></li>
<li><p><strong>权值共享</strong>：整张图像可以共用一个卷积核，对比全连接操作卷积不需要同时和每个像素点相乘求和(内积)。</p></li>
</ul>
<h3 id="池化pooling">池化（pooling）</h3>
<p>  池化也叫子采样（subsampling layer），其作用是进行特征选择，降低特征数量，并从而减少参数数量。</p>
<p>  标准的卷积操作虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以在卷积操作之后加上一个池化操作，从而降低特征维数，避免过拟合。（也可通过增加滑动卷积的步长Stride来达到降维效果）</p>
<p>  池化操作虽然说有降维功能，同时也是存在信息损失的。</p>
<figure>
<img src="/images/池化.png" alt="池化"><figcaption>池化</figcaption>
</figure>
<h4 id="常见的池化方式">常见的池化方式</h4>
<ul>
<li>最大池化（max-pooling）</li>
</ul>
<p>  一般是在池化核对应区域内选取最大值来表征该区域。</p>
<ul>
<li>平均池化（mean-pooling）</li>
</ul>
<p>  一般是取池化核对应区域内所有元素的均值来表征该区域。</p>
<ul>
<li>全局池化（global pooling）</li>
</ul>
<p>  一般是对整个特征图进行最大池化或者平均池化。</p>
<ul>
<li>重叠池化（overlapping-pooling）</li>
</ul>
<p>  相邻池化窗口之间会有重叠区域，此时kernel size&gt;stride</p>
<h3 id="卷积神经网络中的特殊卷积操作">卷积神经网络中的特殊卷积操作</h3>
<h4 id="times1卷积核"><span class="math inline">\(1\times1\)</span>卷积核</h4>
<p>  自从GoogLenet接手发扬光大以后，<span class="math inline">\(1\times1\)</span>卷积广泛出现在新的卷积架构中如xception和小卷积网络中。<span class="math inline">\(1\times1\)</span> 卷积核在单通道上就是给每个像素点同等乘以一个系数，多通道上其实现了通道间的信息流通。从全连接的角度解释即C通道特征图，和C个<span class="math inline">\(1\times1\)</span>卷积核卷积输出1个新的特征图。同理，和K<em>C个1x1的卷积核卷积输出K个特征图。注：全连接运算是元素级的，<span class="math inline">\(1\times1\)</span>卷积是通道级的，对于H</em>W * C的特征图全连接需要K * H * W * C，而1<em>1卷积仅需要K</em>C个参数。图片来自知乎。 <img src="/images/全连接.jpg" alt="全连接"></p>
<h5 id="times1卷积的作用"><span class="math inline">\(1\times1\)</span>卷积的作用</h5>
<ul>
<li>跨通道信息交互（channal 的变换）</li>
<li>升维（用最少的参数拓宽网络channal）</li>
<li>降维（减少参数）</li>
</ul>
<h4 id="深度可分离卷积depthwise-separable-convolution">深度可分离卷积（depthwise separable convolution）</h4>
<p>  深度可分离卷积在执行空间卷积，同时保持通道分离，然后进行深度卷积。 可分离卷积与标准的卷积相比，其在通道上先做了卷积然后在再用<span class="math inline">\(1\times1\)</span>卷积进行通道融合。示意图：</p>
<figure>
<img src="/images/可分离卷积.jpg" alt="可分离卷积"><figcaption>可分离卷积</figcaption>
</figure>
<p>对比标准卷积过程：</p>
<figure>
<img src="/images/普通卷积.jpg" alt="标准卷积"><figcaption>标准卷积</figcaption>
</figure>
<p>  假设我们在16个输入通道和32个输出通道上有一个3x3卷积层。详细情况是，16个3x3内核遍历16个通道中的每一个，产生512（16x32）个特征映射。接下来，我们通过添加它们来合并每个输入通道中的1个特征图。由于我们可以做32次，我们得到了我们想要的32个输出通道。</p>
<p>  针对这个例子应用深度可分离卷积，用1个3×3大小的卷积核遍历16通道的数据，得到了16个特征图谱。在融合操作之前，接着用32个1×1大小的卷积核遍历这16个特征图谱，进行相加融合。这个过程使用了16×3×3+16×32×1×1=656个参数，远少于上面的16×32×3×3=4608个参数。</p>
<p>  这个例子就是深度可分离卷积的具体操作，其中上面的深度乘数（depth multiplier）设为1，这也是目前这类网络层的通用参数。这么做是为了对空间信息和深度信息进行去耦。从Xception模型的效果可以看出，这种方法是比较有效的。由于能够有效利用参数，因此深度可分离卷积也可以用于移动设备中。</p>
<h4 id="空洞卷积dilated-convolution">空洞卷积（dilated convolution）</h4>
<p>  dilated convolution是针对图像语义分割问题中下采样会降低图像分辨率、丢失信息而提出的一种卷积思路。</p>
<p>  空洞卷积的特点是在扩大感受野的同时不增加计算成本。示意图：</p>
<figure>
<img src="/images/空洞卷积静态图.jpg" alt="空洞卷积静态图"><figcaption>空洞卷积静态图</figcaption>
</figure>
<p>  上图b可以理解为卷积核大小依然是3×3，但是每个卷积点之间有1个空洞，也就是在绿色7×7区域里面，只有9个红色点位置作了卷积处理，其余点权重为0。这样即使卷积核大小不变，但它看到的区域变得更大了 。</p>
<p>     <strong>空洞卷积的动机：加pooling层，损失信息，降低精度；不加pooling层，感受野变小，模型学习不到全局信息</strong></p>
<h4 id="组卷积group-convolution">组卷积（Group convolution）</h4>
<p>  分组卷积（Group convolution） ，最早在AlexNet中出现，由于当时的硬件资源有限，训练AlexNet时卷积操作不能全部放在同一个GPU处理，因此作者把feature maps分给多个GPU分别进行处理，最后把多个GPU的结果进行融合（concatenate）。</p>
<p>下图分别是一个正常的、没有分组的卷积层结构和分组卷积结构的示意图：</p>
<figure>
<img src="/images/gc0.jpg" alt="组卷积"><figcaption>组卷积</figcaption>
</figure>
<figure>
<img src="/images/gc1.jpg" alt="标准卷积_分组"><figcaption>标准卷积_分组</figcaption>
</figure>
<p>组卷积通过将卷积运算的输入限制在每个组内，模型的计算量取得了显著的下降。然而这样做也带来了明显的问题：在多层逐点卷积堆叠时，模型的信息流被分割在各个组内，组与组之间没有信息交换。这将可能影响到模型的表示能力和识别精度。 目前旷世科技提出的ShuffleNet引入通道重排来处理组和组之间的信息流通。</p>
<h4 id="转置卷积">转置卷积</h4>
<p>反卷积是一种上采样操作，在图像分割和卷积自编码中应用较多。示意图：</p>
<figure>
<img src="/images/反卷积.gif" alt="转置卷积"><figcaption>转置卷积</figcaption>
</figure>
<h3 id="小结">小结</h3>
<p>  本文简单的介绍了信号卷积的算法原理，引入二维离散卷积引导对卷积神经网络的卷积的理解。卷积是一种滤波操作，在卷积神经网络中有许多卷积变形结构，这些结构从维度控制，信息流通，模型参数大小控制，感受野等角度进行设计。在当前state of art卷积架构中多是融入了部分或者全部上述变形卷积。在设计新的模型时，可从设计目标出发采用上述结构优化，裁剪模型。</p>
<p>  下一篇回顾卷积神经网络经典的Blocks。欢迎关注。</p>
<h2 id="参考">参考</h2>
<blockquote>
<ol type="1">
<li><p>邱锡鹏 ,《神经网络与深度学习》https://nndl.github.io/&gt;</p></li>
<li><p>https://zhuanlan.zhihu.com/p/28749411</p></li>
<li><p>https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d</p></li>
<li><p>https://www.zhihu.com/question/54149221</p></li>
<li><p>https://www.cnblogs.com/ranjiewen/articles/8699268.html</p></li>
<li><p>https://blog.csdn.net/A_a_ron/article/details/79181108</p></li>
</ol>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/信号卷积/" rel="tag"># 信号卷积</a>
          
            <a href="/tags/组卷积/" rel="tag"># 组卷积</a>
          
            <a href="/tags/空洞卷积/" rel="tag"># 空洞卷积</a>
          
            <a href="/tags/可分离卷积/" rel="tag"># 可分离卷积</a>
          
            <a href="/tags/1x1卷积/" rel="tag"># 1x1卷积</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/02/深入boosting经典算法/" rel="next" title="深入boosting经典算法">
                <i class="fa fa-chevron-left"></i> 深入boosting经典算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">会旋转的霸东</p>
              <p class="site-description motion-element" itemprop="description">写个博客，做做总结</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/XuDongHecs" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#信号卷积"><span class="nav-number">1.</span> <span class="nav-text">信号卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cnn的卷积"><span class="nav-number">2.</span> <span class="nav-text">CNN的卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积计算"><span class="nav-number">2.1.</span> <span class="nav-text">卷积计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积层"><span class="nav-number">2.2.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#池化pooling"><span class="nav-number">2.3.</span> <span class="nav-text">池化（pooling）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#常见的池化方式"><span class="nav-number">2.3.1.</span> <span class="nav-text">常见的池化方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积神经网络中的特殊卷积操作"><span class="nav-number">2.4.</span> <span class="nav-text">卷积神经网络中的特殊卷积操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#times1卷积核"><span class="nav-number">2.4.1.</span> <span class="nav-text">\(1\times1\)卷积核</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#times1卷积的作用"><span class="nav-number">2.4.1.1.</span> <span class="nav-text">\(1\times1\)卷积的作用</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度可分离卷积depthwise-separable-convolution"><span class="nav-number">2.4.2.</span> <span class="nav-text">深度可分离卷积（depthwise separable convolution）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#空洞卷积dilated-convolution"><span class="nav-number">2.4.3.</span> <span class="nav-text">空洞卷积（dilated convolution）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#组卷积group-convolution"><span class="nav-number">2.4.4.</span> <span class="nav-text">组卷积（Group convolution）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#转置卷积"><span class="nav-number">2.4.5.</span> <span class="nav-text">转置卷积</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结"><span class="nav-number">2.5.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">3.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">会旋转的霸东</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/08/08/卷积神经网络(上)/';
          this.page.identifier = '2018/08/08/卷积神经网络(上)/';
          this.page.title = '卷积神经网络（上）';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
