<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ResNet,Inception,SENet,ShuffleNet,MobileNet," />










<meta name="description" content="上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNe">
<meta name="keywords" content="ResNet,Inception,SENet,ShuffleNet,MobileNet">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络模型简述">
<meta property="og:url" content="http://yoursite.com/2018/12/23/卷积神经网络模型简述/index.html">
<meta property="og:site_name" content="何先生的Blog">
<meta property="og:description" content="上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNe">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/CNN%20modules_acc_vs_net_vs_ops.png">
<meta property="og:image" content="http://yoursite.com/images/神经网络经典结构.png">
<meta property="og:image" content="http://yoursite.com/images/基础卷积架构.png">
<meta property="og:image" content="http://yoursite.com/images/lenet-5.png">
<meta property="og:image" content="http://yoursite.com/images/alexnet.jpg">
<meta property="og:image" content="http://yoursite.com/images/AlexNet-Ag.png">
<meta property="og:image" content="http://yoursite.com/images/VGG.jpg">
<meta property="og:image" content="http://yoursite.com/images/GoogLeNet-incaption.png">
<meta property="og:image" content="http://yoursite.com/images/1x3卷积核代替3x3卷积.jpg">
<meta property="og:image" content="http://yoursite.com/images/使用一维卷积核代替二维卷积核.jpg">
<meta property="og:image" content="http://yoursite.com/images/shortcut.jpg">
<meta property="og:image" content="http://yoursite.com/images/ResNet-34.jpg">
<meta property="og:image" content="http://yoursite.com/images/ResNet_mode.jpg">
<meta property="og:image" content="http://yoursite.com/images/DenseNetBlock.png">
<meta property="og:image" content="http://yoursite.com/images/densenet.png">
<meta property="og:image" content="http://yoursite.com/images/SENet.jpg">
<meta property="og:updated_time" content="2018-12-23T11:53:24.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷积神经网络模型简述">
<meta name="twitter:description" content="上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNe">
<meta name="twitter:image" content="http://yoursite.com/images/CNN%20modules_acc_vs_net_vs_ops.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/12/23/卷积神经网络模型简述/"/>





  <title>卷积神经网络模型简述 | 何先生的Blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee745d05c094ff5e173cd1223073f1ef";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  
  <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">何先生的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">该搬的砖一块都少不了，Come on!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-baidusitemap">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            baidusitemap
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/23/卷积神经网络模型简述/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="会旋转的霸东">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="何先生的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">卷积神经网络模型简述</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-23T19:53:52+08:00">
                2018-12-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/卷积神经网络/" itemprop="url" rel="index">
                    <span itemprop="name">卷积神经网络</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/12/23/卷积神经网络模型简述/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/12/23/卷积神经网络模型简述/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/12/23/卷积神经网络模型简述/" class="leancloud_visitors" data-flag-title="卷积神经网络模型简述">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  10
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>  上篇博客从信号处理的角度解析了卷积计算和卷积神经网络中的卷积卷积操作。本文主要梳理一遍经典的卷积架构，如：ResNet，Inception架构，这些架构在CV任务上的表现十分出色，同时也给众多深度学习提供了新的思路，例如后来的DenseNet、ResNeXt等。以后有时间将扩展，偏功能的卷积架构有，在细粒度特征上可以采用SPPNet以及Few-shot learning 上的SiameseNet，图像分割中的FCN。本文将以LeNet简单介绍卷积基础架构开始，以讲述Blocks演变的形式简单讲解不同网络的特点。</p>
<a id="more"></a>
<figure>
<img src="/images/CNN%20modules_acc_vs_net_vs_ops.png" alt="卷积模型发展"><figcaption>卷积模型发展</figcaption>
</figure>
<h2 id="卷机神经网络基础架构">卷机神经网络基础架构</h2>
<p>  典型的神经网络由一个输入层，多个隐藏层和一个输出层组成，在卷积神经网络中称为全连接。对于每一层的输出：<span class="math inline">\(y_i=f(W_{i}x +b)，其中f(x)为激活函数\)</span>；其经典结构如下图所示：</p>
<figure>
<img src="/images/神经网络经典结构.png" alt="神经网络结构"><figcaption>神经网络结构</figcaption>
</figure>
<p>  卷积神经网络的基础结构如下图所示：</p>
<figure>
<img src="/images/基础卷积架构.png" alt="完整的卷积结构"><figcaption>完整的卷积结构</figcaption>
</figure>
<p>  卷积神经网络采用的权值共享，相对与全连接操作，通常减少了数量级的参数；此外，卷积神经网络的局部操作，在处理具有局部相关性的任务时，与全连接相比具有明显的优势。</p>
<h4 id="todo">TODO:</h4>
<p>卷积神经网络的构成成分：</p>
<ul>
<li>[x] 卷积层</li>
<li>[x] 池化层</li>
<li>[x] 全连接层（FCN和输出层前是Global_pooling的架构没有）</li>
<li>激活函数</li>
<li>Batch Nomalization(BN层)</li>
<li>局部响应归一化（LRN）</li>
<li>Dropout层</li>
</ul>
<h2 id="lenet5">LeNet5</h2>
<p>  LeNet5架构是一个开创性的工作。图像特征是全局和局部的统一，LeNet5利用一组相同的卷积核在特征图的一个通道上对全局特征进行滤波处理，同时融合局部特征，利用下采样压缩局部的相似特征。当时没有GPU来帮助训练，甚至CPU速度都非常慢。因此，对比使用每个像素作为一个单独的输入的多层神经网络，LeNet5能够节省参数和计算是一个关键的优势。LeNet5论文中提到，全连接不应该被放在第一层，因为图像中有着高度的空间相关性，并利用图像各个像素作为单独的输入特征不会利用这些相关性。因此有了CNN的三个特性了：<strong>1.局部感知、2.平移不变性（下采样）、3.权值共享</strong>。</p>
<figure>
<img src="/images/lenet-5.png" alt="LeNet-5"><figcaption>LeNet-5</figcaption>
</figure>
<h2 id="alexnet">AlexNet</h2>
<p>AlexNet特点：</p>
<ul>
<li>由五层卷积和三层全连接组成，输入图像为三通道 224x224 大小，网络规模远大于 LeNet-5</li>
<li>使用了 ReLU 激活函数，提高了训练速度</li>
<li>使用了 Dropout，可以作为正则项防止过拟合，提升模型鲁棒性</li>
<li>加入了局部响应归一化（LRN）提高了精度</li>
<li>引入最大池化</li>
<li>在训练时使用了分组卷积操作（参见《卷积神经网络（上）》）</li>
<li>一些很好的训练技巧，包括数据增广、学习率策略、weight decay 等</li>
</ul>
<p>AlexNet架构：</p>
<figure>
<img src="/images/alexnet.jpg" alt="AlexNet"><figcaption>AlexNet</figcaption>
</figure>
<figure>
<img src="/images/AlexNet-Ag.png" alt="Ng-AlexNet"><figcaption>Ng-AlexNet</figcaption>
</figure>
<h2 id="vgg">VGG</h2>
<p>  VGG是一种更简单的架构模型，因为它没有使用太多的超参数。它总是使用3 x 3滤波器，在卷积层中步长为1，并使用SAME填充在2 x 2池中，步长为2。</p>
<ul>
<li><p>VGG使用3*3卷积核级联提高感受野，一改LeNet-5和AlexNet的<code>卷积-池化</code>结构。</p></li>
<li><p>VGG模型更深，参数更少，后续的模型基本都遵循了小卷积核级联的设计风格。</p>
<figure>
<img src="/images/VGG.jpg" alt="VGG"><figcaption>VGG</figcaption>
</figure></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keras API</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">block2_conv</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv1'</span>)(x)</span><br><span class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv2'</span>)(x)</span><br><span class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block2_pool'</span>)(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="googlenet">GoogLeNet</h2>
<p>  2014年ILSVRC的获胜者提出GoogLeNet架构也称为InceptionV1模型。它在具有不同感受野大小的并行路径中更深入，并且降低了Top5错误率到6.67％。该架构由22层深层组成。它将参数数量从6000万（AlexNet）减少到500万。<strong>GoogLeNet在使用了中间输出做了多任务学习，防止网络过深造成的梯度消失的情况；每层使用多个卷积核</strong>。</p>
<figure>
<img src="/images/GoogLeNet-incaption.png" alt="GoogLeNet-inception"><figcaption>GoogLeNet-inception</figcaption>
</figure>
<h2 id="inception-v2">Inception V2</h2>
<p>  Iception V2 主要提出了Batch Nomalization(BN)，BN层对mini-batch内部数据进行标准化，再给标准化数据乘以权重和加bias，保证了模型可以学习回原来的分布。用了BN层后减少或者取消 <strong>LRN</strong>。</p>
<ul>
<li>[ ] 后续有时间再写一个博客专门写深度学习的正则化， 可以关注<a href="https://mp.weixin.qq.com/s/uQUUI9G10SOUQwvYqG78Mg" target="_blank" rel="noopener">深度学习中的Nomalization中的分析</a>。</li>
</ul>
<h2 id="inception-v3">Inception V3</h2>
<p>  Inception V3将一个较大的二维卷积拆成两个较小的一维卷积，比如将7x7卷积拆成1x7卷积和7x1卷积，或者将3x3卷积拆成1x3卷积和3x1卷积，另外也使用了将5x5 用两个 3x3 卷积替换，7x7 用三个 3x3 卷积替换，如下图所示。一方面节约了大量参数，加速运算并减轻了过拟合，同时增加了一层非线性扩展模型表达能力。论文中指出，这种非对称的卷积结构拆分，其结果比对称地拆为几个相同的小卷积核效果更明显，可以处理更多、更丰富的空间特征，增加特征多样性。示意图如下：</p>
<figure>
<img src="/images/1x3卷积核代替3x3卷积.jpg" alt="1x3卷积核代替3x3卷积核"><figcaption>1x3卷积核代替3x3卷积核</figcaption>
</figure>
<figure>
<img src="/images/使用一维卷积核代替二维卷积核.jpg" alt="使用一维卷积核代替二维卷积核"><figcaption>使用一维卷积核代替二维卷积核</figcaption>
</figure>
<h2 id="resnet">ResNet</h2>
<p>  ILSRVC 2015的获胜者，被何凯明大神称为残差网络（ResNet）。该架构引入了一个名为“skip connections”的概念。ResNet采用Shortcut单元实现信息的跨层流通。与Inception 模块的Concat操作不同，Residual 模块使用 Add 操作完成信息融合。</p>
<figure>
<img src="/images/shortcut.jpg" alt="shotcut"><figcaption>shotcut</figcaption>
</figure>
<p>  通过引入直连，原来需要学习完全的重构映射，从头创建输出，并不容易，而引入直连之后，只需要学习输出和原来输入的差值即可，绝对量变相对量，容易很多，所以叫残差网络。并且，通过引入残差，identity 恒等映射，相当于一个梯度高速通道，可以容易地训练避免梯度消失的问题，所以可以得到很深的网络，网络层数由 GoogLeNet 的 22 层到了ResNet的 152 层。然而，恒等函数与<span class="math inline">\(H_{\ ell}\)</span>的输出是通过求和组合，这可能阻碍网络中的信息流。</p>
<p>  <a href="https://github.com/keras-team/keras-applications/tree/master/keras_applications" target="_blank" rel="noopener">keras_applications实现的Identity Block:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(input_tensor, kernel_size, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    The identity block is the block that has no conv layer at shortcut.</span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        input_tensor: input tensor</span></span><br><span class="line"><span class="string">        kernel_size: default 3, the kernel size of</span></span><br><span class="line"><span class="string">            middle conv layer at main path</span></span><br><span class="line"><span class="string">        filters: list of integers, the filters of 3 conv layer at main path</span></span><br><span class="line"><span class="string">        stage: integer, current stage label, used for generating layer names</span></span><br><span class="line"><span class="string">        block: 'a','b'..., current block label, used for generating layer names</span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        Output tensor for the block.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    filters1, filters2, filters3 = filters</span><br><span class="line">    <span class="keyword">if</span> backend.image_data_format() == <span class="string">'channels_last'</span>:</span><br><span class="line">        bn_axis = <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bn_axis = <span class="number">1</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line"></span><br><span class="line">    x = layers.Conv2D(filters1, (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                      kernel_initializer=<span class="string">'he_normal'</span>,</span><br><span class="line">                      name=conv_name_base + <span class="string">'2a'</span>)(input_tensor)</span><br><span class="line">    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + <span class="string">'2a'</span>)(x)</span><br><span class="line">    x = layers.Activation(<span class="string">'relu'</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = layers.Conv2D(filters2, kernel_size,</span><br><span class="line">                      padding=<span class="string">'same'</span>,</span><br><span class="line">                      kernel_initializer=<span class="string">'he_normal'</span>,</span><br><span class="line">                      name=conv_name_base + <span class="string">'2b'</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + <span class="string">'2b'</span>)(x)</span><br><span class="line">    x = layers.Activation(<span class="string">'relu'</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = layers.Conv2D(filters3, (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                      kernel_initializer=<span class="string">'he_normal'</span>,</span><br><span class="line">                      name=conv_name_base + <span class="string">'2c'</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + <span class="string">'2c'</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = layers.add([x, input_tensor])</span><br><span class="line">    x = layers.Activation(<span class="string">'relu'</span>)(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>  ResNet-34 的网络结构如下所示：</p>
<figure>
<img src="/images/ResNet-34.jpg" alt="ResNet-34"><figcaption>ResNet-34</figcaption>
</figure>
<p>不同的ResNet结构对比：</p>
<figure>
<img src="/images/ResNet_mode.jpg" alt="resnet结构"><figcaption>resnet结构</figcaption>
</figure>
<h2 id="densenet">DenseNet</h2>
<p>  说到DenseNet全文就俩公式，当时我还觉得我咋写不出来这样的论文~~。</p>
<p>先预览下<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">DenseNet论文</a>里的DenseBlocks:</p>
<figure>
<img src="/images/DenseNetBlock.png" alt="DenseBlock"><figcaption>DenseBlock</figcaption>
</figure>
<p>  <strong>ResNet：</strong>传统的前馈卷积神经网络将第+1层的输入，可表示为：<span class="math inline">\(x_{\ell} = H_{\ell}(x_{\ell-1})\)</span>。ResNet添加了一个跳连接，即使用恒等函数跳过非线性变换： <span class="math display">\[
x_{\ell} = H_{\ell}(x_{\ell-1}) + x_{\ell-1}
\]</span></p>
<p><strong>密集连接：</strong>为了进一步改善层之间的信息流，我们提出了不同的连接模式：我们提出从任何层到所有后续层的直接连接。因此，第<span class="math inline">\(x_{0},…,x_{\ell-1}\)</span>作为输入: <span class="math display">\[
x_{\ell} = H_{\ell}([x_{0},x_{1},...,x_{\ell-1}])
\]</span>   如果说Iception ResNet V2是Inception 思想融合了ResNet，那么我认为 DenseNet就是 Residual 思想（信息跨层流通）结合了Inception 理念。</p>
<figure>
<img src="/images/densenet.png" alt="densenet"><figcaption>densenet</figcaption>
</figure>
<p>DenseNet的几个重要参数：</p>
<p>  <strong>增长率：</strong>如果每个<span class="math inline">\(H_{\ell}\)</span>层输出K个特征图，那么第<span class="math inline">\(\ell\)</span>层输出<span class="math inline">\(K_{0}+K(\ell-1)\)</span>个特征图，其中<span class="math inline">\(K_{0}\)</span>是输入层的通道数。</p>
<p>  <strong>瓶颈层：</strong>即<span class="math inline">\(1\times1\)</span>卷积层。每一层输出<span class="math inline">\(K_{0}+K(\ell-1)\)</span>个，理论上将每个输出为个Feature Maps，理论上将每个Dense Block输出为K_{0}+_{1}^{}K(i-1)$个Fature Maps，。<span class="math inline">\(1\times1\)</span>卷积层的作用是将一个Dense Block的特征图压缩到<span class="math inline">\(K_{0}+K\ell\)</span>个。</p>
<p>  <strong>压缩：</strong>为了进一步提高模型的紧凑性，我们可以在过渡层减少特征图的数量。。作者选择压缩率（theta）为0.5。包含Bottleneck Layer的叫DenseNet-B，包含压缩层的叫DenseNet-C，两者都包含的叫DenseNet-BC</p>
<p><a href="">keras_applications实现的密集连接模块</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_block</span><span class="params">(x, blocks, name)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A dense block.</span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        x: input tensor.</span></span><br><span class="line"><span class="string">        blocks: integer, the number of building blocks.</span></span><br><span class="line"><span class="string">        name: string, block label.</span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        output tensor for the block.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(blocks):</span><br><span class="line">        x = conv_block(x, <span class="number">32</span>, name=name + <span class="string">'_block'</span> + str(i + <span class="number">1</span>))</span><br><span class="line">        <span class="comment">#每个conv_blocks的输出是[x,conv_block(x, 32, name=name + '_block' + str(i + 1))]</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="senet">SENet</h2>
<p>  SENet(Squeeze-and-Excitation Networks)是基于特征通道之间的关系提出的，下图是SENet的Block单元，图中的Ftr是传统的卷积结构，X和U是Ftr的输入和输出，这些都是以往结构中已存在的。SENet增加的部分是U后的结构：对U先做一个Global Average Pooling（称为Squeeze过程），输出是一个1x1xC的数据，再经过两级全连接（称为Excitation过程），最后用sigmoid把输出限制到[0，1]的范围，把这个值作为scale再乘到U的C个通道上，作为下一级的输入数据。这种结构的原理是想通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。</p>
<figure>
<img src="/images/SENet.jpg" alt="SENet"><figcaption>SENet</figcaption>
</figure>
<p>其实<strong>SENet就是通道级attention机制</strong>。</p>
<p><a href="https://github.com/taki0112/SENet-Tensorflow" target="_blank" rel="noopener">由Tensorflow 实现的SE模块：</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Squeeze_excitation_layer</span><span class="params">(self, input_x, out_dim, ratio, layer_name)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(layer_name) :</span><br><span class="line">        squeeze = Global_Average_Pooling(input_x)</span><br><span class="line">        excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+<span class="string">'_fully_connected1'</span>)</span><br><span class="line">        excitation = Relu(excitation)</span><br><span class="line">        excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+<span class="string">'_fully_connected2'</span>)</span><br><span class="line">        excitation = Sigmoid(excitation)</span><br><span class="line">        excitation = tf.reshape(excitation, [<span class="number">-1</span>,<span class="number">1</span>,<span class="number">1</span>,out_dim])</span><br><span class="line">        scale = input_x * excitation</span><br><span class="line">        <span class="keyword">return</span> scale</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结：</h2>
<p>卷积神经网络设计：</p>
<ol type="1">
<li><p>从防止梯度消失出发</p></li>
<li><p>从信息流通出发</p></li>
<li><p>从多尺度特征图融合出发</p></li>
<li><p>从通道级或者像素级特征选择出发</p></li>
</ol>
<h2 id="reference">Reference</h2>
<ol type="1">
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">LeNet</a></li>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a></li>
<li><a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network-in-network</a></li>
<li><a href="http://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a></li>
<li><a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">GoogleNet</a></li>
<li><a href="http://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Inception V3</a></li>
<li><a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch-normalized</a></li>
<li><a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="noopener">ResNet</a></li>
<li>Hu J, Shen L, Sun G. Squeeze-and-Excitation Networks[J]. 2017.</li>
<li>https://chenzomi12.github.io/2016/12/13/CNN-Architectures/</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ResNet/" rel="tag"># ResNet</a>
          
            <a href="/tags/Inception/" rel="tag"># Inception</a>
          
            <a href="/tags/SENet/" rel="tag"># SENet</a>
          
            <a href="/tags/ShuffleNet/" rel="tag"># ShuffleNet</a>
          
            <a href="/tags/MobileNet/" rel="tag"># MobileNet</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/23/卷积神经网络/" rel="next" title="卷积神经网络">
                <i class="fa fa-chevron-left"></i> 卷积神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">会旋转的霸东</p>
              <p class="site-description motion-element" itemprop="description">写个博客，做做总结</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">33</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/XuDongHecs" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#卷机神经网络基础架构"><span class="nav-number">1.</span> <span class="nav-text">卷机神经网络基础架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#todo"><span class="nav-number">1.0.1.</span> <span class="nav-text">TODO:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lenet5"><span class="nav-number">2.</span> <span class="nav-text">LeNet5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#alexnet"><span class="nav-number">3.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vgg"><span class="nav-number">4.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#googlenet"><span class="nav-number">5.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inception-v2"><span class="nav-number">6.</span> <span class="nav-text">Inception V2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inception-v3"><span class="nav-number">7.</span> <span class="nav-text">Inception V3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#resnet"><span class="nav-number">8.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#densenet"><span class="nav-number">9.</span> <span class="nav-text">DenseNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#senet"><span class="nav-number">10.</span> <span class="nav-text">SENet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">11.</span> <span class="nav-text">总结：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-number">12.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>

<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">会旋转的霸东</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">23.1k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://会旋转的霸东.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/12/23/卷积神经网络模型简述/';
          this.page.identifier = '2018/12/23/卷积神经网络模型简述/';
          this.page.title = '卷积神经网络模型简述';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://会旋转的霸东.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === '') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("NXTq4e58jf6dTJ0qi38qshKE-gzGzoHsz", "tIv43xTrayY8bjbM9gRS8q6t");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  
  <!-- custom analytics part create by xiamo -->
<script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
<script>AV.initialize("NXTq4e58jf6dTJ0qi38qshKE-gzGzoHsz", "tIv43xTrayY8bjbM9gRS8q6t");</script>
<script>
function showTime(Counter) {
	var query = new AV.Query(Counter);
	$(".leancloud_visitors").each(function() {
		var url = $(this).attr("id").trim();
		query.equalTo("url", url);
		query.find({
			success: function(results) {
				if (results.length == 0) {
					var content = '0 ' + $(document.getElementById(url)).text();
					$(document.getElementById(url)).text(content);
					return;
				}
				for (var i = 0; i < results.length; i++) {
					var object = results[i];
					var content = object.get('time') + ' ' + $(document.getElementById(url)).text();
					$(document.getElementById(url)).text(content);
				}
			},
			error: function(object, error) {
				console.log("Error: " + error.code + " " + error.message);
			}
		});

	});
}

function addCount(Counter) {
	var Counter = AV.Object.extend("Counter");
	url = $(".leancloud_visitors").attr('id').trim();
	title = $(".leancloud_visitors").attr('data-flag-title').trim();
	var query = new AV.Query(Counter);
	query.equalTo("url", url);
	query.find({
		success: function(results) {
			if (results.length > 0) {
				var counter = results[0];
				counter.fetchWhenSave(true);
				counter.increment("time");
				counter.save(null, {
					success: function(counter) {
						var content =  counter.get('time') + ' ' + $(document.getElementById(url)).text();
						$(document.getElementById(url)).text(content);
					},
					error: function(counter, error) {
						console.log('Failed to save Visitor num, with error message: ' + error.message);
					}
				});
			} else {
				var newcounter = new Counter();
				newcounter.set("title", title);
				newcounter.set("url", url);
				newcounter.set("time", 1);
				newcounter.save(null, {
					success: function(newcounter) {
					    console.log("newcounter.get('time')="+newcounter.get('time'));
						var content = newcounter.get('time') + ' ' + $(document.getElementById(url)).text();
						$(document.getElementById(url)).text(content);
					},
					error: function(newcounter, error) {
						console.log('Failed to create');
					}
				});
			}
		},
		error: function(error) {
			console.log('Error:' + error.code + " " + error.message);
		}
	});
}
$(function() {
	var Counter = AV.Object.extend("Counter");
	if ($('.leancloud_visitors').length == 1) {
		addCount(Counter);
	} else if ($('.post-title-link').length > 1) {
		showTime(Counter);
	}
}); 
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  
</body>
</html>
